{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a00f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.StackedAutoEncoder import SimpleAutoEncoder, AutoEncoder, StackedAutoEncoder\n",
    "from models.LSTM import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bdb462e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 730, 97])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_csv(\"model_data/test_data/ABB.csv\")\n",
    "# data = data[[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]]\n",
    "data = np.load(\"model_data/train.npy\")\n",
    "data = torch.tensor(data, dtype=torch.float32)\n",
    "data.shape\n",
    "test_data = np.load(\"model_data/test.npy\")\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.flatten(data, end_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "489c9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 196.11172103881836 Val Loss: tensor(156.9722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1 Loss: 195.774968624115 Val Loss: tensor(156.8663, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2 Loss: 195.47186756134033 Val Loss: tensor(156.7596, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3 Loss: 195.18717169761658 Val Loss: tensor(156.6531, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4 Loss: 194.91823649406433 Val Loss: tensor(156.5466, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5 Loss: 194.66152358055115 Val Loss: tensor(156.4393, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6 Loss: 194.4129557609558 Val Loss: tensor(156.3306, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7 Loss: 194.16861486434937 Val Loss: tensor(156.2192, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8 Loss: 193.92525100708008 Val Loss: tensor(156.1043, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9 Loss: 193.68030166625977 Val Loss: tensor(155.9828, grad_fn=<MseLossBackward0>)\n",
      "Epoch 10 Loss: 193.43175554275513 Val Loss: tensor(155.8469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 11 Loss: 193.17846608161926 Val Loss: tensor(155.6872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 12 Loss: 192.9212863445282 Val Loss: tensor(155.5128, grad_fn=<MseLossBackward0>)\n",
      "Epoch 13 Loss: 192.66468143463135 Val Loss: tensor(155.3307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 14 Loss: 192.41544127464294 Val Loss: tensor(155.1587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 15 Loss: 192.17641139030457 Val Loss: tensor(155.0119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 16 Loss: 191.94575190544128 Val Loss: tensor(154.8826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 17 Loss: 191.72318625450134 Val Loss: tensor(154.7566, grad_fn=<MseLossBackward0>)\n",
      "Epoch 18 Loss: 191.5093913078308 Val Loss: tensor(154.6251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 19 Loss: 191.30191493034363 Val Loss: tensor(154.4945, grad_fn=<MseLossBackward0>)\n",
      "Epoch 20 Loss: 191.0969319343567 Val Loss: tensor(154.3705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 21 Loss: 190.89213061332703 Val Loss: tensor(154.2505, grad_fn=<MseLossBackward0>)\n",
      "Epoch 22 Loss: 190.68648481369019 Val Loss: tensor(154.1317, grad_fn=<MseLossBackward0>)\n",
      "Epoch 23 Loss: 190.47930216789246 Val Loss: tensor(154.0128, grad_fn=<MseLossBackward0>)\n",
      "Epoch 24 Loss: 190.27004289627075 Val Loss: tensor(153.8928, grad_fn=<MseLossBackward0>)\n",
      "Epoch 25 Loss: 190.05791234970093 Val Loss: tensor(153.7707, grad_fn=<MseLossBackward0>)\n",
      "Epoch 26 Loss: 189.84235048294067 Val Loss: tensor(153.6453, grad_fn=<MseLossBackward0>)\n",
      "Epoch 27 Loss: 189.6227023601532 Val Loss: tensor(153.5160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 28 Loss: 189.39863872528076 Val Loss: tensor(153.3821, grad_fn=<MseLossBackward0>)\n",
      "Epoch 29 Loss: 189.16989922523499 Val Loss: tensor(153.2433, grad_fn=<MseLossBackward0>)\n",
      "Epoch 30 Loss: 188.93622136116028 Val Loss: tensor(153.0999, grad_fn=<MseLossBackward0>)\n",
      "Epoch 31 Loss: 188.69757413864136 Val Loss: tensor(152.9525, grad_fn=<MseLossBackward0>)\n",
      "Epoch 32 Loss: 188.45379042625427 Val Loss: tensor(152.8020, grad_fn=<MseLossBackward0>)\n",
      "Epoch 33 Loss: 188.2047770023346 Val Loss: tensor(152.6492, grad_fn=<MseLossBackward0>)\n",
      "Epoch 34 Loss: 187.95038962364197 Val Loss: tensor(152.4950, grad_fn=<MseLossBackward0>)\n",
      "Epoch 35 Loss: 187.69090604782104 Val Loss: tensor(152.3400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 36 Loss: 187.4269073009491 Val Loss: tensor(152.1844, grad_fn=<MseLossBackward0>)\n",
      "Epoch 37 Loss: 187.159752368927 Val Loss: tensor(152.0285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 38 Loss: 186.89043807983398 Val Loss: tensor(151.8719, grad_fn=<MseLossBackward0>)\n",
      "Epoch 39 Loss: 186.61927342414856 Val Loss: tensor(151.7141, grad_fn=<MseLossBackward0>)\n",
      "Epoch 40 Loss: 186.34559679031372 Val Loss: tensor(151.5543, grad_fn=<MseLossBackward0>)\n",
      "Epoch 41 Loss: 186.06866478919983 Val Loss: tensor(151.3923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 42 Loss: 185.78843760490417 Val Loss: tensor(151.2284, grad_fn=<MseLossBackward0>)\n",
      "Epoch 43 Loss: 185.5057463645935 Val Loss: tensor(151.0627, grad_fn=<MseLossBackward0>)\n",
      "Epoch 44 Loss: 185.22189164161682 Val Loss: tensor(150.8954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 45 Loss: 184.93918299674988 Val Loss: tensor(150.7259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 46 Loss: 184.65924620628357 Val Loss: tensor(150.5533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 47 Loss: 184.3796124458313 Val Loss: tensor(150.3778, grad_fn=<MseLossBackward0>)\n",
      "Epoch 48 Loss: 184.0989637374878 Val Loss: tensor(150.1999, grad_fn=<MseLossBackward0>)\n",
      "Epoch 49 Loss: 183.81842851638794 Val Loss: tensor(150.0210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 50 Loss: 183.53797936439514 Val Loss: tensor(149.8419, grad_fn=<MseLossBackward0>)\n",
      "Epoch 51 Loss: 183.25701451301575 Val Loss: tensor(149.6634, grad_fn=<MseLossBackward0>)\n",
      "Epoch 52 Loss: 182.97685194015503 Val Loss: tensor(149.4866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 53 Loss: 182.6987292766571 Val Loss: tensor(149.3123, grad_fn=<MseLossBackward0>)\n",
      "Epoch 54 Loss: 182.42174410820007 Val Loss: tensor(149.1411, grad_fn=<MseLossBackward0>)\n",
      "Epoch 55 Loss: 182.14572477340698 Val Loss: tensor(148.9724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 56 Loss: 181.8713719844818 Val Loss: tensor(148.8048, grad_fn=<MseLossBackward0>)\n",
      "Epoch 57 Loss: 181.59924578666687 Val Loss: tensor(148.6376, grad_fn=<MseLossBackward0>)\n",
      "Epoch 58 Loss: 181.3289442062378 Val Loss: tensor(148.4722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 59 Loss: 181.06092071533203 Val Loss: tensor(148.3103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 60 Loss: 180.79582333564758 Val Loss: tensor(148.1522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 61 Loss: 180.53357481956482 Val Loss: tensor(147.9982, grad_fn=<MseLossBackward0>)\n",
      "Epoch 62 Loss: 180.27377223968506 Val Loss: tensor(147.8483, grad_fn=<MseLossBackward0>)\n",
      "Epoch 63 Loss: 180.01621985435486 Val Loss: tensor(147.7018, grad_fn=<MseLossBackward0>)\n",
      "Epoch 64 Loss: 179.76054573059082 Val Loss: tensor(147.5607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 65 Loss: 179.50586676597595 Val Loss: tensor(147.4352, grad_fn=<MseLossBackward0>)\n",
      "Epoch 66 Loss: 179.25138020515442 Val Loss: tensor(147.3345, grad_fn=<MseLossBackward0>)\n",
      "Epoch 67 Loss: 178.99638628959656 Val Loss: tensor(147.2475, grad_fn=<MseLossBackward0>)\n",
      "Epoch 68 Loss: 178.7398750782013 Val Loss: tensor(147.1566, grad_fn=<MseLossBackward0>)\n",
      "Epoch 69 Loss: 178.48118424415588 Val Loss: tensor(147.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 70 Loss: 178.2213945388794 Val Loss: tensor(146.9421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 71 Loss: 177.96572518348694 Val Loss: tensor(146.8212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 72 Loss: 177.72685050964355 Val Loss: tensor(146.6923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 73 Loss: 177.49837589263916 Val Loss: tensor(146.5519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 74 Loss: 177.26779913902283 Val Loss: tensor(146.4020, grad_fn=<MseLossBackward0>)\n",
      "Epoch 75 Loss: 177.04283785820007 Val Loss: tensor(146.2700, grad_fn=<MseLossBackward0>)\n",
      "Epoch 76 Loss: 176.81785917282104 Val Loss: tensor(146.1620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 77 Loss: 176.593825340271 Val Loss: tensor(146.0509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 78 Loss: 176.37309741973877 Val Loss: tensor(145.9291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 79 Loss: 176.15554809570312 Val Loss: tensor(145.8087, grad_fn=<MseLossBackward0>)\n",
      "Epoch 80 Loss: 175.93749475479126 Val Loss: tensor(145.7072, grad_fn=<MseLossBackward0>)\n",
      "Epoch 81 Loss: 175.71838092803955 Val Loss: tensor(145.6270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 82 Loss: 175.49829578399658 Val Loss: tensor(145.5470, grad_fn=<MseLossBackward0>)\n",
      "Epoch 83 Loss: 175.2775444984436 Val Loss: tensor(145.4518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 84 Loss: 175.0550820827484 Val Loss: tensor(145.3532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 85 Loss: 174.83112835884094 Val Loss: tensor(145.2683, grad_fn=<MseLossBackward0>)\n",
      "Epoch 86 Loss: 174.60598158836365 Val Loss: tensor(145.1953, grad_fn=<MseLossBackward0>)\n",
      "Epoch 87 Loss: 174.3783721923828 Val Loss: tensor(145.1227, grad_fn=<MseLossBackward0>)\n",
      "Epoch 88 Loss: 174.14733147621155 Val Loss: tensor(145.0444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 89 Loss: 173.91323375701904 Val Loss: tensor(144.9585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 90 Loss: 173.67652797698975 Val Loss: tensor(144.8657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 91 Loss: 173.43698287010193 Val Loss: tensor(144.7724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 92 Loss: 173.1950879096985 Val Loss: tensor(144.6868, grad_fn=<MseLossBackward0>)\n",
      "Epoch 93 Loss: 172.95121932029724 Val Loss: tensor(144.6120, grad_fn=<MseLossBackward0>)\n",
      "Epoch 94 Loss: 172.70516681671143 Val Loss: tensor(144.5434, grad_fn=<MseLossBackward0>)\n",
      "Epoch 95 Loss: 172.45746064186096 Val Loss: tensor(144.4731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 96 Loss: 172.2083432674408 Val Loss: tensor(144.3981, grad_fn=<MseLossBackward0>)\n",
      "Epoch 97 Loss: 171.95777654647827 Val Loss: tensor(144.3204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 98 Loss: 171.70563864707947 Val Loss: tensor(144.2394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 99 Loss: 171.4517059326172 Val Loss: tensor(144.1508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 100 Loss: 171.19539189338684 Val Loss: tensor(144.0521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 101 Loss: 170.93649053573608 Val Loss: tensor(143.9442, grad_fn=<MseLossBackward0>)\n",
      "Epoch 102 Loss: 170.67455124855042 Val Loss: tensor(143.8294, grad_fn=<MseLossBackward0>)\n",
      "Epoch 103 Loss: 170.4090085029602 Val Loss: tensor(143.7070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 104 Loss: 170.13953113555908 Val Loss: tensor(143.5745, grad_fn=<MseLossBackward0>)\n",
      "Epoch 105 Loss: 169.8657877445221 Val Loss: tensor(143.4321, grad_fn=<MseLossBackward0>)\n",
      "Epoch 106 Loss: 169.58759200572968 Val Loss: tensor(143.2817, grad_fn=<MseLossBackward0>)\n",
      "Epoch 107 Loss: 169.30507051944733 Val Loss: tensor(143.1234, grad_fn=<MseLossBackward0>)\n",
      "Epoch 108 Loss: 169.01858830451965 Val Loss: tensor(142.9560, grad_fn=<MseLossBackward0>)\n",
      "Epoch 109 Loss: 168.72879564762115 Val Loss: tensor(142.7799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 110 Loss: 168.4367698431015 Val Loss: tensor(142.5965, grad_fn=<MseLossBackward0>)\n",
      "Epoch 111 Loss: 168.14355182647705 Val Loss: tensor(142.4086, grad_fn=<MseLossBackward0>)\n",
      "Epoch 112 Loss: 167.850191116333 Val Loss: tensor(142.2212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 113 Loss: 167.55708944797516 Val Loss: tensor(142.0385, grad_fn=<MseLossBackward0>)\n",
      "Epoch 114 Loss: 167.26384842395782 Val Loss: tensor(141.8602, grad_fn=<MseLossBackward0>)\n",
      "Epoch 115 Loss: 166.97009229660034 Val Loss: tensor(141.6835, grad_fn=<MseLossBackward0>)\n",
      "Epoch 116 Loss: 166.67534232139587 Val Loss: tensor(141.5060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 117 Loss: 166.37942719459534 Val Loss: tensor(141.3262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 118 Loss: 166.0820734500885 Val Loss: tensor(141.1429, grad_fn=<MseLossBackward0>)\n",
      "Epoch 119 Loss: 165.78317213058472 Val Loss: tensor(140.9547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 120 Loss: 165.48261749744415 Val Loss: tensor(140.7603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 121 Loss: 165.1804565191269 Val Loss: tensor(140.5600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 122 Loss: 164.87687826156616 Val Loss: tensor(140.3555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 123 Loss: 164.57229220867157 Val Loss: tensor(140.1504, grad_fn=<MseLossBackward0>)\n",
      "Epoch 124 Loss: 164.26739871501923 Val Loss: tensor(139.9480, grad_fn=<MseLossBackward0>)\n",
      "Epoch 125 Loss: 163.96270728111267 Val Loss: tensor(139.7489, grad_fn=<MseLossBackward0>)\n",
      "Epoch 126 Loss: 163.65834033489227 Val Loss: tensor(139.5505, grad_fn=<MseLossBackward0>)\n",
      "Epoch 127 Loss: 163.35413432121277 Val Loss: tensor(139.3521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 128 Loss: 163.04990780353546 Val Loss: tensor(139.1552, grad_fn=<MseLossBackward0>)\n",
      "Epoch 129 Loss: 162.74568724632263 Val Loss: tensor(138.9604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 130 Loss: 162.441845536232 Val Loss: tensor(138.7679, grad_fn=<MseLossBackward0>)\n",
      "Epoch 131 Loss: 162.13907396793365 Val Loss: tensor(138.5796, grad_fn=<MseLossBackward0>)\n",
      "Epoch 132 Loss: 161.83790731430054 Val Loss: tensor(138.3982, grad_fn=<MseLossBackward0>)\n",
      "Epoch 133 Loss: 161.5385525226593 Val Loss: tensor(138.2252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 134 Loss: 161.24032747745514 Val Loss: tensor(138.0615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 135 Loss: 160.94238698482513 Val Loss: tensor(137.9071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 136 Loss: 160.6441549062729 Val Loss: tensor(137.7612, grad_fn=<MseLossBackward0>)\n",
      "Epoch 137 Loss: 160.34529638290405 Val Loss: tensor(137.6190, grad_fn=<MseLossBackward0>)\n",
      "Epoch 138 Loss: 160.0452584028244 Val Loss: tensor(137.4722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 139 Loss: 159.7437266111374 Val Loss: tensor(137.3155, grad_fn=<MseLossBackward0>)\n",
      "Epoch 140 Loss: 159.44116127490997 Val Loss: tensor(137.1521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 141 Loss: 159.1390002965927 Val Loss: tensor(136.9881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 142 Loss: 158.83883261680603 Val Loss: tensor(136.8305, grad_fn=<MseLossBackward0>)\n",
      "Epoch 143 Loss: 158.5425775051117 Val Loss: tensor(136.6801, grad_fn=<MseLossBackward0>)\n",
      "Epoch 144 Loss: 158.25109469890594 Val Loss: tensor(136.5416, grad_fn=<MseLossBackward0>)\n",
      "Epoch 145 Loss: 157.96487522125244 Val Loss: tensor(136.4095, grad_fn=<MseLossBackward0>)\n",
      "Epoch 146 Loss: 157.68178284168243 Val Loss: tensor(136.2901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 147 Loss: 157.4022535085678 Val Loss: tensor(136.1649, grad_fn=<MseLossBackward0>)\n",
      "Epoch 148 Loss: 157.12259769439697 Val Loss: tensor(136.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 149 Loss: 156.84734427928925 Val Loss: tensor(135.9172, grad_fn=<MseLossBackward0>)\n",
      "Epoch 150 Loss: 156.56861245632172 Val Loss: tensor(135.8191, grad_fn=<MseLossBackward0>)\n",
      "Epoch 151 Loss: 156.29997873306274 Val Loss: tensor(135.6549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 152 Loss: 156.02084636688232 Val Loss: tensor(135.5971, grad_fn=<MseLossBackward0>)\n",
      "Epoch 153 Loss: 155.76706635951996 Val Loss: tensor(135.3743, grad_fn=<MseLossBackward0>)\n",
      "Epoch 154 Loss: 155.48846173286438 Val Loss: tensor(135.4110, grad_fn=<MseLossBackward0>)\n",
      "Epoch 155 Loss: 155.27257204055786 Val Loss: tensor(135.0708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 156 Loss: 155.00028240680695 Val Loss: tensor(135.1982, grad_fn=<MseLossBackward0>)\n",
      "Epoch 157 Loss: 154.82323384284973 Val Loss: tensor(134.8235, grad_fn=<MseLossBackward0>)\n",
      "Epoch 158 Loss: 154.51344895362854 Val Loss: tensor(134.7443, grad_fn=<MseLossBackward0>)\n",
      "Epoch 159 Loss: 154.21974766254425 Val Loss: tensor(134.7102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 160 Loss: 153.90700018405914 Val Loss: tensor(134.5109, grad_fn=<MseLossBackward0>)\n",
      "Epoch 161 Loss: 153.64955186843872 Val Loss: tensor(134.5280, grad_fn=<MseLossBackward0>)\n",
      "Epoch 162 Loss: 153.39448308944702 Val Loss: tensor(134.2839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 163 Loss: 153.1360296010971 Val Loss: tensor(134.1533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 164 Loss: 152.87635385990143 Val Loss: tensor(134.0753, grad_fn=<MseLossBackward0>)\n",
      "Epoch 165 Loss: 152.60642313957214 Val Loss: tensor(133.8735, grad_fn=<MseLossBackward0>)\n",
      "Epoch 166 Loss: 152.35689115524292 Val Loss: tensor(133.8308, grad_fn=<MseLossBackward0>)\n",
      "Epoch 167 Loss: 152.0911101102829 Val Loss: tensor(133.6739, grad_fn=<MseLossBackward0>)\n",
      "Epoch 168 Loss: 151.84217166900635 Val Loss: tensor(133.5477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 169 Loss: 151.58568930625916 Val Loss: tensor(133.4435, grad_fn=<MseLossBackward0>)\n",
      "Epoch 170 Loss: 151.33057641983032 Val Loss: tensor(133.2845, grad_fn=<MseLossBackward0>)\n",
      "Epoch 171 Loss: 151.08318614959717 Val Loss: tensor(133.1839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 172 Loss: 150.82378947734833 Val Loss: tensor(133.0471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 173 Loss: 150.5777189731598 Val Loss: tensor(132.9350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 174 Loss: 150.32009053230286 Val Loss: tensor(132.8161, grad_fn=<MseLossBackward0>)\n",
      "Epoch 175 Loss: 150.07171034812927 Val Loss: tensor(132.6886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 176 Loss: 149.81835758686066 Val Loss: tensor(132.5731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 177 Loss: 149.56779861450195 Val Loss: tensor(132.4450, grad_fn=<MseLossBackward0>)\n",
      "Epoch 178 Loss: 149.31624579429626 Val Loss: tensor(132.3324, grad_fn=<MseLossBackward0>)\n",
      "Epoch 179 Loss: 149.06374216079712 Val Loss: tensor(132.2029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 180 Loss: 148.81173753738403 Val Loss: tensor(132.0906, grad_fn=<MseLossBackward0>)\n",
      "Epoch 181 Loss: 148.55891489982605 Val Loss: tensor(131.9656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 182 Loss: 148.30436277389526 Val Loss: tensor(131.8497, grad_fn=<MseLossBackward0>)\n",
      "Epoch 183 Loss: 148.0518137216568 Val Loss: tensor(131.7259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 184 Loss: 147.79272723197937 Val Loss: tensor(131.6083, grad_fn=<MseLossBackward0>)\n",
      "Epoch 185 Loss: 147.54265880584717 Val Loss: tensor(131.4841, grad_fn=<MseLossBackward0>)\n",
      "Epoch 186 Loss: 147.27776634693146 Val Loss: tensor(131.3700, grad_fn=<MseLossBackward0>)\n",
      "Epoch 187 Loss: 147.0369917154312 Val Loss: tensor(131.2385, grad_fn=<MseLossBackward0>)\n",
      "Epoch 188 Loss: 146.76874577999115 Val Loss: tensor(131.1289, grad_fn=<MseLossBackward0>)\n",
      "Epoch 189 Loss: 146.5563633441925 Val Loss: tensor(131.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch 190 Loss: 146.29970347881317 Val Loss: tensor(130.8422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 191 Loss: 146.12467300891876 Val Loss: tensor(130.7544, grad_fn=<MseLossBackward0>)\n",
      "Epoch 192 Loss: 145.84226632118225 Val Loss: tensor(130.4637, grad_fn=<MseLossBackward0>)\n",
      "Epoch 193 Loss: 145.59989500045776 Val Loss: tensor(130.4373, grad_fn=<MseLossBackward0>)\n",
      "Epoch 194 Loss: 145.21936881542206 Val Loss: tensor(130.2006, grad_fn=<MseLossBackward0>)\n",
      "Epoch 195 Loss: 144.93387174606323 Val Loss: tensor(130.0943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 196 Loss: 144.63988935947418 Val Loss: tensor(130.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch 197 Loss: 144.39429783821106 Val Loss: tensor(129.7386, grad_fn=<MseLossBackward0>)\n",
      "Epoch 198 Loss: 144.16834223270416 Val Loss: tensor(129.7216, grad_fn=<MseLossBackward0>)\n",
      "Epoch 199 Loss: 143.91233885288239 Val Loss: tensor(129.4696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 200 Loss: 143.69049096107483 Val Loss: tensor(129.4477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 201 Loss: 143.42655777931213 Val Loss: tensor(129.2065, grad_fn=<MseLossBackward0>)\n",
      "Epoch 202 Loss: 143.2004907131195 Val Loss: tensor(129.1894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 203 Loss: 142.9546743631363 Val Loss: tensor(128.9242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 204 Loss: 142.72487831115723 Val Loss: tensor(128.9447, grad_fn=<MseLossBackward0>)\n",
      "Epoch 205 Loss: 142.50640869140625 Val Loss: tensor(128.6150, grad_fn=<MseLossBackward0>)\n",
      "Epoch 206 Loss: 142.27551543712616 Val Loss: tensor(128.7315, grad_fn=<MseLossBackward0>)\n",
      "Epoch 207 Loss: 142.08302700519562 Val Loss: tensor(128.3060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 208 Loss: 141.85800862312317 Val Loss: tensor(128.5516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 209 Loss: 141.68984407186508 Val Loss: tensor(128.0286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 210 Loss: 141.45857727527618 Val Loss: tensor(128.2727, grad_fn=<MseLossBackward0>)\n",
      "Epoch 211 Loss: 141.26445245742798 Val Loss: tensor(127.8250, grad_fn=<MseLossBackward0>)\n",
      "Epoch 212 Loss: 140.99600064754486 Val Loss: tensor(127.9028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 213 Loss: 140.74191743135452 Val Loss: tensor(127.7022, grad_fn=<MseLossBackward0>)\n",
      "Epoch 214 Loss: 140.4901477098465 Val Loss: tensor(127.5779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 215 Loss: 140.25960451364517 Val Loss: tensor(127.5614, grad_fn=<MseLossBackward0>)\n",
      "Epoch 216 Loss: 140.0449560880661 Val Loss: tensor(127.2968, grad_fn=<MseLossBackward0>)\n",
      "Epoch 217 Loss: 139.82859927415848 Val Loss: tensor(127.3460, grad_fn=<MseLossBackward0>)\n",
      "Epoch 218 Loss: 139.61666482686996 Val Loss: tensor(127.0647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 219 Loss: 139.3939564228058 Val Loss: tensor(127.1108, grad_fn=<MseLossBackward0>)\n",
      "Epoch 220 Loss: 139.17400389909744 Val Loss: tensor(126.8724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 221 Loss: 138.95166701078415 Val Loss: tensor(126.8708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 222 Loss: 138.73068183660507 Val Loss: tensor(126.6865, grad_fn=<MseLossBackward0>)\n",
      "Epoch 223 Loss: 138.51457345485687 Val Loss: tensor(126.6237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 224 Loss: 138.29570484161377 Val Loss: tensor(126.4897, grad_fn=<MseLossBackward0>)\n",
      "Epoch 225 Loss: 138.08484953641891 Val Loss: tensor(126.3857, grad_fn=<MseLossBackward0>)\n",
      "Epoch 226 Loss: 137.86752271652222 Val Loss: tensor(126.2889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 227 Loss: 137.6605976819992 Val Loss: tensor(126.1611, grad_fn=<MseLossBackward0>)\n",
      "Epoch 228 Loss: 137.44409960508347 Val Loss: tensor(126.0842, grad_fn=<MseLossBackward0>)\n",
      "Epoch 229 Loss: 137.24296694993973 Val Loss: tensor(125.9378, grad_fn=<MseLossBackward0>)\n",
      "Epoch 230 Loss: 137.02853804826736 Val Loss: tensor(125.8861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 231 Loss: 136.8393121957779 Val Loss: tensor(125.7126, grad_fn=<MseLossBackward0>)\n",
      "Epoch 232 Loss: 136.62953346967697 Val Loss: tensor(125.7058, grad_fn=<MseLossBackward0>)\n",
      "Epoch 233 Loss: 136.46500778198242 Val Loss: tensor(125.4763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 234 Loss: 136.25950288772583 Val Loss: tensor(125.5396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 235 Loss: 136.12822061777115 Val Loss: tensor(125.2195, grad_fn=<MseLossBackward0>)\n",
      "Epoch 236 Loss: 135.8963115811348 Val Loss: tensor(125.3421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 237 Loss: 135.7504773736 Val Loss: tensor(124.9562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 238 Loss: 135.45171123743057 Val Loss: tensor(125.0885, grad_fn=<MseLossBackward0>)\n",
      "Epoch 239 Loss: 135.25276684761047 Val Loss: tensor(124.7493, grad_fn=<MseLossBackward0>)\n",
      "Epoch 240 Loss: 134.9604641199112 Val Loss: tensor(124.8210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 241 Loss: 134.76344525814056 Val Loss: tensor(124.5759, grad_fn=<MseLossBackward0>)\n",
      "Epoch 242 Loss: 134.52221578359604 Val Loss: tensor(124.5481, grad_fn=<MseLossBackward0>)\n",
      "Epoch 243 Loss: 134.32943338155746 Val Loss: tensor(124.3743, grad_fn=<MseLossBackward0>)\n",
      "Epoch 244 Loss: 134.11459058523178 Val Loss: tensor(124.3069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 245 Loss: 133.912453353405 Val Loss: tensor(124.1488, grad_fn=<MseLossBackward0>)\n",
      "Epoch 246 Loss: 133.70606344938278 Val Loss: tensor(124.1040, grad_fn=<MseLossBackward0>)\n",
      "Epoch 247 Loss: 133.4967224597931 Val Loss: tensor(123.9155, grad_fn=<MseLossBackward0>)\n",
      "Epoch 248 Loss: 133.2952257990837 Val Loss: tensor(123.9181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 249 Loss: 133.08538842201233 Val Loss: tensor(123.6749, grad_fn=<MseLossBackward0>)\n",
      "Epoch 250 Loss: 132.88866633176804 Val Loss: tensor(123.7425, grad_fn=<MseLossBackward0>)\n",
      "Epoch 251 Loss: 132.68436580896378 Val Loss: tensor(123.4160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 252 Loss: 132.4956768155098 Val Loss: tensor(123.6081, grad_fn=<MseLossBackward0>)\n",
      "Epoch 253 Loss: 132.30971562862396 Val Loss: tensor(123.1293, grad_fn=<MseLossBackward0>)\n",
      "Epoch 254 Loss: 132.1436786055565 Val Loss: tensor(123.5377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 255 Loss: 132.01181733608246 Val Loss: tensor(122.8402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 256 Loss: 131.88341265916824 Val Loss: tensor(123.4135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 257 Loss: 131.7999935746193 Val Loss: tensor(122.6068, grad_fn=<MseLossBackward0>)\n",
      "Epoch 258 Loss: 131.59978234767914 Val Loss: tensor(122.9622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 259 Loss: 131.31425368785858 Val Loss: tensor(122.4626, grad_fn=<MseLossBackward0>)\n",
      "Epoch 260 Loss: 131.01112353801727 Val Loss: tensor(122.4547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 261 Loss: 130.71247792243958 Val Loss: tensor(122.4761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 262 Loss: 130.55667304992676 Val Loss: tensor(122.1103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 263 Loss: 130.35120475292206 Val Loss: tensor(122.2567, grad_fn=<MseLossBackward0>)\n",
      "Epoch 264 Loss: 130.20911413431168 Val Loss: tensor(121.9295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 265 Loss: 129.95725363492966 Val Loss: tensor(121.8703, grad_fn=<MseLossBackward0>)\n",
      "Epoch 266 Loss: 129.75457781553268 Val Loss: tensor(121.8426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 267 Loss: 129.5055261850357 Val Loss: tensor(121.5755, grad_fn=<MseLossBackward0>)\n",
      "Epoch 268 Loss: 129.31434333324432 Val Loss: tensor(121.6936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 269 Loss: 129.09507191181183 Val Loss: tensor(121.3814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 270 Loss: 128.90953654050827 Val Loss: tensor(121.4305, grad_fn=<MseLossBackward0>)\n",
      "Epoch 271 Loss: 128.7005769610405 Val Loss: tensor(121.2017, grad_fn=<MseLossBackward0>)\n",
      "Epoch 272 Loss: 128.5068307518959 Val Loss: tensor(121.1641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 273 Loss: 128.30356520414352 Val Loss: tensor(121.0237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 274 Loss: 128.10589426755905 Val Loss: tensor(120.9377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 275 Loss: 127.90715110301971 Val Loss: tensor(120.8206, grad_fn=<MseLossBackward0>)\n",
      "Epoch 276 Loss: 127.70989263057709 Val Loss: tensor(120.7192, grad_fn=<MseLossBackward0>)\n",
      "Epoch 277 Loss: 127.5146359205246 Val Loss: tensor(120.5966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 278 Loss: 127.31916809082031 Val Loss: tensor(120.5079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 279 Loss: 127.12627929449081 Val Loss: tensor(120.3846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 280 Loss: 126.93284463882446 Val Loss: tensor(120.3053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 281 Loss: 126.74435061216354 Val Loss: tensor(120.1789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 282 Loss: 126.55656492710114 Val Loss: tensor(120.0999, grad_fn=<MseLossBackward0>)\n",
      "Epoch 283 Loss: 126.38164985179901 Val Loss: tensor(119.9794, grad_fn=<MseLossBackward0>)\n",
      "Epoch 284 Loss: 126.21350848674774 Val Loss: tensor(119.9073, grad_fn=<MseLossBackward0>)\n",
      "Epoch 285 Loss: 126.07302349805832 Val Loss: tensor(119.8028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 286 Loss: 125.95621770620346 Val Loss: tensor(119.7252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 287 Loss: 125.84716796875 Val Loss: tensor(119.5951, grad_fn=<MseLossBackward0>)\n",
      "Epoch 288 Loss: 125.74810236692429 Val Loss: tensor(119.4812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 289 Loss: 125.48126572370529 Val Loss: tensor(119.2582, grad_fn=<MseLossBackward0>)\n",
      "Epoch 290 Loss: 125.24692392349243 Val Loss: tensor(119.2433, grad_fn=<MseLossBackward0>)\n",
      "Epoch 291 Loss: 124.88134801387787 Val Loss: tensor(118.9758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 292 Loss: 124.67298859357834 Val Loss: tensor(119.0962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 293 Loss: 124.4388707280159 Val Loss: tensor(118.7396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 294 Loss: 124.28730314970016 Val Loss: tensor(118.8743, grad_fn=<MseLossBackward0>)\n",
      "Epoch 295 Loss: 124.11305367946625 Val Loss: tensor(118.5163, grad_fn=<MseLossBackward0>)\n",
      "Epoch 296 Loss: 123.94625246524811 Val Loss: tensor(118.6257, grad_fn=<MseLossBackward0>)\n",
      "Epoch 297 Loss: 123.7715436220169 Val Loss: tensor(118.3028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 298 Loss: 123.57983404397964 Val Loss: tensor(118.3829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 299 Loss: 123.3803419470787 Val Loss: tensor(118.0790, grad_fn=<MseLossBackward0>)\n",
      "Epoch 300 Loss: 123.19110804796219 Val Loss: tensor(118.1760, grad_fn=<MseLossBackward0>)\n",
      "Epoch 301 Loss: 122.9853241443634 Val Loss: tensor(117.8357, grad_fn=<MseLossBackward0>)\n",
      "Epoch 302 Loss: 122.80866461992264 Val Loss: tensor(117.9725, grad_fn=<MseLossBackward0>)\n",
      "Epoch 303 Loss: 122.60866129398346 Val Loss: tensor(117.5807, grad_fn=<MseLossBackward0>)\n",
      "Epoch 304 Loss: 122.4403206706047 Val Loss: tensor(117.7841, grad_fn=<MseLossBackward0>)\n",
      "Epoch 305 Loss: 122.2432479262352 Val Loss: tensor(117.3380, grad_fn=<MseLossBackward0>)\n",
      "Epoch 306 Loss: 122.07853257656097 Val Loss: tensor(117.5820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 307 Loss: 121.87770760059357 Val Loss: tensor(117.1070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 308 Loss: 121.71294540166855 Val Loss: tensor(117.3462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 309 Loss: 121.50254851579666 Val Loss: tensor(116.8875, grad_fn=<MseLossBackward0>)\n",
      "Epoch 310 Loss: 121.3356065750122 Val Loss: tensor(117.0878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 311 Loss: 121.11554342508316 Val Loss: tensor(116.6792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 312 Loss: 120.94875144958496 Val Loss: tensor(116.8233, grad_fn=<MseLossBackward0>)\n",
      "Epoch 313 Loss: 120.72397303581238 Val Loss: tensor(116.4773, grad_fn=<MseLossBackward0>)\n",
      "Epoch 314 Loss: 120.55900168418884 Val Loss: tensor(116.5652, grad_fn=<MseLossBackward0>)\n",
      "Epoch 315 Loss: 120.33448153734207 Val Loss: tensor(116.2728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 316 Loss: 120.17047733068466 Val Loss: tensor(116.3204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 317 Loss: 119.94872844219208 Val Loss: tensor(116.0613, grad_fn=<MseLossBackward0>)\n",
      "Epoch 318 Loss: 119.7845321893692 Val Loss: tensor(116.0896, grad_fn=<MseLossBackward0>)\n",
      "Epoch 319 Loss: 119.56630212068558 Val Loss: tensor(115.8440, grad_fn=<MseLossBackward0>)\n",
      "Epoch 320 Loss: 119.40179920196533 Val Loss: tensor(115.8675, grad_fn=<MseLossBackward0>)\n",
      "Epoch 321 Loss: 119.18766742944717 Val Loss: tensor(115.6257, grad_fn=<MseLossBackward0>)\n",
      "Epoch 322 Loss: 119.02419924736023 Val Loss: tensor(115.6498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 323 Loss: 118.81523084640503 Val Loss: tensor(115.4106, grad_fn=<MseLossBackward0>)\n",
      "Epoch 324 Loss: 118.65575569868088 Val Loss: tensor(115.4352, grad_fn=<MseLossBackward0>)\n",
      "Epoch 325 Loss: 118.45504730939865 Val Loss: tensor(115.2037, grad_fn=<MseLossBackward0>)\n",
      "Epoch 326 Loss: 118.30697083473206 Val Loss: tensor(115.2250, grad_fn=<MseLossBackward0>)\n",
      "Epoch 327 Loss: 118.12412858009338 Val Loss: tensor(115.0140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 328 Loss: 118.00365424156189 Val Loss: tensor(115.0225, grad_fn=<MseLossBackward0>)\n",
      "Epoch 329 Loss: 117.86063998937607 Val Loss: tensor(114.8510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 330 Loss: 117.77671641111374 Val Loss: tensor(114.8001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 331 Loss: 117.6689036488533 Val Loss: tensor(114.6725, grad_fn=<MseLossBackward0>)\n",
      "Epoch 332 Loss: 117.51016438007355 Val Loss: tensor(114.4578, grad_fn=<MseLossBackward0>)\n",
      "Epoch 333 Loss: 117.29507201910019 Val Loss: tensor(114.4327, grad_fn=<MseLossBackward0>)\n",
      "Epoch 334 Loss: 116.96643835306168 Val Loss: tensor(114.0890, grad_fn=<MseLossBackward0>)\n",
      "Epoch 335 Loss: 116.69642341136932 Val Loss: tensor(114.2495, grad_fn=<MseLossBackward0>)\n",
      "Epoch 336 Loss: 116.4345234632492 Val Loss: tensor(113.8450, grad_fn=<MseLossBackward0>)\n",
      "Epoch 337 Loss: 116.24619936943054 Val Loss: tensor(114.0422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 338 Loss: 116.05672091245651 Val Loss: tensor(113.6621, grad_fn=<MseLossBackward0>)\n",
      "Epoch 339 Loss: 115.88797825574875 Val Loss: tensor(113.7628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 340 Loss: 115.70733433961868 Val Loss: tensor(113.5003, grad_fn=<MseLossBackward0>)\n",
      "Epoch 341 Loss: 115.529840528965 Val Loss: tensor(113.4756, grad_fn=<MseLossBackward0>)\n",
      "Epoch 342 Loss: 115.34318882226944 Val Loss: tensor(113.3346, grad_fn=<MseLossBackward0>)\n",
      "Epoch 343 Loss: 115.1632120013237 Val Loss: tensor(113.2220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 344 Loss: 114.97457802295685 Val Loss: tensor(113.1498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 345 Loss: 114.79765886068344 Val Loss: tensor(112.9800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 346 Loss: 114.61212599277496 Val Loss: tensor(112.9471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 347 Loss: 114.43879836797714 Val Loss: tensor(112.7539, grad_fn=<MseLossBackward0>)\n",
      "Epoch 348 Loss: 114.25848561525345 Val Loss: tensor(112.7482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 349 Loss: 114.08912992477417 Val Loss: tensor(112.5415, grad_fn=<MseLossBackward0>)\n",
      "Epoch 350 Loss: 113.91689097881317 Val Loss: tensor(112.5494, grad_fn=<MseLossBackward0>)\n",
      "Epoch 351 Loss: 113.75593078136444 Val Loss: tensor(112.3289, grad_fn=<MseLossBackward0>)\n",
      "Epoch 352 Loss: 113.59905248880386 Val Loss: tensor(112.3596, grad_fn=<MseLossBackward0>)\n",
      "Epoch 353 Loss: 113.45273274183273 Val Loss: tensor(112.1167, grad_fn=<MseLossBackward0>)\n",
      "Epoch 354 Loss: 113.31980085372925 Val Loss: tensor(112.1852, grad_fn=<MseLossBackward0>)\n",
      "Epoch 355 Loss: 113.18525749444962 Val Loss: tensor(111.8869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 356 Loss: 113.06527698040009 Val Loss: tensor(112.0083, grad_fn=<MseLossBackward0>)\n",
      "Epoch 357 Loss: 112.89809906482697 Val Loss: tensor(111.5977, grad_fn=<MseLossBackward0>)\n",
      "Epoch 358 Loss: 112.73672193288803 Val Loss: tensor(111.8121, grad_fn=<MseLossBackward0>)\n",
      "Epoch 359 Loss: 112.48804074525833 Val Loss: tensor(111.2711, grad_fn=<MseLossBackward0>)\n",
      "Epoch 360 Loss: 112.28222507238388 Val Loss: tensor(111.6396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 361 Loss: 112.0172643661499 Val Loss: tensor(110.9837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 362 Loss: 111.83383297920227 Val Loss: tensor(111.4849, grad_fn=<MseLossBackward0>)\n",
      "Epoch 363 Loss: 111.61593347787857 Val Loss: tensor(110.7464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 364 Loss: 111.47451025247574 Val Loss: tensor(111.2910, grad_fn=<MseLossBackward0>)\n",
      "Epoch 365 Loss: 111.3104755282402 Val Loss: tensor(110.5505, grad_fn=<MseLossBackward0>)\n",
      "Epoch 366 Loss: 111.20425701141357 Val Loss: tensor(111.0356, grad_fn=<MseLossBackward0>)\n",
      "Epoch 367 Loss: 111.07901799678802 Val Loss: tensor(110.3800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 368 Loss: 110.97207850217819 Val Loss: tensor(110.6981, grad_fn=<MseLossBackward0>)\n",
      "Epoch 369 Loss: 110.81575959920883 Val Loss: tensor(110.2130, grad_fn=<MseLossBackward0>)\n",
      "Epoch 370 Loss: 110.63427472114563 Val Loss: tensor(110.3134, grad_fn=<MseLossBackward0>)\n",
      "Epoch 371 Loss: 110.40692710876465 Val Loss: tensor(110.0809, grad_fn=<MseLossBackward0>)\n",
      "Epoch 372 Loss: 110.17953503131866 Val Loss: tensor(109.9730, grad_fn=<MseLossBackward0>)\n",
      "Epoch 373 Loss: 109.94837647676468 Val Loss: tensor(109.9711, grad_fn=<MseLossBackward0>)\n",
      "Epoch 374 Loss: 109.74289900064468 Val Loss: tensor(109.6990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 375 Loss: 109.53190416097641 Val Loss: tensor(109.7917, grad_fn=<MseLossBackward0>)\n",
      "Epoch 376 Loss: 109.34933716058731 Val Loss: tensor(109.4826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 377 Loss: 109.15278571844101 Val Loss: tensor(109.5499, grad_fn=<MseLossBackward0>)\n",
      "Epoch 378 Loss: 108.98178523778915 Val Loss: tensor(109.2985, grad_fn=<MseLossBackward0>)\n",
      "Epoch 379 Loss: 108.79387754201889 Val Loss: tensor(109.2992, grad_fn=<MseLossBackward0>)\n",
      "Epoch 380 Loss: 108.62744361162186 Val Loss: tensor(109.1160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 381 Loss: 108.44505727291107 Val Loss: tensor(109.0668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 382 Loss: 108.27891135215759 Val Loss: tensor(108.9204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 383 Loss: 108.10188907384872 Val Loss: tensor(108.8590, grad_fn=<MseLossBackward0>)\n",
      "Epoch 384 Loss: 107.93546289205551 Val Loss: tensor(108.7149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 385 Loss: 107.76265740394592 Val Loss: tensor(108.6729, grad_fn=<MseLossBackward0>)\n",
      "Epoch 386 Loss: 107.5959979891777 Val Loss: tensor(108.5002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 387 Loss: 107.42583626508713 Val Loss: tensor(108.4962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 388 Loss: 107.25782078504562 Val Loss: tensor(108.2754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 389 Loss: 107.08906501531601 Val Loss: tensor(108.3222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 390 Loss: 106.91836529970169 Val Loss: tensor(108.0452, grad_fn=<MseLossBackward0>)\n",
      "Epoch 391 Loss: 106.7498242855072 Val Loss: tensor(108.1482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 392 Loss: 106.57585906982422 Val Loss: tensor(107.8150, grad_fn=<MseLossBackward0>)\n",
      "Epoch 393 Loss: 106.4071477651596 Val Loss: tensor(107.9685, grad_fn=<MseLossBackward0>)\n",
      "Epoch 394 Loss: 106.23156934976578 Val Loss: tensor(107.5931, grad_fn=<MseLossBackward0>)\n",
      "Epoch 395 Loss: 106.06544303894043 Val Loss: tensor(107.7775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 396 Loss: 105.89635682106018 Val Loss: tensor(107.3936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 397 Loss: 105.74612426757812 Val Loss: tensor(107.5788, grad_fn=<MseLossBackward0>)\n",
      "Epoch 398 Loss: 105.60887485742569 Val Loss: tensor(107.2467, grad_fn=<MseLossBackward0>)\n",
      "Epoch 399 Loss: 105.51971131563187 Val Loss: tensor(107.3995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 400 Loss: 105.47867345809937 Val Loss: tensor(107.1987, grad_fn=<MseLossBackward0>)\n",
      "Epoch 401 Loss: 105.5429418683052 Val Loss: tensor(107.2078, grad_fn=<MseLossBackward0>)\n",
      "Epoch 402 Loss: 105.60711842775345 Val Loss: tensor(107.1287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 403 Loss: 105.65571808815002 Val Loss: tensor(106.7828, grad_fn=<MseLossBackward0>)\n",
      "Epoch 404 Loss: 105.3809614777565 Val Loss: tensor(106.8028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 405 Loss: 104.94667387008667 Val Loss: tensor(106.3938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 406 Loss: 104.53665333986282 Val Loss: tensor(106.7186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 407 Loss: 104.1902489066124 Val Loss: tensor(106.1685, grad_fn=<MseLossBackward0>)\n",
      "Epoch 408 Loss: 103.9689245223999 Val Loss: tensor(106.4839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 409 Loss: 103.78911209106445 Val Loss: tensor(105.9713, grad_fn=<MseLossBackward0>)\n",
      "Epoch 410 Loss: 103.64877438545227 Val Loss: tensor(106.1645, grad_fn=<MseLossBackward0>)\n",
      "Epoch 411 Loss: 103.4835324883461 Val Loss: tensor(105.8045, grad_fn=<MseLossBackward0>)\n",
      "Epoch 412 Loss: 103.30029886960983 Val Loss: tensor(105.8593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 413 Loss: 103.0935874581337 Val Loss: tensor(105.6481, grad_fn=<MseLossBackward0>)\n",
      "Epoch 414 Loss: 102.90709602832794 Val Loss: tensor(105.6223, grad_fn=<MseLossBackward0>)\n",
      "Epoch 415 Loss: 102.7170420885086 Val Loss: tensor(105.4911, grad_fn=<MseLossBackward0>)\n",
      "Epoch 416 Loss: 102.54839199781418 Val Loss: tensor(105.3901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 417 Loss: 102.37341731786728 Val Loss: tensor(105.3162, grad_fn=<MseLossBackward0>)\n",
      "Epoch 418 Loss: 102.2105301618576 Val Loss: tensor(105.1573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 419 Loss: 102.03932636976242 Val Loss: tensor(105.1660, grad_fn=<MseLossBackward0>)\n",
      "Epoch 420 Loss: 101.87957388162613 Val Loss: tensor(104.9378, grad_fn=<MseLossBackward0>)\n",
      "Epoch 421 Loss: 101.71263164281845 Val Loss: tensor(105.0286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 422 Loss: 101.55559772253036 Val Loss: tensor(104.7141, grad_fn=<MseLossBackward0>)\n",
      "Epoch 423 Loss: 101.39969974756241 Val Loss: tensor(104.8834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 424 Loss: 101.25024664402008 Val Loss: tensor(104.5112, grad_fn=<MseLossBackward0>)\n",
      "Epoch 425 Loss: 101.11915534734726 Val Loss: tensor(104.7398, grad_fn=<MseLossBackward0>)\n",
      "Epoch 426 Loss: 100.99581629037857 Val Loss: tensor(104.3488, grad_fn=<MseLossBackward0>)\n",
      "Epoch 427 Loss: 100.91376984119415 Val Loss: tensor(104.5724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 428 Loss: 100.83023703098297 Val Loss: tensor(104.2238, grad_fn=<MseLossBackward0>)\n",
      "Epoch 429 Loss: 100.79463392496109 Val Loss: tensor(104.3193, grad_fn=<MseLossBackward0>)\n",
      "Epoch 430 Loss: 100.67656803131104 Val Loss: tensor(104.0662, grad_fn=<MseLossBackward0>)\n",
      "Epoch 431 Loss: 100.56514406204224 Val Loss: tensor(104.0045, grad_fn=<MseLossBackward0>)\n",
      "Epoch 432 Loss: 100.30908769369125 Val Loss: tensor(103.8935, grad_fn=<MseLossBackward0>)\n",
      "Epoch 433 Loss: 100.06857579946518 Val Loss: tensor(103.7264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 434 Loss: 99.79124706983566 Val Loss: tensor(103.7952, grad_fn=<MseLossBackward0>)\n",
      "Epoch 435 Loss: 99.55531013011932 Val Loss: tensor(103.4591, grad_fn=<MseLossBackward0>)\n",
      "Epoch 436 Loss: 99.35256320238113 Val Loss: tensor(103.7121, grad_fn=<MseLossBackward0>)\n",
      "Epoch 437 Loss: 99.16876429319382 Val Loss: tensor(103.1920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 438 Loss: 99.03324192762375 Val Loss: tensor(103.6334, grad_fn=<MseLossBackward0>)\n",
      "Epoch 439 Loss: 98.89643657207489 Val Loss: tensor(102.9565, grad_fn=<MseLossBackward0>)\n",
      "Epoch 440 Loss: 98.81025981903076 Val Loss: tensor(103.5201, grad_fn=<MseLossBackward0>)\n",
      "Epoch 441 Loss: 98.69557178020477 Val Loss: tensor(102.7537, grad_fn=<MseLossBackward0>)\n",
      "Epoch 442 Loss: 98.62652897834778 Val Loss: tensor(103.2626, grad_fn=<MseLossBackward0>)\n",
      "Epoch 443 Loss: 98.46978163719177 Val Loss: tensor(102.5675, grad_fn=<MseLossBackward0>)\n",
      "Epoch 444 Loss: 98.33884423971176 Val Loss: tensor(102.9315, grad_fn=<MseLossBackward0>)\n",
      "Epoch 445 Loss: 98.10911184549332 Val Loss: tensor(102.4443, grad_fn=<MseLossBackward0>)\n",
      "Epoch 446 Loss: 97.91899037361145 Val Loss: tensor(102.6787, grad_fn=<MseLossBackward0>)\n",
      "Epoch 447 Loss: 97.69158917665482 Val Loss: tensor(102.4155, grad_fn=<MseLossBackward0>)\n",
      "Epoch 448 Loss: 97.51958483457565 Val Loss: tensor(102.4590, grad_fn=<MseLossBackward0>)\n",
      "Epoch 449 Loss: 97.33291518688202 Val Loss: tensor(102.3943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 450 Loss: 97.18291062116623 Val Loss: tensor(102.2121, grad_fn=<MseLossBackward0>)\n",
      "Epoch 451 Loss: 97.02169287204742 Val Loss: tensor(102.3376, grad_fn=<MseLossBackward0>)\n",
      "Epoch 452 Loss: 96.8774886727333 Val Loss: tensor(101.9413, grad_fn=<MseLossBackward0>)\n",
      "Epoch 453 Loss: 96.72507762908936 Val Loss: tensor(102.2652, grad_fn=<MseLossBackward0>)\n",
      "Epoch 454 Loss: 96.57546645402908 Val Loss: tensor(101.6497, grad_fn=<MseLossBackward0>)\n",
      "Epoch 455 Loss: 96.42648494243622 Val Loss: tensor(102.1775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 456 Loss: 96.26773631572723 Val Loss: tensor(101.3571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 457 Loss: 96.12384164333344 Val Loss: tensor(102.0522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 458 Loss: 95.95810729265213 Val Loss: tensor(101.1108, grad_fn=<MseLossBackward0>)\n",
      "Epoch 459 Loss: 95.81959468126297 Val Loss: tensor(101.8741, grad_fn=<MseLossBackward0>)\n",
      "Epoch 460 Loss: 95.64661103487015 Val Loss: tensor(100.9339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 461 Loss: 95.5084348320961 Val Loss: tensor(101.6475, grad_fn=<MseLossBackward0>)\n",
      "Epoch 462 Loss: 95.32473635673523 Val Loss: tensor(100.8180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 463 Loss: 95.18419110774994 Val Loss: tensor(101.3964, grad_fn=<MseLossBackward0>)\n",
      "Epoch 464 Loss: 94.99164122343063 Val Loss: tensor(100.7473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 465 Loss: 94.85184097290039 Val Loss: tensor(101.1475, grad_fn=<MseLossBackward0>)\n",
      "Epoch 466 Loss: 94.65898263454437 Val Loss: tensor(100.7097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 467 Loss: 94.52049338817596 Val Loss: tensor(100.8992, grad_fn=<MseLossBackward0>)\n",
      "Epoch 468 Loss: 94.33307856321335 Val Loss: tensor(100.6907, grad_fn=<MseLossBackward0>)\n",
      "Epoch 469 Loss: 94.19197535514832 Val Loss: tensor(100.6336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 470 Loss: 94.01068115234375 Val Loss: tensor(100.6760, grad_fn=<MseLossBackward0>)\n",
      "Epoch 471 Loss: 93.86555516719818 Val Loss: tensor(100.3424, grad_fn=<MseLossBackward0>)\n",
      "Epoch 472 Loss: 93.69386637210846 Val Loss: tensor(100.6642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 473 Loss: 93.54970097541809 Val Loss: tensor(100.0347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 474 Loss: 93.40055882930756 Val Loss: tensor(100.6542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 475 Loss: 93.26982194185257 Val Loss: tensor(99.7413, grad_fn=<MseLossBackward0>)\n",
      "Epoch 476 Loss: 93.16474485397339 Val Loss: tensor(100.6013, grad_fn=<MseLossBackward0>)\n",
      "Epoch 477 Loss: 93.05679821968079 Val Loss: tensor(99.5091, grad_fn=<MseLossBackward0>)\n",
      "Epoch 478 Loss: 92.99674969911575 Val Loss: tensor(100.3934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 479 Loss: 92.87061506509781 Val Loss: tensor(99.3594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 480 Loss: 92.78154516220093 Val Loss: tensor(100.0339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 481 Loss: 92.5620813369751 Val Loss: tensor(99.3138, grad_fn=<MseLossBackward0>)\n",
      "Epoch 482 Loss: 92.39247280359268 Val Loss: tensor(99.7339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 483 Loss: 92.1294407248497 Val Loss: tensor(99.3606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 484 Loss: 91.9525158405304 Val Loss: tensor(99.5334, grad_fn=<MseLossBackward0>)\n",
      "Epoch 485 Loss: 91.72349381446838 Val Loss: tensor(99.3722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 486 Loss: 91.56562346220016 Val Loss: tensor(99.3319, grad_fn=<MseLossBackward0>)\n",
      "Epoch 487 Loss: 91.37346583604813 Val Loss: tensor(99.2913, grad_fn=<MseLossBackward0>)\n",
      "Epoch 488 Loss: 91.22673028707504 Val Loss: tensor(99.0922, grad_fn=<MseLossBackward0>)\n",
      "Epoch 489 Loss: 91.05608987808228 Val Loss: tensor(99.1826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 490 Loss: 90.91310203075409 Val Loss: tensor(98.8251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 491 Loss: 90.7573407292366 Val Loss: tensor(99.1031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 492 Loss: 90.61377894878387 Val Loss: tensor(98.5506, grad_fn=<MseLossBackward0>)\n",
      "Epoch 493 Loss: 90.47639101743698 Val Loss: tensor(99.0371, grad_fn=<MseLossBackward0>)\n",
      "Epoch 494 Loss: 90.33716005086899 Val Loss: tensor(98.3194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 495 Loss: 90.22396755218506 Val Loss: tensor(98.9192, grad_fn=<MseLossBackward0>)\n",
      "Epoch 496 Loss: 90.08891069889069 Val Loss: tensor(98.1762, grad_fn=<MseLossBackward0>)\n",
      "Epoch 497 Loss: 89.99203157424927 Val Loss: tensor(98.6928, grad_fn=<MseLossBackward0>)\n",
      "Epoch 498 Loss: 89.84177279472351 Val Loss: tensor(98.1152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 499 Loss: 89.73214131593704 Val Loss: tensor(98.3942, grad_fn=<MseLossBackward0>)\n",
      "Epoch 500 Loss: 89.54532396793365 Val Loss: tensor(98.1052, grad_fn=<MseLossBackward0>)\n",
      "Epoch 501 Loss: 89.40120381116867 Val Loss: tensor(98.1155, grad_fn=<MseLossBackward0>)\n",
      "Epoch 502 Loss: 89.19766664505005 Val Loss: tensor(98.1348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 503 Loss: 89.03278285264969 Val Loss: tensor(97.8573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 504 Loss: 88.85198646783829 Val Loss: tensor(98.1988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 505 Loss: 88.68905705213547 Val Loss: tensor(97.5720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 506 Loss: 88.54995065927505 Val Loss: tensor(98.2598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 507 Loss: 88.40893191099167 Val Loss: tensor(97.2664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 508 Loss: 88.3207858800888 Val Loss: tensor(98.2483, grad_fn=<MseLossBackward0>)\n",
      "Epoch 509 Loss: 88.20613986253738 Val Loss: tensor(97.0023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 510 Loss: 88.15568333864212 Val Loss: tensor(98.0138, grad_fn=<MseLossBackward0>)\n",
      "Epoch 511 Loss: 88.01474392414093 Val Loss: tensor(96.8514, grad_fn=<MseLossBackward0>)\n",
      "Epoch 512 Loss: 87.93296766281128 Val Loss: tensor(97.5612, grad_fn=<MseLossBackward0>)\n",
      "Epoch 513 Loss: 87.71469408273697 Val Loss: tensor(96.8931, grad_fn=<MseLossBackward0>)\n",
      "Epoch 514 Loss: 87.55649536848068 Val Loss: tensor(97.2029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 515 Loss: 87.32823973894119 Val Loss: tensor(97.0912, grad_fn=<MseLossBackward0>)\n",
      "Epoch 516 Loss: 87.16479635238647 Val Loss: tensor(97.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 517 Loss: 86.96429359912872 Val Loss: tensor(97.2194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 518 Loss: 86.81963813304901 Val Loss: tensor(96.8314, grad_fn=<MseLossBackward0>)\n",
      "Epoch 519 Loss: 86.6351763010025 Val Loss: tensor(97.1478, grad_fn=<MseLossBackward0>)\n",
      "Epoch 520 Loss: 86.4981477856636 Val Loss: tensor(96.6571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 521 Loss: 86.32592797279358 Val Loss: tensor(96.9517, grad_fn=<MseLossBackward0>)\n",
      "Epoch 522 Loss: 86.18578881025314 Val Loss: tensor(96.4816, grad_fn=<MseLossBackward0>)\n",
      "Epoch 523 Loss: 86.02276277542114 Val Loss: tensor(96.7365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 524 Loss: 85.87266570329666 Val Loss: tensor(96.2894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 525 Loss: 85.71449452638626 Val Loss: tensor(96.5560, grad_fn=<MseLossBackward0>)\n",
      "Epoch 526 Loss: 85.5596331357956 Val Loss: tensor(96.1286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 527 Loss: 85.40618789196014 Val Loss: tensor(96.4099, grad_fn=<MseLossBackward0>)\n",
      "Epoch 528 Loss: 85.25381642580032 Val Loss: tensor(96.0389, grad_fn=<MseLossBackward0>)\n",
      "Epoch 529 Loss: 85.10320645570755 Val Loss: tensor(96.2584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 530 Loss: 84.9553884267807 Val Loss: tensor(95.9905, grad_fn=<MseLossBackward0>)\n",
      "Epoch 531 Loss: 84.80373936891556 Val Loss: tensor(96.0670, grad_fn=<MseLossBackward0>)\n",
      "Epoch 532 Loss: 84.66207540035248 Val Loss: tensor(95.9502, grad_fn=<MseLossBackward0>)\n",
      "Epoch 533 Loss: 84.50557559728622 Val Loss: tensor(95.8480, grad_fn=<MseLossBackward0>)\n",
      "Epoch 534 Loss: 84.37704396247864 Val Loss: tensor(95.9194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 535 Loss: 84.21520131826401 Val Loss: tensor(95.6317, grad_fn=<MseLossBackward0>)\n",
      "Epoch 536 Loss: 84.1139287352562 Val Loss: tensor(95.9030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 537 Loss: 83.95714694261551 Val Loss: tensor(95.4300, grad_fn=<MseLossBackward0>)\n",
      "Epoch 538 Loss: 83.90009862184525 Val Loss: tensor(95.8764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 539 Loss: 83.77064734697342 Val Loss: tensor(95.2521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 540 Loss: 83.75911951065063 Val Loss: tensor(95.8026, grad_fn=<MseLossBackward0>)\n",
      "Epoch 541 Loss: 83.65974527597427 Val Loss: tensor(95.1008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 542 Loss: 83.64794301986694 Val Loss: tensor(95.7204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 543 Loss: 83.52231526374817 Val Loss: tensor(94.9550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 544 Loss: 83.46196413040161 Val Loss: tensor(95.7599, grad_fn=<MseLossBackward0>)\n",
      "Epoch 545 Loss: 83.24728834629059 Val Loss: tensor(94.8140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 546 Loss: 83.14368933439255 Val Loss: tensor(95.8605, grad_fn=<MseLossBackward0>)\n",
      "Epoch 547 Loss: 82.8068550825119 Val Loss: tensor(94.7015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 548 Loss: 82.67112100124359 Val Loss: tensor(95.8247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 549 Loss: 82.34686410427094 Val Loss: tensor(94.5467, grad_fn=<MseLossBackward0>)\n",
      "Epoch 550 Loss: 82.2079107761383 Val Loss: tensor(95.4692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 551 Loss: 81.9525066614151 Val Loss: tensor(94.2564, grad_fn=<MseLossBackward0>)\n",
      "Epoch 552 Loss: 81.82905513048172 Val Loss: tensor(95.0449, grad_fn=<MseLossBackward0>)\n",
      "Epoch 553 Loss: 81.60691118240356 Val Loss: tensor(94.1150, grad_fn=<MseLossBackward0>)\n",
      "Epoch 554 Loss: 81.48163956403732 Val Loss: tensor(94.7814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 555 Loss: 81.27891099452972 Val Loss: tensor(94.1117, grad_fn=<MseLossBackward0>)\n",
      "Epoch 556 Loss: 81.14899569749832 Val Loss: tensor(94.5898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 557 Loss: 80.96191692352295 Val Loss: tensor(94.0724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 558 Loss: 80.83226752281189 Val Loss: tensor(94.3892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 559 Loss: 80.66093170642853 Val Loss: tensor(93.9620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 560 Loss: 80.5288850069046 Val Loss: tensor(94.1659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 561 Loss: 80.37366807460785 Val Loss: tensor(93.8285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 562 Loss: 80.23535108566284 Val Loss: tensor(93.9484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 563 Loss: 80.09558415412903 Val Loss: tensor(93.7274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 564 Loss: 79.94862407445908 Val Loss: tensor(93.7331, grad_fn=<MseLossBackward0>)\n",
      "Epoch 565 Loss: 79.82471698522568 Val Loss: tensor(93.6623, grad_fn=<MseLossBackward0>)\n",
      "Epoch 566 Loss: 79.66864329576492 Val Loss: tensor(93.5139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 567 Loss: 79.56505709886551 Val Loss: tensor(93.6235, grad_fn=<MseLossBackward0>)\n",
      "Epoch 568 Loss: 79.40201979875565 Val Loss: tensor(93.2893, grad_fn=<MseLossBackward0>)\n",
      "Epoch 569 Loss: 79.32602173089981 Val Loss: tensor(93.5863, grad_fn=<MseLossBackward0>)\n",
      "Epoch 570 Loss: 79.1651064157486 Val Loss: tensor(93.0512, grad_fn=<MseLossBackward0>)\n",
      "Epoch 571 Loss: 79.12094813585281 Val Loss: tensor(93.5229, grad_fn=<MseLossBackward0>)\n",
      "Epoch 572 Loss: 78.97681301832199 Val Loss: tensor(92.8008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 573 Loss: 78.9568864107132 Val Loss: tensor(93.4111, grad_fn=<MseLossBackward0>)\n",
      "Epoch 574 Loss: 78.83227491378784 Val Loss: tensor(92.5409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 575 Loss: 78.80938076972961 Val Loss: tensor(93.2938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 576 Loss: 78.67570489645004 Val Loss: tensor(92.2890, grad_fn=<MseLossBackward0>)\n",
      "Epoch 577 Loss: 78.62836557626724 Val Loss: tensor(93.1856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 578 Loss: 78.45827341079712 Val Loss: tensor(92.1426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 579 Loss: 78.37282192707062 Val Loss: tensor(92.9601, grad_fn=<MseLossBackward0>)\n",
      "Epoch 580 Loss: 78.12814730405807 Val Loss: tensor(92.1697, grad_fn=<MseLossBackward0>)\n",
      "Epoch 581 Loss: 77.96385914087296 Val Loss: tensor(92.7223, grad_fn=<MseLossBackward0>)\n",
      "Epoch 582 Loss: 77.67611974477768 Val Loss: tensor(92.2201, grad_fn=<MseLossBackward0>)\n",
      "Epoch 583 Loss: 77.49974393844604 Val Loss: tensor(92.5244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 584 Loss: 77.24568951129913 Val Loss: tensor(92.1692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 585 Loss: 77.11382788419724 Val Loss: tensor(92.3228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 586 Loss: 76.91188728809357 Val Loss: tensor(92.1076, grad_fn=<MseLossBackward0>)\n",
      "Epoch 587 Loss: 76.79917216300964 Val Loss: tensor(92.0452, grad_fn=<MseLossBackward0>)\n",
      "Epoch 588 Loss: 76.6281486749649 Val Loss: tensor(92.1129, grad_fn=<MseLossBackward0>)\n",
      "Epoch 589 Loss: 76.50899970531464 Val Loss: tensor(91.7354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 590 Loss: 76.3739482164383 Val Loss: tensor(92.1620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 591 Loss: 76.2508236169815 Val Loss: tensor(91.4165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 592 Loss: 76.17021971940994 Val Loss: tensor(92.1961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 593 Loss: 76.05729085206985 Val Loss: tensor(91.1024, grad_fn=<MseLossBackward0>)\n",
      "Epoch 594 Loss: 76.039501786232 Val Loss: tensor(92.1700, grad_fn=<MseLossBackward0>)\n",
      "Epoch 595 Loss: 75.93834716081619 Val Loss: tensor(90.8158, grad_fn=<MseLossBackward0>)\n",
      "Epoch 596 Loss: 75.95152044296265 Val Loss: tensor(92.0289, grad_fn=<MseLossBackward0>)\n",
      "Epoch 597 Loss: 75.81601506471634 Val Loss: tensor(90.5901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 598 Loss: 75.78148001432419 Val Loss: tensor(91.7824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 599 Loss: 75.55176031589508 Val Loss: tensor(90.5129, grad_fn=<MseLossBackward0>)\n",
      "Epoch 600 Loss: 75.43699717521667 Val Loss: tensor(91.5437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 601 Loss: 75.14447927474976 Val Loss: tensor(90.6020, grad_fn=<MseLossBackward0>)\n",
      "Epoch 602 Loss: 74.9936073422432 Val Loss: tensor(91.3769, grad_fn=<MseLossBackward0>)\n",
      "Epoch 603 Loss: 74.70223957300186 Val Loss: tensor(90.7009, grad_fn=<MseLossBackward0>)\n",
      "Epoch 604 Loss: 74.55929762125015 Val Loss: tensor(91.2222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 605 Loss: 74.30838584899902 Val Loss: tensor(90.6698, grad_fn=<MseLossBackward0>)\n",
      "Epoch 606 Loss: 74.18383723497391 Val Loss: tensor(91.0156, grad_fn=<MseLossBackward0>)\n",
      "Epoch 607 Loss: 73.97920912504196 Val Loss: tensor(90.5594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 608 Loss: 73.863800406456 Val Loss: tensor(90.7609, grad_fn=<MseLossBackward0>)\n",
      "Epoch 609 Loss: 73.68963718414307 Val Loss: tensor(90.4779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 610 Loss: 73.57457208633423 Val Loss: tensor(90.4943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 611 Loss: 73.42239969968796 Val Loss: tensor(90.4346, grad_fn=<MseLossBackward0>)\n",
      "Epoch 612 Loss: 73.30634832382202 Val Loss: tensor(90.2194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 613 Loss: 73.1785860657692 Val Loss: tensor(90.3962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 614 Loss: 73.06490725278854 Val Loss: tensor(89.9402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 615 Loss: 72.96994370222092 Val Loss: tensor(90.3597, grad_fn=<MseLossBackward0>)\n",
      "Epoch 616 Loss: 72.86488544940948 Val Loss: tensor(89.6802, grad_fn=<MseLossBackward0>)\n",
      "Epoch 617 Loss: 72.81328159570694 Val Loss: tensor(90.3330, grad_fn=<MseLossBackward0>)\n",
      "Epoch 618 Loss: 72.72653353214264 Val Loss: tensor(89.4615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 619 Loss: 72.7220071554184 Val Loss: tensor(90.2969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 620 Loss: 72.6560247540474 Val Loss: tensor(89.2979, grad_fn=<MseLossBackward0>)\n",
      "Epoch 621 Loss: 72.66637480258942 Val Loss: tensor(90.1878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 622 Loss: 72.57518869638443 Val Loss: tensor(89.2029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 623 Loss: 72.51584511995316 Val Loss: tensor(89.9709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 624 Loss: 72.31325894594193 Val Loss: tensor(89.1500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 625 Loss: 72.15777629613876 Val Loss: tensor(89.7390, grad_fn=<MseLossBackward0>)\n",
      "Epoch 626 Loss: 71.85776448249817 Val Loss: tensor(89.0510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 627 Loss: 71.6785352230072 Val Loss: tensor(89.5957, grad_fn=<MseLossBackward0>)\n",
      "Epoch 628 Loss: 71.38714224100113 Val Loss: tensor(88.8596, grad_fn=<MseLossBackward0>)\n",
      "Epoch 629 Loss: 71.2229933142662 Val Loss: tensor(89.4694, grad_fn=<MseLossBackward0>)\n",
      "Epoch 630 Loss: 71.00785768032074 Val Loss: tensor(88.5878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 631 Loss: 70.85183203220367 Val Loss: tensor(89.2532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 632 Loss: 70.69447010755539 Val Loss: tensor(88.3446, grad_fn=<MseLossBackward0>)\n",
      "Epoch 633 Loss: 70.53788113594055 Val Loss: tensor(89.0448, grad_fn=<MseLossBackward0>)\n",
      "Epoch 634 Loss: 70.41081297397614 Val Loss: tensor(88.1913, grad_fn=<MseLossBackward0>)\n",
      "Epoch 635 Loss: 70.25170344114304 Val Loss: tensor(88.8797, grad_fn=<MseLossBackward0>)\n",
      "Epoch 636 Loss: 70.13996070623398 Val Loss: tensor(88.0813, grad_fn=<MseLossBackward0>)\n",
      "Epoch 637 Loss: 69.97979724407196 Val Loss: tensor(88.7352, grad_fn=<MseLossBackward0>)\n",
      "Epoch 638 Loss: 69.87456637620926 Val Loss: tensor(87.9628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 639 Loss: 69.71665692329407 Val Loss: tensor(88.5839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 640 Loss: 69.61462140083313 Val Loss: tensor(87.8030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 641 Loss: 69.46086758375168 Val Loss: tensor(88.4316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 642 Loss: 69.36106914281845 Val Loss: tensor(87.6152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 643 Loss: 69.21239984035492 Val Loss: tensor(88.2921, grad_fn=<MseLossBackward0>)\n",
      "Epoch 644 Loss: 69.1141214966774 Val Loss: tensor(87.4264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 645 Loss: 68.97120815515518 Val Loss: tensor(88.1684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 646 Loss: 68.87350541353226 Val Loss: tensor(87.2538, grad_fn=<MseLossBackward0>)\n",
      "Epoch 647 Loss: 68.73706191778183 Val Loss: tensor(88.0541, grad_fn=<MseLossBackward0>)\n",
      "Epoch 648 Loss: 68.63793641328812 Val Loss: tensor(87.0959, grad_fn=<MseLossBackward0>)\n",
      "Epoch 649 Loss: 68.50843721628189 Val Loss: tensor(87.9327, grad_fn=<MseLossBackward0>)\n",
      "Epoch 650 Loss: 68.4049322605133 Val Loss: tensor(86.9485, grad_fn=<MseLossBackward0>)\n",
      "Epoch 651 Loss: 68.28235113620758 Val Loss: tensor(87.7941, grad_fn=<MseLossBackward0>)\n",
      "Epoch 652 Loss: 68.17163455486298 Val Loss: tensor(86.8139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 653 Loss: 68.05565452575684 Val Loss: tensor(87.6362, grad_fn=<MseLossBackward0>)\n",
      "Epoch 654 Loss: 67.93642216920853 Val Loss: tensor(86.6948, grad_fn=<MseLossBackward0>)\n",
      "Epoch 655 Loss: 67.82821887731552 Val Loss: tensor(87.4588, grad_fn=<MseLossBackward0>)\n",
      "Epoch 656 Loss: 67.70241570472717 Val Loss: tensor(86.5943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 657 Loss: 67.60791355371475 Val Loss: tensor(87.2619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 658 Loss: 67.48332351446152 Val Loss: tensor(86.5196, grad_fn=<MseLossBackward0>)\n",
      "Epoch 659 Loss: 67.41765761375427 Val Loss: tensor(87.0406, grad_fn=<MseLossBackward0>)\n",
      "Epoch 660 Loss: 67.31431394815445 Val Loss: tensor(86.4940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 661 Loss: 67.3046128153801 Val Loss: tensor(86.7606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 662 Loss: 67.26279711723328 Val Loss: tensor(86.5796, grad_fn=<MseLossBackward0>)\n",
      "Epoch 663 Loss: 67.31423735618591 Val Loss: tensor(86.2978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 664 Loss: 67.34058558940887 Val Loss: tensor(86.8919, grad_fn=<MseLossBackward0>)\n",
      "Epoch 665 Loss: 67.28611600399017 Val Loss: tensor(85.5053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 666 Loss: 67.18037056922913 Val Loss: tensor(87.2792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 667 Loss: 66.83510607481003 Val Loss: tensor(84.8165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 668 Loss: 66.5505023598671 Val Loss: tensor(87.0199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 669 Loss: 66.19148272275925 Val Loss: tensor(84.7560, grad_fn=<MseLossBackward0>)\n",
      "Epoch 670 Loss: 65.94722718000412 Val Loss: tensor(86.2911, grad_fn=<MseLossBackward0>)\n",
      "Epoch 671 Loss: 65.72469663619995 Val Loss: tensor(84.9827, grad_fn=<MseLossBackward0>)\n",
      "Epoch 672 Loss: 65.58108347654343 Val Loss: tensor(85.7119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 673 Loss: 65.4153493642807 Val Loss: tensor(85.1016, grad_fn=<MseLossBackward0>)\n",
      "Epoch 674 Loss: 65.31948590278625 Val Loss: tensor(85.5143, grad_fn=<MseLossBackward0>)\n",
      "Epoch 675 Loss: 65.15213614702225 Val Loss: tensor(85.1463, grad_fn=<MseLossBackward0>)\n",
      "Epoch 676 Loss: 65.06652617454529 Val Loss: tensor(85.5254, grad_fn=<MseLossBackward0>)\n",
      "Epoch 677 Loss: 64.89382857084274 Val Loss: tensor(85.1521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 678 Loss: 64.80782520771027 Val Loss: tensor(85.4969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 679 Loss: 64.64103269577026 Val Loss: tensor(85.0741, grad_fn=<MseLossBackward0>)\n",
      "Epoch 680 Loss: 64.55243974924088 Val Loss: tensor(85.3491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 681 Loss: 64.3949345946312 Val Loss: tensor(84.9345, grad_fn=<MseLossBackward0>)\n",
      "Epoch 682 Loss: 64.30870866775513 Val Loss: tensor(85.1543, grad_fn=<MseLossBackward0>)\n",
      "Epoch 683 Loss: 64.15836542844772 Val Loss: tensor(84.7775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 684 Loss: 64.07834178209305 Val Loss: tensor(84.9666, grad_fn=<MseLossBackward0>)\n",
      "Epoch 685 Loss: 63.934709548950195 Val Loss: tensor(84.6252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 686 Loss: 63.861506044864655 Val Loss: tensor(84.8007, grad_fn=<MseLossBackward0>)\n",
      "Epoch 687 Loss: 63.722938537597656 Val Loss: tensor(84.4863, grad_fn=<MseLossBackward0>)\n",
      "Epoch 688 Loss: 63.657103419303894 Val Loss: tensor(84.6550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 689 Loss: 63.52088159322739 Val Loss: tensor(84.3563, grad_fn=<MseLossBackward0>)\n",
      "Epoch 690 Loss: 63.46180868148804 Val Loss: tensor(84.5145, grad_fn=<MseLossBackward0>)\n",
      "Epoch 691 Loss: 63.32433384656906 Val Loss: tensor(84.2187, grad_fn=<MseLossBackward0>)\n",
      "Epoch 692 Loss: 63.26843249797821 Val Loss: tensor(84.3808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 693 Loss: 63.12520706653595 Val Loss: tensor(84.0586, grad_fn=<MseLossBackward0>)\n",
      "Epoch 694 Loss: 63.066171646118164 Val Loss: tensor(84.2746, grad_fn=<MseLossBackward0>)\n",
      "Epoch 695 Loss: 62.91323900222778 Val Loss: tensor(83.8666, grad_fn=<MseLossBackward0>)\n",
      "Epoch 696 Loss: 62.8418083190918 Val Loss: tensor(84.2164, grad_fn=<MseLossBackward0>)\n",
      "Epoch 697 Loss: 62.67766261100769 Val Loss: tensor(83.6398, grad_fn=<MseLossBackward0>)\n",
      "Epoch 698 Loss: 62.584439396858215 Val Loss: tensor(84.1981, grad_fn=<MseLossBackward0>)\n",
      "Epoch 699 Loss: 62.413241505622864 Val Loss: tensor(83.3912, grad_fn=<MseLossBackward0>)\n",
      "Epoch 700 Loss: 62.29552274942398 Val Loss: tensor(84.1810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 701 Loss: 62.12970560789108 Val Loss: tensor(83.1471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 702 Loss: 61.995397329330444 Val Loss: tensor(84.1212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 703 Loss: 61.84874230623245 Val Loss: tensor(82.9269, grad_fn=<MseLossBackward0>)\n",
      "Epoch 704 Loss: 61.7097207903862 Val Loss: tensor(84.0028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 705 Loss: 61.587555170059204 Val Loss: tensor(82.7331, grad_fn=<MseLossBackward0>)\n",
      "Epoch 706 Loss: 61.45212405920029 Val Loss: tensor(83.8400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 707 Loss: 61.35063457489014 Val Loss: tensor(82.5585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 708 Loss: 61.22248411178589 Val Loss: tensor(83.6528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 709 Loss: 61.13359844684601 Val Loss: tensor(82.3954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 710 Loss: 61.01370394229889 Val Loss: tensor(83.4487, grad_fn=<MseLossBackward0>)\n",
      "Epoch 711 Loss: 60.92829293012619 Val Loss: tensor(82.2419, grad_fn=<MseLossBackward0>)\n",
      "Epoch 712 Loss: 60.81516891717911 Val Loss: tensor(83.2239, grad_fn=<MseLossBackward0>)\n",
      "Epoch 713 Loss: 60.72429233789444 Val Loss: tensor(82.1031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 714 Loss: 60.61251848936081 Val Loss: tensor(82.9790, grad_fn=<MseLossBackward0>)\n",
      "Epoch 715 Loss: 60.510204434394836 Val Loss: tensor(81.9882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 716 Loss: 60.390829145908356 Val Loss: tensor(82.7313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 717 Loss: 60.27881306409836 Val Loss: tensor(81.9054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 718 Loss: 60.144631028175354 Val Loss: tensor(82.5094, grad_fn=<MseLossBackward0>)\n",
      "Epoch 719 Loss: 60.03277897834778 Val Loss: tensor(81.8545, grad_fn=<MseLossBackward0>)\n",
      "Epoch 720 Loss: 59.88401663303375 Val Loss: tensor(82.3336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 721 Loss: 59.7804029583931 Val Loss: tensor(81.8247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 722 Loss: 59.62140452861786 Val Loss: tensor(82.2051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 723 Loss: 59.52440923452377 Val Loss: tensor(81.7988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 724 Loss: 59.357367157936096 Val Loss: tensor(82.1074, grad_fn=<MseLossBackward0>)\n",
      "Epoch 725 Loss: 59.26075881719589 Val Loss: tensor(81.7615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 726 Loss: 59.08696949481964 Val Loss: tensor(82.0156, grad_fn=<MseLossBackward0>)\n",
      "Epoch 727 Loss: 58.98858517408371 Val Loss: tensor(81.7031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 728 Loss: 58.812469363212585 Val Loss: tensor(81.9103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 729 Loss: 58.71469730138779 Val Loss: tensor(81.6204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 730 Loss: 58.54226142168045 Val Loss: tensor(81.7891, grad_fn=<MseLossBackward0>)\n",
      "Epoch 731 Loss: 58.44780778884888 Val Loss: tensor(81.5145, grad_fn=<MseLossBackward0>)\n",
      "Epoch 732 Loss: 58.282793164253235 Val Loss: tensor(81.6634, grad_fn=<MseLossBackward0>)\n",
      "Epoch 733 Loss: 58.1923942565918 Val Loss: tensor(81.3900, grad_fn=<MseLossBackward0>)\n",
      "Epoch 734 Loss: 58.03548139333725 Val Loss: tensor(81.5475, grad_fn=<MseLossBackward0>)\n",
      "Epoch 735 Loss: 57.94847482442856 Val Loss: tensor(81.2500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 736 Loss: 57.79893207550049 Val Loss: tensor(81.4501, grad_fn=<MseLossBackward0>)\n",
      "Epoch 737 Loss: 57.71416890621185 Val Loss: tensor(81.0958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 738 Loss: 57.57147425413132 Val Loss: tensor(81.3743, grad_fn=<MseLossBackward0>)\n",
      "Epoch 739 Loss: 57.48769587278366 Val Loss: tensor(80.9280, grad_fn=<MseLossBackward0>)\n",
      "Epoch 740 Loss: 57.3521928191185 Val Loss: tensor(81.3191, grad_fn=<MseLossBackward0>)\n",
      "Epoch 741 Loss: 57.26819121837616 Val Loss: tensor(80.7491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 742 Loss: 57.141693353652954 Val Loss: tensor(81.2785, grad_fn=<MseLossBackward0>)\n",
      "Epoch 743 Loss: 57.056930124759674 Val Loss: tensor(80.5662, grad_fn=<MseLossBackward0>)\n",
      "Epoch 744 Loss: 56.943476378917694 Val Loss: tensor(81.2375, grad_fn=<MseLossBackward0>)\n",
      "Epoch 745 Loss: 56.8597936630249 Val Loss: tensor(80.3922, grad_fn=<MseLossBackward0>)\n",
      "Epoch 746 Loss: 56.76737296581268 Val Loss: tensor(81.1703, grad_fn=<MseLossBackward0>)\n",
      "Epoch 747 Loss: 56.69110196828842 Val Loss: tensor(80.2412, grad_fn=<MseLossBackward0>)\n",
      "Epoch 748 Loss: 56.63233429193497 Val Loss: tensor(81.0534, grad_fn=<MseLossBackward0>)\n",
      "Epoch 749 Loss: 56.56996411085129 Val Loss: tensor(80.1106, grad_fn=<MseLossBackward0>)\n",
      "Epoch 750 Loss: 56.54812961816788 Val Loss: tensor(80.9028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 751 Loss: 56.47268611192703 Val Loss: tensor(79.9752, grad_fn=<MseLossBackward0>)\n",
      "Epoch 752 Loss: 56.43137139081955 Val Loss: tensor(80.7568, grad_fn=<MseLossBackward0>)\n",
      "Epoch 753 Loss: 56.27420634031296 Val Loss: tensor(79.8581, grad_fn=<MseLossBackward0>)\n",
      "Epoch 754 Loss: 56.14113360643387 Val Loss: tensor(80.5520, grad_fn=<MseLossBackward0>)\n",
      "Epoch 755 Loss: 55.9373841881752 Val Loss: tensor(79.8338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 756 Loss: 55.7673379778862 Val Loss: tensor(80.2409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 757 Loss: 55.61188006401062 Val Loss: tensor(79.8677, grad_fn=<MseLossBackward0>)\n",
      "Epoch 758 Loss: 55.46283060312271 Val Loss: tensor(79.9334, grad_fn=<MseLossBackward0>)\n",
      "Epoch 759 Loss: 55.36886042356491 Val Loss: tensor(79.9139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 760 Loss: 55.237964391708374 Val Loss: tensor(79.7324, grad_fn=<MseLossBackward0>)\n",
      "Epoch 761 Loss: 55.186967968940735 Val Loss: tensor(79.9743, grad_fn=<MseLossBackward0>)\n",
      "Epoch 762 Loss: 55.0578288435936 Val Loss: tensor(79.6507, grad_fn=<MseLossBackward0>)\n",
      "Epoch 763 Loss: 55.03205984830856 Val Loss: tensor(80.0281, grad_fn=<MseLossBackward0>)\n",
      "Epoch 764 Loss: 54.88849484920502 Val Loss: tensor(79.6558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 765 Loss: 54.864666223526 Val Loss: tensor(80.0379, grad_fn=<MseLossBackward0>)\n",
      "Epoch 766 Loss: 54.69157838821411 Val Loss: tensor(79.7122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 767 Loss: 54.65559130907059 Val Loss: tensor(79.9896, grad_fn=<MseLossBackward0>)\n",
      "Epoch 768 Loss: 54.457625925540924 Val Loss: tensor(79.7595, grad_fn=<MseLossBackward0>)\n",
      "Epoch 769 Loss: 54.42012983560562 Val Loss: tensor(79.8641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 770 Loss: 54.221086740493774 Val Loss: tensor(79.7371, grad_fn=<MseLossBackward0>)\n",
      "Epoch 771 Loss: 54.19916892051697 Val Loss: tensor(79.6664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 772 Loss: 54.01869308948517 Val Loss: tensor(79.6381, grad_fn=<MseLossBackward0>)\n",
      "Epoch 773 Loss: 54.017268776893616 Val Loss: tensor(79.4452, grad_fn=<MseLossBackward0>)\n",
      "Epoch 774 Loss: 53.85128676891327 Val Loss: tensor(79.4865, grad_fn=<MseLossBackward0>)\n",
      "Epoch 775 Loss: 53.856009125709534 Val Loss: tensor(79.2708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 776 Loss: 53.680240988731384 Val Loss: tensor(79.2886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 777 Loss: 53.66362339258194 Val Loss: tensor(79.1959, grad_fn=<MseLossBackward0>)\n",
      "Epoch 778 Loss: 53.46057766675949 Val Loss: tensor(79.0573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 779 Loss: 53.413068413734436 Val Loss: tensor(79.2204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 780 Loss: 53.199209451675415 Val Loss: tensor(78.8248, grad_fn=<MseLossBackward0>)\n",
      "Epoch 781 Loss: 53.14156496524811 Val Loss: tensor(79.3065, grad_fn=<MseLossBackward0>)\n",
      "Epoch 782 Loss: 52.94820314645767 Val Loss: tensor(78.6079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 783 Loss: 52.90659445524216 Val Loss: tensor(79.4096, grad_fn=<MseLossBackward0>)\n",
      "Epoch 784 Loss: 52.74500775337219 Val Loss: tensor(78.4168, grad_fn=<MseLossBackward0>)\n",
      "Epoch 785 Loss: 52.7384535074234 Val Loss: tensor(79.4634, grad_fn=<MseLossBackward0>)\n",
      "Epoch 786 Loss: 52.589328944683075 Val Loss: tensor(78.2608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 787 Loss: 52.62221986055374 Val Loss: tensor(79.3936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 788 Loss: 52.45222598314285 Val Loss: tensor(78.1241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 789 Loss: 52.511274099349976 Val Loss: tensor(79.1915, grad_fn=<MseLossBackward0>)\n",
      "Epoch 790 Loss: 52.30393236875534 Val Loss: tensor(77.9876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 791 Loss: 52.36400371789932 Val Loss: tensor(78.9047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 792 Loss: 52.123333156108856 Val Loss: tensor(77.8606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 793 Loss: 52.15284538269043 Val Loss: tensor(78.5705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 794 Loss: 51.886547923088074 Val Loss: tensor(77.7755, grad_fn=<MseLossBackward0>)\n",
      "Epoch 795 Loss: 51.856918931007385 Val Loss: tensor(78.2229, grad_fn=<MseLossBackward0>)\n",
      "Epoch 796 Loss: 51.584019005298615 Val Loss: tensor(77.7534, grad_fn=<MseLossBackward0>)\n",
      "Epoch 797 Loss: 51.495987951755524 Val Loss: tensor(77.9195, grad_fn=<MseLossBackward0>)\n",
      "Epoch 798 Loss: 51.2532924413681 Val Loss: tensor(77.7615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 799 Loss: 51.14362549781799 Val Loss: tensor(77.7026, grad_fn=<MseLossBackward0>)\n",
      "Epoch 800 Loss: 50.94523733854294 Val Loss: tensor(77.7365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 801 Loss: 50.842477440834045 Val Loss: tensor(77.5737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 802 Loss: 50.671383917331696 Val Loss: tensor(77.6603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 803 Loss: 50.58046621084213 Val Loss: tensor(77.4998, grad_fn=<MseLossBackward0>)\n",
      "Epoch 804 Loss: 50.419920444488525 Val Loss: tensor(77.5629, grad_fn=<MseLossBackward0>)\n",
      "Epoch 805 Loss: 50.33896195888519 Val Loss: tensor(77.4469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 806 Loss: 50.18383026123047 Val Loss: tensor(77.4742, grad_fn=<MseLossBackward0>)\n",
      "Epoch 807 Loss: 50.10935777425766 Val Loss: tensor(77.4005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 808 Loss: 49.959656953811646 Val Loss: tensor(77.3958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 809 Loss: 49.887531757354736 Val Loss: tensor(77.3550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 810 Loss: 49.74378538131714 Val Loss: tensor(77.3181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 811 Loss: 49.67091649770737 Val Loss: tensor(77.3063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 812 Loss: 49.53351837396622 Val Loss: tensor(77.2399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 813 Loss: 49.45826131105423 Val Loss: tensor(77.2519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 814 Loss: 49.32756423950195 Val Loss: tensor(77.1650, grad_fn=<MseLossBackward0>)\n",
      "Epoch 815 Loss: 49.2491859793663 Val Loss: tensor(77.1936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 816 Loss: 49.125643730163574 Val Loss: tensor(77.0948, grad_fn=<MseLossBackward0>)\n",
      "Epoch 817 Loss: 49.04378205537796 Val Loss: tensor(77.1371, grad_fn=<MseLossBackward0>)\n",
      "Epoch 818 Loss: 48.927847266197205 Val Loss: tensor(77.0271, grad_fn=<MseLossBackward0>)\n",
      "Epoch 819 Loss: 48.84205859899521 Val Loss: tensor(77.0871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 820 Loss: 48.73441815376282 Val Loss: tensor(76.9600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 821 Loss: 48.643888890743256 Val Loss: tensor(77.0462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 822 Loss: 48.54521161317825 Val Loss: tensor(76.8928, grad_fn=<MseLossBackward0>)\n",
      "Epoch 823 Loss: 48.44881093502045 Val Loss: tensor(77.0154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 824 Loss: 48.359511494636536 Val Loss: tensor(76.8251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 825 Loss: 48.25590020418167 Val Loss: tensor(76.9952, grad_fn=<MseLossBackward0>)\n",
      "Epoch 826 Loss: 48.17590481042862 Val Loss: tensor(76.7562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 827 Loss: 48.0638222694397 Val Loss: tensor(76.9857, grad_fn=<MseLossBackward0>)\n",
      "Epoch 828 Loss: 47.992510080337524 Val Loss: tensor(76.6851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 829 Loss: 47.87109446525574 Val Loss: tensor(76.9865, grad_fn=<MseLossBackward0>)\n",
      "Epoch 830 Loss: 47.80764412879944 Val Loss: tensor(76.6103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 831 Loss: 47.676956713199615 Val Loss: tensor(76.9956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 832 Loss: 47.62055718898773 Val Loss: tensor(76.5303, grad_fn=<MseLossBackward0>)\n",
      "Epoch 833 Loss: 47.482086539268494 Val Loss: tensor(77.0100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 834 Loss: 47.432226836681366 Val Loss: tensor(76.4422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 835 Loss: 47.2890510559082 Val Loss: tensor(77.0255, grad_fn=<MseLossBackward0>)\n",
      "Epoch 836 Loss: 47.24547743797302 Val Loss: tensor(76.3424, grad_fn=<MseLossBackward0>)\n",
      "Epoch 837 Loss: 47.1016144156456 Val Loss: tensor(77.0361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 838 Loss: 47.063632905483246 Val Loss: tensor(76.2269, grad_fn=<MseLossBackward0>)\n",
      "Epoch 839 Loss: 46.922691822052 Val Loss: tensor(77.0323, grad_fn=<MseLossBackward0>)\n",
      "Epoch 840 Loss: 46.887631475925446 Val Loss: tensor(76.0943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 841 Loss: 46.75060486793518 Val Loss: tensor(76.9991, grad_fn=<MseLossBackward0>)\n",
      "Epoch 842 Loss: 46.711594462394714 Val Loss: tensor(75.9502, grad_fn=<MseLossBackward0>)\n",
      "Epoch 843 Loss: 46.57538902759552 Val Loss: tensor(76.9218, grad_fn=<MseLossBackward0>)\n",
      "Epoch 844 Loss: 46.5223987698555 Val Loss: tensor(75.8090, grad_fn=<MseLossBackward0>)\n",
      "Epoch 845 Loss: 46.38261127471924 Val Loss: tensor(76.7988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 846 Loss: 46.309778571128845 Val Loss: tensor(75.6882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 847 Loss: 46.16804814338684 Val Loss: tensor(76.6445, grad_fn=<MseLossBackward0>)\n",
      "Epoch 848 Loss: 46.08055400848389 Val Loss: tensor(75.5982, grad_fn=<MseLossBackward0>)\n",
      "Epoch 849 Loss: 45.946584701538086 Val Loss: tensor(76.4753, grad_fn=<MseLossBackward0>)\n",
      "Epoch 850 Loss: 45.853456914424896 Val Loss: tensor(75.5444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 851 Loss: 45.73587852716446 Val Loss: tensor(76.2996, grad_fn=<MseLossBackward0>)\n",
      "Epoch 852 Loss: 45.63836711645126 Val Loss: tensor(75.5360, grad_fn=<MseLossBackward0>)\n",
      "Epoch 853 Loss: 45.537130653858185 Val Loss: tensor(76.1213, grad_fn=<MseLossBackward0>)\n",
      "Epoch 854 Loss: 45.42921805381775 Val Loss: tensor(75.5809, grad_fn=<MseLossBackward0>)\n",
      "Epoch 855 Loss: 45.33875346183777 Val Loss: tensor(75.9450, grad_fn=<MseLossBackward0>)\n",
      "Epoch 856 Loss: 45.215139985084534 Val Loss: tensor(75.6720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 857 Loss: 45.13177931308746 Val Loss: tensor(75.7752, grad_fn=<MseLossBackward0>)\n",
      "Epoch 858 Loss: 44.99383705854416 Val Loss: tensor(75.7835, grad_fn=<MseLossBackward0>)\n",
      "Epoch 859 Loss: 44.91819125413895 Val Loss: tensor(75.6178, grad_fn=<MseLossBackward0>)\n",
      "Epoch 860 Loss: 44.772914350032806 Val Loss: tensor(75.8810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 861 Loss: 44.70465987920761 Val Loss: tensor(75.4812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 862 Loss: 44.56011343002319 Val Loss: tensor(75.9364, grad_fn=<MseLossBackward0>)\n",
      "Epoch 863 Loss: 44.49430936574936 Val Loss: tensor(75.3734, grad_fn=<MseLossBackward0>)\n",
      "Epoch 864 Loss: 44.35665565729141 Val Loss: tensor(75.9357, grad_fn=<MseLossBackward0>)\n",
      "Epoch 865 Loss: 44.287093222141266 Val Loss: tensor(75.2986, grad_fn=<MseLossBackward0>)\n",
      "Epoch 866 Loss: 44.160032123327255 Val Loss: tensor(75.8762, grad_fn=<MseLossBackward0>)\n",
      "Epoch 867 Loss: 44.08343416452408 Val Loss: tensor(75.2595, grad_fn=<MseLossBackward0>)\n",
      "Epoch 868 Loss: 43.96849298477173 Val Loss: tensor(75.7602, grad_fn=<MseLossBackward0>)\n",
      "Epoch 869 Loss: 43.88492530584335 Val Loss: tensor(75.2620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 870 Loss: 43.78241717815399 Val Loss: tensor(75.5910, grad_fn=<MseLossBackward0>)\n",
      "Epoch 871 Loss: 43.693528950214386 Val Loss: tensor(75.3177, grad_fn=<MseLossBackward0>)\n",
      "Epoch 872 Loss: 43.60530638694763 Val Loss: tensor(75.3793, grad_fn=<MseLossBackward0>)\n",
      "Epoch 873 Loss: 43.514515817165375 Val Loss: tensor(75.4444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 874 Loss: 43.44674128293991 Val Loss: tensor(75.1511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 875 Loss: 43.36542594432831 Val Loss: tensor(75.6556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 876 Loss: 43.32745182514191 Val Loss: tensor(74.9550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 877 Loss: 43.2851665019989 Val Loss: tensor(75.9198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 878 Loss: 43.27179032564163 Val Loss: tensor(74.8461, grad_fn=<MseLossBackward0>)\n",
      "Epoch 879 Loss: 43.30346292257309 Val Loss: tensor(76.1152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 880 Loss: 43.25723868608475 Val Loss: tensor(74.8152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 881 Loss: 43.34912270307541 Val Loss: tensor(76.1332, grad_fn=<MseLossBackward0>)\n",
      "Epoch 882 Loss: 43.185204803943634 Val Loss: tensor(74.7555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 883 Loss: 43.27255439758301 Val Loss: tensor(75.9810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 884 Loss: 42.98679196834564 Val Loss: tensor(74.6062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 885 Loss: 43.017810702323914 Val Loss: tensor(75.7240, grad_fn=<MseLossBackward0>)\n",
      "Epoch 886 Loss: 42.70680516958237 Val Loss: tensor(74.4655, grad_fn=<MseLossBackward0>)\n",
      "Epoch 887 Loss: 42.69670897722244 Val Loss: tensor(75.4519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 888 Loss: 42.4635426402092 Val Loss: tensor(74.4389, grad_fn=<MseLossBackward0>)\n",
      "Epoch 889 Loss: 42.47389590740204 Val Loss: tensor(75.1799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 890 Loss: 42.33522832393646 Val Loss: tensor(74.4978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 891 Loss: 42.38197237253189 Val Loss: tensor(74.9193, grad_fn=<MseLossBackward0>)\n",
      "Epoch 892 Loss: 42.271955609321594 Val Loss: tensor(74.5934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 893 Loss: 42.25852358341217 Val Loss: tensor(74.7165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 894 Loss: 42.11591577529907 Val Loss: tensor(74.7503, grad_fn=<MseLossBackward0>)\n",
      "Epoch 895 Loss: 41.96265512704849 Val Loss: tensor(74.6050, grad_fn=<MseLossBackward0>)\n",
      "Epoch 896 Loss: 41.825130879879 Val Loss: tensor(74.8921, grad_fn=<MseLossBackward0>)\n",
      "Epoch 897 Loss: 41.6251100897789 Val Loss: tensor(74.5478, grad_fn=<MseLossBackward0>)\n",
      "Epoch 898 Loss: 41.53969359397888 Val Loss: tensor(74.8687, grad_fn=<MseLossBackward0>)\n",
      "Epoch 899 Loss: 41.36928188800812 Val Loss: tensor(74.4624, grad_fn=<MseLossBackward0>)\n",
      "Epoch 900 Loss: 41.32236194610596 Val Loss: tensor(74.7320, grad_fn=<MseLossBackward0>)\n",
      "Epoch 901 Loss: 41.16162860393524 Val Loss: tensor(74.3255, grad_fn=<MseLossBackward0>)\n",
      "Epoch 902 Loss: 41.134037017822266 Val Loss: tensor(74.6107, grad_fn=<MseLossBackward0>)\n",
      "Epoch 903 Loss: 40.96441215276718 Val Loss: tensor(74.1851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 904 Loss: 40.937130987644196 Val Loss: tensor(74.5393, grad_fn=<MseLossBackward0>)\n",
      "Epoch 905 Loss: 40.77135270833969 Val Loss: tensor(74.0771, grad_fn=<MseLossBackward0>)\n",
      "Epoch 906 Loss: 40.728350937366486 Val Loss: tensor(74.4839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 907 Loss: 40.58040863275528 Val Loss: tensor(73.9949, grad_fn=<MseLossBackward0>)\n",
      "Epoch 908 Loss: 40.51256549358368 Val Loss: tensor(74.4393, grad_fn=<MseLossBackward0>)\n",
      "Epoch 909 Loss: 40.390600860118866 Val Loss: tensor(73.9336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 910 Loss: 40.30264210700989 Val Loss: tensor(74.4045, grad_fn=<MseLossBackward0>)\n",
      "Epoch 911 Loss: 40.2135374546051 Val Loss: tensor(73.9139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 912 Loss: 40.117369651794434 Val Loss: tensor(74.3642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 913 Loss: 40.06278085708618 Val Loss: tensor(73.9545, grad_fn=<MseLossBackward0>)\n",
      "Epoch 914 Loss: 39.96579474210739 Val Loss: tensor(74.3055, grad_fn=<MseLossBackward0>)\n",
      "Epoch 915 Loss: 39.937952756881714 Val Loss: tensor(74.0461, grad_fn=<MseLossBackward0>)\n",
      "Epoch 916 Loss: 39.84161961078644 Val Loss: tensor(74.2341, grad_fn=<MseLossBackward0>)\n",
      "Epoch 917 Loss: 39.83061498403549 Val Loss: tensor(74.1555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 918 Loss: 39.734162747859955 Val Loss: tensor(74.1615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 919 Loss: 39.73496115207672 Val Loss: tensor(74.2526, grad_fn=<MseLossBackward0>)\n",
      "Epoch 920 Loss: 39.635054647922516 Val Loss: tensor(74.0876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 921 Loss: 39.6450469493866 Val Loss: tensor(74.3230, grad_fn=<MseLossBackward0>)\n",
      "Epoch 922 Loss: 39.531548380851746 Val Loss: tensor(74.0042, grad_fn=<MseLossBackward0>)\n",
      "Epoch 923 Loss: 39.546278297901154 Val Loss: tensor(74.3672, grad_fn=<MseLossBackward0>)\n",
      "Epoch 924 Loss: 39.407048523426056 Val Loss: tensor(73.9068, grad_fn=<MseLossBackward0>)\n",
      "Epoch 925 Loss: 39.41946601867676 Val Loss: tensor(74.3818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 926 Loss: 39.24946165084839 Val Loss: tensor(73.7989, grad_fn=<MseLossBackward0>)\n",
      "Epoch 927 Loss: 39.250217854976654 Val Loss: tensor(74.3498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 928 Loss: 39.05288100242615 Val Loss: tensor(73.6896, grad_fn=<MseLossBackward0>)\n",
      "Epoch 929 Loss: 39.031326949596405 Val Loss: tensor(74.2530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 930 Loss: 38.81694179773331 Val Loss: tensor(73.5937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 931 Loss: 38.76688593626022 Val Loss: tensor(74.0950, grad_fn=<MseLossBackward0>)\n",
      "Epoch 932 Loss: 38.550734758377075 Val Loss: tensor(73.5237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 933 Loss: 38.47587031126022 Val Loss: tensor(73.9037, grad_fn=<MseLossBackward0>)\n",
      "Epoch 934 Loss: 38.2723827958107 Val Loss: tensor(73.4771, grad_fn=<MseLossBackward0>)\n",
      "Epoch 935 Loss: 38.18360900878906 Val Loss: tensor(73.7148, grad_fn=<MseLossBackward0>)\n",
      "Epoch 936 Loss: 38.000814735889435 Val Loss: tensor(73.4391, grad_fn=<MseLossBackward0>)\n",
      "Epoch 937 Loss: 37.90871089696884 Val Loss: tensor(73.5532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 938 Loss: 37.748289823532104 Val Loss: tensor(73.3997, grad_fn=<MseLossBackward0>)\n",
      "Epoch 939 Loss: 37.659081161022186 Val Loss: tensor(73.4275, grad_fn=<MseLossBackward0>)\n",
      "Epoch 940 Loss: 37.5184810757637 Val Loss: tensor(73.3601, grad_fn=<MseLossBackward0>)\n",
      "Epoch 941 Loss: 37.434762597084045 Val Loss: tensor(73.3361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 942 Loss: 37.30945760011673 Val Loss: tensor(73.3265, grad_fn=<MseLossBackward0>)\n",
      "Epoch 943 Loss: 37.232058465480804 Val Loss: tensor(73.2731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 944 Loss: 37.11740928888321 Val Loss: tensor(73.3027, grad_fn=<MseLossBackward0>)\n",
      "Epoch 945 Loss: 37.046510487794876 Val Loss: tensor(73.2299, grad_fn=<MseLossBackward0>)\n",
      "Epoch 946 Loss: 36.939016819000244 Val Loss: tensor(73.2889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 947 Loss: 36.87483033537865 Val Loss: tensor(73.1978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 948 Loss: 36.77235534787178 Val Loss: tensor(73.2847, grad_fn=<MseLossBackward0>)\n",
      "Epoch 949 Loss: 36.71549832820892 Val Loss: tensor(73.1696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 950 Loss: 36.61722815036774 Val Loss: tensor(73.2897, grad_fn=<MseLossBackward0>)\n",
      "Epoch 951 Loss: 36.568705439567566 Val Loss: tensor(73.1409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 952 Loss: 36.474933952093124 Val Loss: tensor(73.3044, grad_fn=<MseLossBackward0>)\n",
      "Epoch 953 Loss: 36.436086386442184 Val Loss: tensor(73.1094, grad_fn=<MseLossBackward0>)\n",
      "Epoch 954 Loss: 36.34815630316734 Val Loss: tensor(73.3283, grad_fn=<MseLossBackward0>)\n",
      "Epoch 955 Loss: 36.320205330848694 Val Loss: tensor(73.0743, grad_fn=<MseLossBackward0>)\n",
      "Epoch 956 Loss: 36.240769386291504 Val Loss: tensor(73.3599, grad_fn=<MseLossBackward0>)\n",
      "Epoch 957 Loss: 36.22422868013382 Val Loss: tensor(73.0353, grad_fn=<MseLossBackward0>)\n",
      "Epoch 958 Loss: 36.157556772232056 Val Loss: tensor(73.3945, grad_fn=<MseLossBackward0>)\n",
      "Epoch 959 Loss: 36.15117031335831 Val Loss: tensor(72.9933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 960 Loss: 36.103758335113525 Val Loss: tensor(73.4217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 961 Loss: 36.10313856601715 Val Loss: tensor(72.9510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 962 Loss: 36.08376443386078 Val Loss: tensor(73.4227, grad_fn=<MseLossBackward0>)\n",
      "Epoch 963 Loss: 36.079255282878876 Val Loss: tensor(72.9150, grad_fn=<MseLossBackward0>)\n",
      "Epoch 964 Loss: 36.095053911209106 Val Loss: tensor(73.3683, grad_fn=<MseLossBackward0>)\n",
      "Epoch 965 Loss: 36.06779545545578 Val Loss: tensor(72.8984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 966 Loss: 36.1057453751564 Val Loss: tensor(73.2314, grad_fn=<MseLossBackward0>)\n",
      "Epoch 967 Loss: 36.023971021175385 Val Loss: tensor(72.9237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 968 Loss: 36.02186578512192 Val Loss: tensor(73.0271, grad_fn=<MseLossBackward0>)\n",
      "Epoch 969 Loss: 35.85090583562851 Val Loss: tensor(73.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 970 Loss: 35.739319026470184 Val Loss: tensor(72.8244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 971 Loss: 35.47659385204315 Val Loss: tensor(73.0689, grad_fn=<MseLossBackward0>)\n",
      "Epoch 972 Loss: 35.28805762529373 Val Loss: tensor(72.6571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 973 Loss: 35.010004699230194 Val Loss: tensor(72.9821, grad_fn=<MseLossBackward0>)\n",
      "Epoch 974 Loss: 34.83197605609894 Val Loss: tensor(72.5060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 975 Loss: 34.620987236499786 Val Loss: tensor(72.7523, grad_fn=<MseLossBackward0>)\n",
      "Epoch 976 Loss: 34.48872524499893 Val Loss: tensor(72.3917, grad_fn=<MseLossBackward0>)\n",
      "Epoch 977 Loss: 34.34708687663078 Val Loss: tensor(72.5224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 978 Loss: 34.24701535701752 Val Loss: tensor(72.3337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 979 Loss: 34.14265736937523 Val Loss: tensor(72.3949, grad_fn=<MseLossBackward0>)\n",
      "Epoch 980 Loss: 34.053485095500946 Val Loss: tensor(72.3222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 981 Loss: 33.96466335654259 Val Loss: tensor(72.3622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 982 Loss: 33.87856015563011 Val Loss: tensor(72.3266, grad_fn=<MseLossBackward0>)\n",
      "Epoch 983 Loss: 33.795551002025604 Val Loss: tensor(72.3578, grad_fn=<MseLossBackward0>)\n",
      "Epoch 984 Loss: 33.711243122816086 Val Loss: tensor(72.3242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 985 Loss: 33.630052626132965 Val Loss: tensor(72.3465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 986 Loss: 33.546870470047 Val Loss: tensor(72.3198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 987 Loss: 33.46640092134476 Val Loss: tensor(72.3357, grad_fn=<MseLossBackward0>)\n",
      "Epoch 988 Loss: 33.38362693786621 Val Loss: tensor(72.3213, grad_fn=<MseLossBackward0>)\n",
      "Epoch 989 Loss: 33.30384710431099 Val Loss: tensor(72.3309, grad_fn=<MseLossBackward0>)\n",
      "Epoch 990 Loss: 33.22110864520073 Val Loss: tensor(72.3268, grad_fn=<MseLossBackward0>)\n",
      "Epoch 991 Loss: 33.142202854156494 Val Loss: tensor(72.3271, grad_fn=<MseLossBackward0>)\n",
      "Epoch 992 Loss: 33.05940666794777 Val Loss: tensor(72.3309, grad_fn=<MseLossBackward0>)\n",
      "Epoch 993 Loss: 32.981395214796066 Val Loss: tensor(72.3212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 994 Loss: 32.89856278896332 Val Loss: tensor(72.3332, grad_fn=<MseLossBackward0>)\n",
      "Epoch 995 Loss: 32.82135230302811 Val Loss: tensor(72.3153, grad_fn=<MseLossBackward0>)\n",
      "Epoch 996 Loss: 32.73854207992554 Val Loss: tensor(72.3350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 997 Loss: 32.66210380196571 Val Loss: tensor(72.3100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 998 Loss: 32.579500675201416 Val Loss: tensor(72.3359, grad_fn=<MseLossBackward0>)\n",
      "Epoch 999 Loss: 32.50393310189247 Val Loss: tensor(72.3043, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1000 Loss: 32.421834379434586 Val Loss: tensor(72.3355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1001 Loss: 32.347354620695114 Val Loss: tensor(72.2977, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1002 Loss: 32.266267508268356 Val Loss: tensor(72.3333, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1003 Loss: 32.19327861070633 Val Loss: tensor(72.2914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1004 Loss: 32.114411026239395 Val Loss: tensor(72.3289, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1005 Loss: 32.04380676150322 Val Loss: tensor(72.2881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1006 Loss: 31.96950051188469 Val Loss: tensor(72.3215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1007 Loss: 31.903308898210526 Val Loss: tensor(72.2911, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1008 Loss: 31.838061928749084 Val Loss: tensor(72.3092, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1009 Loss: 31.780555546283722 Val Loss: tensor(72.3054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1010 Loss: 31.732495605945587 Val Loss: tensor(72.2905, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1011 Loss: 31.69181215763092 Val Loss: tensor(72.3385, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1012 Loss: 31.67474514245987 Val Loss: tensor(72.2626, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1013 Loss: 31.664957761764526 Val Loss: tensor(72.4028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1014 Loss: 31.698829293251038 Val Loss: tensor(72.2180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1015 Loss: 31.740903973579407 Val Loss: tensor(72.5161, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1016 Loss: 31.842763543128967 Val Loss: tensor(72.1372, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1017 Loss: 31.957084894180298 Val Loss: tensor(72.6957, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1018 Loss: 32.097391188144684 Val Loss: tensor(72.0099, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1019 Loss: 32.27186179161072 Val Loss: tensor(72.9405, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1020 Loss: 32.29629063606262 Val Loss: tensor(71.9057, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1021 Loss: 32.45352840423584 Val Loss: tensor(73.1607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1022 Loss: 32.17761147022247 Val Loss: tensor(71.8559, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1023 Loss: 32.19046038389206 Val Loss: tensor(73.0288, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1024 Loss: 31.660514175891876 Val Loss: tensor(71.7202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1025 Loss: 31.465655207633972 Val Loss: tensor(72.5074, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1026 Loss: 31.009287774562836 Val Loss: tensor(71.6137, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1027 Loss: 30.82371175289154 Val Loss: tensor(71.9487, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1028 Loss: 30.600997507572174 Val Loss: tensor(71.6402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1029 Loss: 30.4999817609787 Val Loss: tensor(71.5824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1030 Loss: 30.40905797481537 Val Loss: tensor(71.7882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1031 Loss: 30.332102715969086 Val Loss: tensor(71.4035, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1032 Loss: 30.291558742523193 Val Loss: tensor(71.9117, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1033 Loss: 30.20488953590393 Val Loss: tensor(71.3194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1034 Loss: 30.170281291007996 Val Loss: tensor(71.9042, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1035 Loss: 30.07436031103134 Val Loss: tensor(71.2899, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1036 Loss: 30.037084341049194 Val Loss: tensor(71.8077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1037 Loss: 29.938524782657623 Val Loss: tensor(71.3104, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1038 Loss: 29.90924561023712 Val Loss: tensor(71.7051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1039 Loss: 29.820255637168884 Val Loss: tensor(71.3479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1040 Loss: 29.80330115556717 Val Loss: tensor(71.6366, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1041 Loss: 29.730833649635315 Val Loss: tensor(71.3791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1042 Loss: 29.72323578596115 Val Loss: tensor(71.5986, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1043 Loss: 29.666717290878296 Val Loss: tensor(71.3901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1044 Loss: 29.66220724582672 Val Loss: tensor(71.5698, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1045 Loss: 29.613894879817963 Val Loss: tensor(71.3880, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1046 Loss: 29.599882781505585 Val Loss: tensor(71.5396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1047 Loss: 29.54675316810608 Val Loss: tensor(71.3737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1048 Loss: 29.504755318164825 Val Loss: tensor(71.4981, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1049 Loss: 29.430642247200012 Val Loss: tensor(71.3430, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1050 Loss: 29.344891607761383 Val Loss: tensor(71.4429, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1051 Loss: 29.24299842119217 Val Loss: tensor(71.2927, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1052 Loss: 29.115275740623474 Val Loss: tensor(71.3822, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1053 Loss: 28.994760751724243 Val Loss: tensor(71.2237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1054 Loss: 28.844485819339752 Val Loss: tensor(71.3224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1055 Loss: 28.723519146442413 Val Loss: tensor(71.1462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1056 Loss: 28.573729872703552 Val Loss: tensor(71.2618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1057 Loss: 28.465474545955658 Val Loss: tensor(71.0793, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1058 Loss: 28.330956637859344 Val Loss: tensor(71.1990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1059 Loss: 28.238483369350433 Val Loss: tensor(71.0379, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1060 Loss: 28.123069643974304 Val Loss: tensor(71.1400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1061 Loss: 28.043169021606445 Val Loss: tensor(71.0239, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1062 Loss: 27.94407159090042 Val Loss: tensor(71.0915, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1063 Loss: 27.872859299182892 Val Loss: tensor(71.0290, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1064 Loss: 27.785909175872803 Val Loss: tensor(71.0525, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1065 Loss: 27.72129762172699 Val Loss: tensor(71.0446, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1066 Loss: 27.64360922574997 Val Loss: tensor(71.0176, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1067 Loss: 27.58549988269806 Val Loss: tensor(71.0662, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1068 Loss: 27.515635669231415 Val Loss: tensor(70.9822, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1069 Loss: 27.465616464614868 Val Loss: tensor(71.0925, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1070 Loss: 27.403186559677124 Val Loss: tensor(70.9442, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1071 Loss: 27.36436951160431 Val Loss: tensor(71.1227, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1072 Loss: 27.308959901332855 Val Loss: tensor(70.9041, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1073 Loss: 27.28521853685379 Val Loss: tensor(71.1534, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1074 Loss: 27.234642207622528 Val Loss: tensor(70.8645, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1075 Loss: 27.22848552465439 Val Loss: tensor(71.1762, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1076 Loss: 27.175686538219452 Val Loss: tensor(70.8308, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1077 Loss: 27.18378621339798 Val Loss: tensor(71.1790, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1078 Loss: 27.11459481716156 Val Loss: tensor(70.8096, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1079 Loss: 27.122972011566162 Val Loss: tensor(71.1519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1080 Loss: 27.020544052124023 Val Loss: tensor(70.8039, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1081 Loss: 27.00775271654129 Val Loss: tensor(71.0945, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1082 Loss: 26.867720186710358 Val Loss: tensor(70.8062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1083 Loss: 26.821546971797943 Val Loss: tensor(71.0152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1084 Loss: 26.662453770637512 Val Loss: tensor(70.8001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1085 Loss: 26.591577529907227 Val Loss: tensor(70.9250, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1086 Loss: 26.44105976819992 Val Loss: tensor(70.7753, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1087 Loss: 26.36340057849884 Val Loss: tensor(70.8368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1088 Loss: 26.237287521362305 Val Loss: tensor(70.7388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1089 Loss: 26.16362690925598 Val Loss: tensor(70.7642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1090 Loss: 26.062433540821075 Val Loss: tensor(70.7051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1091 Loss: 25.995045959949493 Val Loss: tensor(70.7144, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1092 Loss: 25.91281533241272 Val Loss: tensor(70.6815, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1093 Loss: 25.851028442382812 Val Loss: tensor(70.6851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1094 Loss: 25.781885385513306 Val Loss: tensor(70.6646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1095 Loss: 25.725382447242737 Val Loss: tensor(70.6694, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1096 Loss: 25.66539216041565 Val Loss: tensor(70.6485, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1097 Loss: 25.614701330661774 Val Loss: tensor(70.6622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1098 Loss: 25.561374962329865 Val Loss: tensor(70.6307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1099 Loss: 25.51771432161331 Val Loss: tensor(70.6605, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1100 Loss: 25.46895533800125 Val Loss: tensor(70.6118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1101 Loss: 25.4332115650177 Val Loss: tensor(70.6604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1102 Loss: 25.386169910430908 Val Loss: tensor(70.5937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1103 Loss: 25.357473373413086 Val Loss: tensor(70.6566, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1104 Loss: 25.307655096054077 Val Loss: tensor(70.5780, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1105 Loss: 25.28180754184723 Val Loss: tensor(70.6437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1106 Loss: 25.223110258579254 Val Loss: tensor(70.5658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1107 Loss: 25.192441642284393 Val Loss: tensor(70.6187, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1108 Loss: 25.119353532791138 Val Loss: tensor(70.5562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1109 Loss: 25.075970351696014 Val Loss: tensor(70.5820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1110 Loss: 24.9879047870636 Val Loss: tensor(70.5456, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1111 Loss: 24.928915083408356 Val Loss: tensor(70.5363, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1112 Loss: 24.83241504430771 Val Loss: tensor(70.5307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1113 Loss: 24.76185178756714 Val Loss: tensor(70.4855, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1114 Loss: 24.667507469654083 Val Loss: tensor(70.5117, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1115 Loss: 24.592579185962677 Val Loss: tensor(70.4354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1116 Loss: 24.50903731584549 Val Loss: tensor(70.4936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1117 Loss: 24.435043811798096 Val Loss: tensor(70.3925, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1118 Loss: 24.36583971977234 Val Loss: tensor(70.4820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1119 Loss: 24.294526517391205 Val Loss: tensor(70.3611, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1120 Loss: 24.238819301128387 Val Loss: tensor(70.4791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1121 Loss: 24.16953408718109 Val Loss: tensor(70.3413, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1122 Loss: 24.123992025852203 Val Loss: tensor(70.4814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1123 Loss: 24.055068492889404 Val Loss: tensor(70.3300, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1124 Loss: 24.01543015241623 Val Loss: tensor(70.4840, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1125 Loss: 23.945157051086426 Val Loss: tensor(70.3229, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1126 Loss: 23.907362282276154 Val Loss: tensor(70.4827, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1127 Loss: 23.834727346897125 Val Loss: tensor(70.3159, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1128 Loss: 23.795773923397064 Val Loss: tensor(70.4768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1129 Loss: 23.720993757247925 Val Loss: tensor(70.3055, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1130 Loss: 23.67961621284485 Val Loss: tensor(70.4677, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1131 Loss: 23.604085683822632 Val Loss: tensor(70.2899, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1132 Loss: 23.56083381175995 Val Loss: tensor(70.4582, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1133 Loss: 23.486502587795258 Val Loss: tensor(70.2689, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1134 Loss: 23.443064630031586 Val Loss: tensor(70.4515, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1135 Loss: 23.371577262878418 Val Loss: tensor(70.2438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1136 Loss: 23.329843819141388 Val Loss: tensor(70.4492, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1137 Loss: 23.261911809444427 Val Loss: tensor(70.2159, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1138 Loss: 23.22339141368866 Val Loss: tensor(70.4518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1139 Loss: 23.158569753170013 Val Loss: tensor(70.1866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1140 Loss: 23.12402993440628 Val Loss: tensor(70.4578, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1141 Loss: 23.060885548591614 Val Loss: tensor(70.1565, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1142 Loss: 23.03044867515564 Val Loss: tensor(70.4645, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1143 Loss: 22.966887772083282 Val Loss: tensor(70.1259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1144 Loss: 22.939886569976807 Val Loss: tensor(70.4685, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1145 Loss: 22.87349569797516 Val Loss: tensor(70.0943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1146 Loss: 22.848571002483368 Val Loss: tensor(70.4655, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1147 Loss: 22.77702444791794 Val Loss: tensor(70.0606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1148 Loss: 22.751959919929504 Val Loss: tensor(70.4513, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1149 Loss: 22.6735320687294 Val Loss: tensor(70.0238, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1150 Loss: 22.645261228084564 Val Loss: tensor(70.4230, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1151 Loss: 22.559130549430847 Val Loss: tensor(69.9831, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1152 Loss: 22.524021089076996 Val Loss: tensor(70.3795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1153 Loss: 22.430652678012848 Val Loss: tensor(69.9398, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1154 Loss: 22.385249197483063 Val Loss: tensor(70.3225, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1155 Loss: 22.286523044109344 Val Loss: tensor(69.8966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1156 Loss: 22.228692948818207 Val Loss: tensor(70.2566, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1157 Loss: 22.127851247787476 Val Loss: tensor(69.8574, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1158 Loss: 22.057895600795746 Val Loss: tensor(70.1879, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1159 Loss: 21.958954632282257 Val Loss: tensor(69.8261, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1160 Loss: 21.879746973514557 Val Loss: tensor(70.1220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1161 Loss: 21.786509692668915 Val Loss: tensor(69.8047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1162 Loss: 21.702587068080902 Val Loss: tensor(70.0628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1163 Loss: 21.61756521463394 Val Loss: tensor(69.7938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1164 Loss: 21.533460915088654 Val Loss: tensor(70.0113, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1165 Loss: 21.457586586475372 Val Loss: tensor(69.7927, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1166 Loss: 21.376744151115417 Val Loss: tensor(69.9669, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1167 Loss: 21.3098002076149 Val Loss: tensor(69.8002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1168 Loss: 21.234312891960144 Val Loss: tensor(69.9267, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1169 Loss: 21.175818026065826 Val Loss: tensor(69.8157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1170 Loss: 21.10676670074463 Val Loss: tensor(69.8874, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1171 Loss: 21.056689083576202 Val Loss: tensor(69.8388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1172 Loss: 20.994905829429626 Val Loss: tensor(69.8453, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1173 Loss: 20.954107880592346 Val Loss: tensor(69.8697, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1174 Loss: 20.900225281715393 Val Loss: tensor(69.7976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1175 Loss: 20.87054693698883 Val Loss: tensor(69.9078, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1176 Loss: 20.82442730665207 Val Loss: tensor(69.7432, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1177 Loss: 20.807833969593048 Val Loss: tensor(69.9511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1178 Loss: 20.766781330108643 Val Loss: tensor(69.6844, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1179 Loss: 20.763185560703278 Val Loss: tensor(69.9935, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1180 Loss: 20.718735992908478 Val Loss: tensor(69.6269, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1181 Loss: 20.722816050052643 Val Loss: tensor(70.0248, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1182 Loss: 20.65980988740921 Val Loss: tensor(69.5779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1183 Loss: 20.659605026245117 Val Loss: tensor(70.0347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1184 Loss: 20.563318848609924 Val Loss: tensor(69.5402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1185 Loss: 20.545922100543976 Val Loss: tensor(70.0179, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1186 Loss: 20.416363060474396 Val Loss: tensor(69.5100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1187 Loss: 20.377997756004333 Val Loss: tensor(69.9757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1188 Loss: 20.234005749225616 Val Loss: tensor(69.4829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1189 Loss: 20.182051062583923 Val Loss: tensor(69.9151, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1190 Loss: 20.04589557647705 Val Loss: tensor(69.4598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1191 Loss: 19.990569353103638 Val Loss: tensor(69.8485, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1192 Loss: 19.87340557575226 Val Loss: tensor(69.4438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1193 Loss: 19.82080715894699 Val Loss: tensor(69.7904, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1194 Loss: 19.722940653562546 Val Loss: tensor(69.4338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1195 Loss: 19.674962013959885 Val Loss: tensor(69.7495, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1196 Loss: 19.592314958572388 Val Loss: tensor(69.4241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1197 Loss: 19.54902932047844 Val Loss: tensor(69.7261, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1198 Loss: 19.47721293568611 Val Loss: tensor(69.4101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1199 Loss: 19.4384828209877 Val Loss: tensor(69.7162, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1200 Loss: 19.37418806552887 Val Loss: tensor(69.3912, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1201 Loss: 19.340506851673126 Val Loss: tensor(69.7164, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1202 Loss: 19.2816721200943 Val Loss: tensor(69.3704, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1203 Loss: 19.254334270954132 Val Loss: tensor(69.7242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1204 Loss: 19.199699252843857 Val Loss: tensor(69.3503, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1205 Loss: 19.180452466011047 Val Loss: tensor(69.7362, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1206 Loss: 19.128777146339417 Val Loss: tensor(69.3325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1207 Loss: 19.119156897068024 Val Loss: tensor(69.7474, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1208 Loss: 19.068181216716766 Val Loss: tensor(69.3187, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1209 Loss: 19.06819188594818 Val Loss: tensor(69.7518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1210 Loss: 19.01351809501648 Val Loss: tensor(69.3102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1211 Loss: 19.020056009292603 Val Loss: tensor(69.7429, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1212 Loss: 18.955139636993408 Val Loss: tensor(69.3071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1213 Loss: 18.961174368858337 Val Loss: tensor(69.7156, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1214 Loss: 18.879705548286438 Val Loss: tensor(69.3071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1215 Loss: 18.87590777873993 Val Loss: tensor(69.6691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1216 Loss: 18.77667623758316 Val Loss: tensor(69.3065, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1217 Loss: 18.756249845027924 Val Loss: tensor(69.6071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1218 Loss: 18.64614826440811 Val Loss: tensor(69.3028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1219 Loss: 18.609051644802094 Val Loss: tensor(69.5371, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1220 Loss: 18.49984246492386 Val Loss: tensor(69.2970, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1221 Loss: 18.452095091342926 Val Loss: tensor(69.4669, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1222 Loss: 18.35367751121521 Val Loss: tensor(69.2926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1223 Loss: 18.302659392356873 Val Loss: tensor(69.4031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1224 Loss: 18.219489097595215 Val Loss: tensor(69.2933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1225 Loss: 18.170284807682037 Val Loss: tensor(69.3490, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1226 Loss: 18.102371275424957 Val Loss: tensor(69.3001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1227 Loss: 18.05734521150589 Val Loss: tensor(69.3053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1228 Loss: 18.002938449382782 Val Loss: tensor(69.3125, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1229 Loss: 17.962631702423096 Val Loss: tensor(69.2704, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1230 Loss: 17.919844269752502 Val Loss: tensor(69.3287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1231 Loss: 17.884084045886993 Val Loss: tensor(69.2433, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1232 Loss: 17.85134655237198 Val Loss: tensor(69.3468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1233 Loss: 17.819470047950745 Val Loss: tensor(69.2231, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1234 Loss: 17.795259058475494 Val Loss: tensor(69.3649, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1235 Loss: 17.766005039215088 Val Loss: tensor(69.2093, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1236 Loss: 17.74834007024765 Val Loss: tensor(69.3808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1237 Loss: 17.71942490339279 Val Loss: tensor(69.2021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1238 Loss: 17.705655217170715 Val Loss: tensor(69.3923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1239 Loss: 17.673629879951477 Val Loss: tensor(69.2010, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1240 Loss: 17.660636723041534 Val Loss: tensor(69.3976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1241 Loss: 17.62172770500183 Val Loss: tensor(69.2054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1242 Loss: 17.607277154922485 Val Loss: tensor(69.3972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1243 Loss: 17.559134423732758 Val Loss: tensor(69.2135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1244 Loss: 17.543581783771515 Val Loss: tensor(69.3933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1245 Loss: 17.486957788467407 Val Loss: tensor(69.2222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1246 Loss: 17.473920106887817 Val Loss: tensor(69.3906, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1247 Loss: 17.412154853343964 Val Loss: tensor(69.2270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1248 Loss: 17.406756281852722 Val Loss: tensor(69.3920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1249 Loss: 17.34299063682556 Val Loss: tensor(69.2235, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1250 Loss: 17.348499298095703 Val Loss: tensor(69.3966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1251 Loss: 17.282828867435455 Val Loss: tensor(69.2067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1252 Loss: 17.2973330616951 Val Loss: tensor(69.3962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1253 Loss: 17.22628480195999 Val Loss: tensor(69.1715, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1254 Loss: 17.241246819496155 Val Loss: tensor(69.3788, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1255 Loss: 17.160067319869995 Val Loss: tensor(69.1151, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1256 Loss: 17.161749720573425 Val Loss: tensor(69.3348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1257 Loss: 17.067509323358536 Val Loss: tensor(69.0411, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1258 Loss: 17.041815489530563 Val Loss: tensor(69.2646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1259 Loss: 16.935681968927383 Val Loss: tensor(68.9628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1260 Loss: 16.875223845243454 Val Loss: tensor(69.1805, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1261 Loss: 16.763278633356094 Val Loss: tensor(68.8963, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1262 Loss: 16.672258913517 Val Loss: tensor(69.1005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1263 Loss: 16.563874006271362 Val Loss: tensor(68.8519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1264 Loss: 16.45635735988617 Val Loss: tensor(69.0374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1265 Loss: 16.359966069459915 Val Loss: tensor(68.8302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1266 Loss: 16.252052158117294 Val Loss: tensor(68.9936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1267 Loss: 16.171393394470215 Val Loss: tensor(68.8273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1268 Loss: 16.074005603790283 Val Loss: tensor(68.9658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1269 Loss: 16.007803320884705 Val Loss: tensor(68.8392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1270 Loss: 15.924601554870605 Val Loss: tensor(68.9494, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1271 Loss: 15.869064539670944 Val Loss: tensor(68.8615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1272 Loss: 15.79889577627182 Val Loss: tensor(68.9404, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1273 Loss: 15.75036695599556 Val Loss: tensor(68.8900, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1274 Loss: 15.690528959035873 Val Loss: tensor(68.9341, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1275 Loss: 15.646731227636337 Val Loss: tensor(68.9212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1276 Loss: 15.5949065387249 Val Loss: tensor(68.9262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1277 Loss: 15.555068790912628 Val Loss: tensor(68.9540, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1278 Loss: 15.510040998458862 Val Loss: tensor(68.9134, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1279 Loss: 15.474651426076889 Val Loss: tensor(68.9896, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1280 Loss: 15.436360239982605 Val Loss: tensor(68.8942, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1281 Loss: 15.407099276781082 Val Loss: tensor(69.0295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1282 Loss: 15.3764129281044 Val Loss: tensor(68.8675, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1283 Loss: 15.356134116649628 Val Loss: tensor(69.0746, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1284 Loss: 15.33419445157051 Val Loss: tensor(68.8336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1285 Loss: 15.326598286628723 Val Loss: tensor(69.1228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1286 Loss: 15.31324216723442 Val Loss: tensor(68.7946, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1287 Loss: 15.321150839328766 Val Loss: tensor(69.1672, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1288 Loss: 15.31127941608429 Val Loss: tensor(68.7560, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1289 Loss: 15.33255261182785 Val Loss: tensor(69.1944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1290 Loss: 15.31196415424347 Val Loss: tensor(68.7250, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1291 Loss: 15.334486544132233 Val Loss: tensor(69.1894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1292 Loss: 15.281972885131836 Val Loss: tensor(68.7059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1293 Loss: 15.285343527793884 Val Loss: tensor(69.1465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1294 Loss: 15.188843071460724 Val Loss: tensor(68.6955, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1295 Loss: 15.159039556980133 Val Loss: tensor(69.0747, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1296 Loss: 15.032531797885895 Val Loss: tensor(68.6851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1297 Loss: 14.97531533241272 Val Loss: tensor(68.9895, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1298 Loss: 14.84874576330185 Val Loss: tensor(68.6722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1299 Loss: 14.782195389270782 Val Loss: tensor(68.9060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1300 Loss: 14.676086753606796 Val Loss: tensor(68.6622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1301 Loss: 14.613907039165497 Val Loss: tensor(68.8382, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1302 Loss: 14.531826078891754 Val Loss: tensor(68.6601, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1303 Loss: 14.478189051151276 Val Loss: tensor(68.7944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1304 Loss: 14.415245115756989 Val Loss: tensor(68.6655, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1305 Loss: 14.369083166122437 Val Loss: tensor(68.7726, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1306 Loss: 14.319516748189926 Val Loss: tensor(68.6737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1307 Loss: 14.278810292482376 Val Loss: tensor(68.7642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1308 Loss: 14.238700985908508 Val Loss: tensor(68.6813, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1309 Loss: 14.20216566324234 Val Loss: tensor(68.7624, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1310 Loss: 14.169534415006638 Val Loss: tensor(68.6876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1311 Loss: 14.137116879224777 Val Loss: tensor(68.7657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1312 Loss: 14.11138778924942 Val Loss: tensor(68.6940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1313 Loss: 14.084032952785492 Val Loss: tensor(68.7740, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1314 Loss: 14.065380334854126 Val Loss: tensor(68.7008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1315 Loss: 14.044617176055908 Val Loss: tensor(68.7856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1316 Loss: 14.033376455307007 Val Loss: tensor(68.7072, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1317 Loss: 14.020517259836197 Val Loss: tensor(68.7966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1318 Loss: 14.01609742641449 Val Loss: tensor(68.7135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1319 Loss: 14.011109471321106 Val Loss: tensor(68.8013, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1320 Loss: 14.010577499866486 Val Loss: tensor(68.7198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1321 Loss: 14.010636031627655 Val Loss: tensor(68.7930, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1322 Loss: 14.0077263712883 Val Loss: tensor(68.7264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1323 Loss: 14.006040036678314 Val Loss: tensor(68.7663, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1324 Loss: 13.992236018180847 Val Loss: tensor(68.7331, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1325 Loss: 13.979711174964905 Val Loss: tensor(68.7205, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1326 Loss: 13.948725521564484 Val Loss: tensor(68.7387, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1327 Loss: 13.91910320520401 Val Loss: tensor(68.6616, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1328 Loss: 13.872134745121002 Val Loss: tensor(68.7427, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1329 Loss: 13.826556324958801 Val Loss: tensor(68.5993, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1330 Loss: 13.772936761379242 Val Loss: tensor(68.7459, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1331 Loss: 13.717942714691162 Val Loss: tensor(68.5441, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1332 Loss: 13.669807195663452 Val Loss: tensor(68.7502, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1333 Loss: 13.61000943183899 Val Loss: tensor(68.5038, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1334 Loss: 13.575599312782288 Val Loss: tensor(68.7563, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1335 Loss: 13.509605407714844 Val Loss: tensor(68.4816, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1336 Loss: 13.490503787994385 Val Loss: tensor(68.7616, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1337 Loss: 13.414004534482956 Val Loss: tensor(68.4720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1338 Loss: 13.40598252415657 Val Loss: tensor(68.7613, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1339 Loss: 13.317734628915787 Val Loss: tensor(68.4637, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1340 Loss: 13.313757538795471 Val Loss: tensor(68.7527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1341 Loss: 13.218315720558167 Val Loss: tensor(68.4479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1342 Loss: 13.211705029010773 Val Loss: tensor(68.7381, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1343 Loss: 13.116719216108322 Val Loss: tensor(68.4249, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1344 Loss: 13.103342682123184 Val Loss: tensor(68.7230, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1345 Loss: 13.014977306127548 Val Loss: tensor(68.4009, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1346 Loss: 12.993448078632355 Val Loss: tensor(68.7112, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1347 Loss: 12.914033979177475 Val Loss: tensor(68.3820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1348 Loss: 12.88478809595108 Val Loss: tensor(68.7025, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1349 Loss: 12.813762038946152 Val Loss: tensor(68.3712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1350 Loss: 12.778278231620789 Val Loss: tensor(68.6949, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1351 Loss: 12.713974803686142 Val Loss: tensor(68.3685, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1352 Loss: 12.674402475357056 Val Loss: tensor(68.6867, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1353 Loss: 12.61538678407669 Val Loss: tensor(68.3730, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1354 Loss: 12.573999166488647 Val Loss: tensor(68.6777, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1355 Loss: 12.519381910562515 Val Loss: tensor(68.3826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1356 Loss: 12.47817787528038 Val Loss: tensor(68.6686, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1357 Loss: 12.4274280667305 Val Loss: tensor(68.3954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1358 Loss: 12.387888073921204 Val Loss: tensor(68.6596, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1359 Loss: 12.340702444314957 Val Loss: tensor(68.4102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1360 Loss: 12.30381315946579 Val Loss: tensor(68.6503, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1361 Loss: 12.26006093621254 Val Loss: tensor(68.4267, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1362 Loss: 12.226491272449493 Val Loss: tensor(68.6397, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1363 Loss: 12.186237096786499 Val Loss: tensor(68.4459, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1364 Loss: 12.156631588935852 Val Loss: tensor(68.6260, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1365 Loss: 12.120261669158936 Val Loss: tensor(68.4686, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1366 Loss: 12.095273733139038 Val Loss: tensor(68.6068, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1367 Loss: 12.0635926425457 Val Loss: tensor(68.4962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1368 Loss: 12.04402482509613 Val Loss: tensor(68.5795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1369 Loss: 12.018322676420212 Val Loss: tensor(68.5296, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1370 Loss: 12.004834473133087 Val Loss: tensor(68.5416, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1371 Loss: 11.986737608909607 Val Loss: tensor(68.5690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1372 Loss: 11.979127317667007 Val Loss: tensor(68.4920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1373 Loss: 11.969919979572296 Val Loss: tensor(68.6123, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1374 Loss: 11.96521258354187 Val Loss: tensor(68.4325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1375 Loss: 11.963866114616394 Val Loss: tensor(68.6526, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1376 Loss: 11.953566312789917 Val Loss: tensor(68.3695, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1377 Loss: 11.953871250152588 Val Loss: tensor(68.6785, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1378 Loss: 11.922952890396118 Val Loss: tensor(68.3126, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1379 Loss: 11.912904739379883 Val Loss: tensor(68.6800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1380 Loss: 11.84735631942749 Val Loss: tensor(68.2697, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1381 Loss: 11.816463232040405 Val Loss: tensor(68.6567, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1382 Loss: 11.718226253986359 Val Loss: tensor(68.2407, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1383 Loss: 11.668100893497467 Val Loss: tensor(68.6177, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1384 Loss: 11.557772278785706 Val Loss: tensor(68.2210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1385 Loss: 11.500638782978058 Val Loss: tensor(68.5737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1386 Loss: 11.399794280529022 Val Loss: tensor(68.2087, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1387 Loss: 11.346293687820435 Val Loss: tensor(68.5342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1388 Loss: 11.263484209775925 Val Loss: tensor(68.2050, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1389 Loss: 11.217271119356155 Val Loss: tensor(68.5077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1390 Loss: 11.15092048048973 Val Loss: tensor(68.2089, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1391 Loss: 11.111437320709229 Val Loss: tensor(68.4979, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1392 Loss: 11.057044446468353 Val Loss: tensor(68.2159, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1393 Loss: 11.02269658446312 Val Loss: tensor(68.5022, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1394 Loss: 10.976386785507202 Val Loss: tensor(68.2214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1395 Loss: 10.945837527513504 Val Loss: tensor(68.5160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1396 Loss: 10.90495365858078 Val Loss: tensor(68.2244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1397 Loss: 10.877591967582703 Val Loss: tensor(68.5370, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1398 Loss: 10.840487360954285 Val Loss: tensor(68.2263, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1399 Loss: 10.816604733467102 Val Loss: tensor(68.5656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1400 Loss: 10.782412350177765 Val Loss: tensor(68.2284, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1401 Loss: 10.763025224208832 Val Loss: tensor(68.6025, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1402 Loss: 10.731387555599213 Val Loss: tensor(68.2305, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1403 Loss: 10.718307942152023 Val Loss: tensor(68.6479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1404 Loss: 10.689050495624542 Val Loss: tensor(68.2324, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1405 Loss: 10.684820145368576 Val Loss: tensor(68.7016, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1406 Loss: 10.657659590244293 Val Loss: tensor(68.2352, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1407 Loss: 10.665484338998795 Val Loss: tensor(68.7626, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1408 Loss: 10.639514654874802 Val Loss: tensor(68.2410, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1409 Loss: 10.6628997027874 Val Loss: tensor(68.8283, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1410 Loss: 10.635923147201538 Val Loss: tensor(68.2519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1411 Loss: 10.677675396203995 Val Loss: tensor(68.8928, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1412 Loss: 10.645708590745926 Val Loss: tensor(68.2686, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1413 Loss: 10.706478714942932 Val Loss: tensor(68.9457, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1414 Loss: 10.664049834012985 Val Loss: tensor(68.2886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1415 Loss: 10.74025896191597 Val Loss: tensor(68.9738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1416 Loss: 10.682469427585602 Val Loss: tensor(68.3072, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1417 Loss: 10.764798253774643 Val Loss: tensor(68.9635, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1418 Loss: 10.690106272697449 Val Loss: tensor(68.3195, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1419 Loss: 10.7637919485569 Val Loss: tensor(68.9067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1420 Loss: 10.675418555736542 Val Loss: tensor(68.3251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1421 Loss: 10.72439792752266 Val Loss: tensor(68.8039, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1422 Loss: 10.629017055034637 Val Loss: tensor(68.3270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1423 Loss: 10.642462998628616 Val Loss: tensor(68.6657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1424 Loss: 10.547317177057266 Val Loss: tensor(68.3283, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1425 Loss: 10.524396002292633 Val Loss: tensor(68.5107, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1426 Loss: 10.43499305844307 Val Loss: tensor(68.3286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1427 Loss: 10.384213000535965 Val Loss: tensor(68.3610, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1428 Loss: 10.30414292216301 Val Loss: tensor(68.3266, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1429 Loss: 10.23893517255783 Val Loss: tensor(68.2351, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1430 Loss: 10.171103984117508 Val Loss: tensor(68.3242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1431 Loss: 10.10431644320488 Val Loss: tensor(68.1421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1432 Loss: 10.050902158021927 Val Loss: tensor(68.3253, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1433 Loss: 9.991157114505768 Val Loss: tensor(68.0817, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1434 Loss: 9.952645391225815 Val Loss: tensor(68.3329, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1435 Loss: 9.903266310691833 Val Loss: tensor(68.0475, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1436 Loss: 9.877772122621536 Val Loss: tensor(68.3456, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1437 Loss: 9.837710231542587 Val Loss: tensor(68.0321, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1438 Loss: 9.821346789598465 Val Loss: tensor(68.3580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1439 Loss: 9.78647455573082 Val Loss: tensor(68.0292, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1440 Loss: 9.774009615182877 Val Loss: tensor(68.3638, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1441 Loss: 9.73881584405899 Val Loss: tensor(68.0344, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1442 Loss: 9.724661439657211 Val Loss: tensor(68.3580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1443 Loss: 9.684405267238617 Val Loss: tensor(68.0444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1444 Loss: 9.664344787597656 Val Loss: tensor(68.3405, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1445 Loss: 9.617326557636261 Val Loss: tensor(68.0559, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1446 Loss: 9.590001672506332 Val Loss: tensor(68.3152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1447 Loss: 9.538256585597992 Val Loss: tensor(68.0665, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1448 Loss: 9.505241394042969 Val Loss: tensor(68.2881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1449 Loss: 9.452726483345032 Val Loss: tensor(68.0754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1450 Loss: 9.416938811540604 Val Loss: tensor(68.2642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1451 Loss: 9.367134302854538 Val Loss: tensor(68.0827, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1452 Loss: 9.331177711486816 Val Loss: tensor(68.2467, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1453 Loss: 9.285986095666885 Val Loss: tensor(68.0887, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1454 Loss: 9.251429468393326 Val Loss: tensor(68.2362, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1455 Loss: 9.21135088801384 Val Loss: tensor(68.0938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1456 Loss: 9.178926885128021 Val Loss: tensor(68.2319, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1457 Loss: 9.143721371889114 Val Loss: tensor(68.0983, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1458 Loss: 9.113675743341446 Val Loss: tensor(68.2322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1459 Loss: 9.082858949899673 Val Loss: tensor(68.1025, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1460 Loss: 9.055279433727264 Val Loss: tensor(68.2354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1461 Loss: 9.028417974710464 Val Loss: tensor(68.1064, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1462 Loss: 9.003408640623093 Val Loss: tensor(68.2401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1463 Loss: 8.980155766010284 Val Loss: tensor(68.1103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1464 Loss: 8.957859098911285 Val Loss: tensor(68.2456, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1465 Loss: 8.937932580709457 Val Loss: tensor(68.1138, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1466 Loss: 8.918451845645905 Val Loss: tensor(68.2508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1467 Loss: 8.901480883359909 Val Loss: tensor(68.1168, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1468 Loss: 8.884794443845749 Val Loss: tensor(68.2546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1469 Loss: 8.87015125155449 Val Loss: tensor(68.1186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1470 Loss: 8.855907082557678 Val Loss: tensor(68.2558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1471 Loss: 8.84254565834999 Val Loss: tensor(68.1189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1472 Loss: 8.829923957586288 Val Loss: tensor(68.2528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1473 Loss: 8.816261291503906 Val Loss: tensor(68.1166, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1474 Loss: 8.803862780332565 Val Loss: tensor(68.2448, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1475 Loss: 8.787957549095154 Val Loss: tensor(68.1111, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1476 Loss: 8.77394551038742 Val Loss: tensor(68.2311, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1477 Loss: 8.753783375024796 Val Loss: tensor(68.1018, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1478 Loss: 8.736207246780396 Val Loss: tensor(68.2125, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1479 Loss: 8.710363656282425 Val Loss: tensor(68.0884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1480 Loss: 8.687853544950485 Val Loss: tensor(68.1909, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1481 Loss: 8.656035482883453 Val Loss: tensor(68.0712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1482 Loss: 8.628159701824188 Val Loss: tensor(68.1687, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1483 Loss: 8.591469079256058 Val Loss: tensor(68.0515, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1484 Loss: 8.558962017297745 Val Loss: tensor(68.1489, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1485 Loss: 8.519583255052567 Val Loss: tensor(68.0310, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1486 Loss: 8.48400741815567 Val Loss: tensor(68.1337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1487 Loss: 8.444588392972946 Val Loss: tensor(68.0115, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1488 Loss: 8.407773643732071 Val Loss: tensor(68.1247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1489 Loss: 8.37083950638771 Val Loss: tensor(67.9945, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1490 Loss: 8.334492534399033 Val Loss: tensor(68.1231, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1491 Loss: 8.30188775062561 Val Loss: tensor(67.9811, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1492 Loss: 8.26720005273819 Val Loss: tensor(68.1288, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1493 Loss: 8.239960134029388 Val Loss: tensor(67.9716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1494 Loss: 8.207655757665634 Val Loss: tensor(68.1408, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1495 Loss: 8.186190396547318 Val Loss: tensor(67.9656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1496 Loss: 8.156624227762222 Val Loss: tensor(68.1581, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1497 Loss: 8.140950471162796 Val Loss: tensor(67.9628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1498 Loss: 8.114247143268585 Val Loss: tensor(68.1792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1499 Loss: 8.104122370481491 Val Loss: tensor(67.9625, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1500 Loss: 8.080236732959747 Val Loss: tensor(68.2029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1501 Loss: 8.075226664543152 Val Loss: tensor(67.9644, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1502 Loss: 8.053931951522827 Val Loss: tensor(68.2275, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1503 Loss: 8.053317040205002 Val Loss: tensor(67.9681, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1504 Loss: 8.034041970968246 Val Loss: tensor(68.2510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1505 Loss: 8.036729097366333 Val Loss: tensor(67.9735, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1506 Loss: 8.018457919359207 Val Loss: tensor(68.2709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1507 Loss: 8.022910982370377 Val Loss: tensor(67.9807, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1508 Loss: 8.004079341888428 Val Loss: tensor(68.2845, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1509 Loss: 8.008414715528488 Val Loss: tensor(67.9895, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1510 Loss: 7.987106502056122 Val Loss: tensor(68.2898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1511 Loss: 7.989445239305496 Val Loss: tensor(68.0000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1512 Loss: 7.963629335165024 Val Loss: tensor(68.2852, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1513 Loss: 7.962611347436905 Val Loss: tensor(68.0122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1514 Loss: 7.930566281080246 Val Loss: tensor(68.2707, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1515 Loss: 7.925513565540314 Val Loss: tensor(68.0260, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1516 Loss: 7.88614758849144 Val Loss: tensor(68.2475, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1517 Loss: 7.877202600240707 Val Loss: tensor(68.0409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1518 Loss: 7.830259382724762 Val Loss: tensor(68.2178, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1519 Loss: 7.818042516708374 Val Loss: tensor(68.0562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1520 Loss: 7.764195144176483 Val Loss: tensor(68.1837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1521 Loss: 7.749399423599243 Val Loss: tensor(68.0715, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1522 Loss: 7.6903403997421265 Val Loss: tensor(68.1478, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1523 Loss: 7.67368882894516 Val Loss: tensor(68.0864, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1524 Loss: 7.611925899982452 Val Loss: tensor(68.1125, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1525 Loss: 7.593988060951233 Val Loss: tensor(68.1015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1526 Loss: 7.532318145036697 Val Loss: tensor(68.0799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1527 Loss: 7.513667613267899 Val Loss: tensor(68.1173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1528 Loss: 7.454557627439499 Val Loss: tensor(68.0513, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1529 Loss: 7.435813277959824 Val Loss: tensor(68.1345, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1530 Loss: 7.3809380531311035 Val Loss: tensor(68.0277, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1531 Loss: 7.362669438123703 Val Loss: tensor(68.1534, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1532 Loss: 7.312814146280289 Val Loss: tensor(68.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1533 Loss: 7.295510649681091 Val Loss: tensor(68.1739, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1534 Loss: 7.250673234462738 Val Loss: tensor(67.9942, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1535 Loss: 7.234700411558151 Val Loss: tensor(68.1950, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1536 Loss: 7.194387376308441 Val Loss: tensor(67.9829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1537 Loss: 7.1799376308918 Val Loss: tensor(68.2159, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1538 Loss: 7.143328636884689 Val Loss: tensor(67.9744, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1539 Loss: 7.130357533693314 Val Loss: tensor(68.2354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1540 Loss: 7.096548825502396 Val Loss: tensor(67.9676, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1541 Loss: 7.08485272526741 Val Loss: tensor(68.2521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1542 Loss: 7.052891343832016 Val Loss: tensor(67.9621, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1543 Loss: 7.042087137699127 Val Loss: tensor(68.2651, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1544 Loss: 7.0111358761787415 Val Loss: tensor(67.9572, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1545 Loss: 7.00070384144783 Val Loss: tensor(68.2733, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1546 Loss: 6.9700610637664795 Val Loss: tensor(67.9530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1547 Loss: 6.959495723247528 Val Loss: tensor(68.2763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1548 Loss: 6.928759783506393 Val Loss: tensor(67.9491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1549 Loss: 6.917578190565109 Val Loss: tensor(68.2738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1550 Loss: 6.886588901281357 Val Loss: tensor(67.9458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1551 Loss: 6.874511212110519 Val Loss: tensor(68.2660, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1552 Loss: 6.843393474817276 Val Loss: tensor(67.9431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1553 Loss: 6.830342382192612 Val Loss: tensor(68.2532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1554 Loss: 6.799522161483765 Val Loss: tensor(67.9416, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1555 Loss: 6.785721510648727 Val Loss: tensor(68.2358, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1556 Loss: 6.755767822265625 Val Loss: tensor(67.9419, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1557 Loss: 6.741639018058777 Val Loss: tensor(68.2139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1558 Loss: 6.713247567415237 Val Loss: tensor(67.9453, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1559 Loss: 6.699513882398605 Val Loss: tensor(68.1866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1560 Loss: 6.6735294461250305 Val Loss: tensor(67.9536, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1561 Loss: 6.6612430810928345 Val Loss: tensor(68.1532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1562 Loss: 6.6389020383358 Val Loss: tensor(67.9689, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1563 Loss: 6.629612296819687 Val Loss: tensor(68.1115, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1564 Loss: 6.612840414047241 Val Loss: tensor(67.9948, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1565 Loss: 6.6088623106479645 Val Loss: tensor(68.0603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1566 Loss: 6.600947827100754 Val Loss: tensor(68.0344, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1567 Loss: 6.605086147785187 Val Loss: tensor(68.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1568 Loss: 6.610673815011978 Val Loss: tensor(68.0880, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1569 Loss: 6.6241039633750916 Val Loss: tensor(67.9374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1570 Loss: 6.6465069353580475 Val Loss: tensor(68.1443, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1571 Loss: 6.661116153001785 Val Loss: tensor(67.8862, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1572 Loss: 6.692291170358658 Val Loss: tensor(68.1727, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1573 Loss: 6.680657356977463 Val Loss: tensor(67.8619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1574 Loss: 6.692025810480118 Val Loss: tensor(68.1381, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1575 Loss: 6.624048471450806 Val Loss: tensor(67.8603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1576 Loss: 6.592570096254349 Val Loss: tensor(68.0471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1577 Loss: 6.486334443092346 Val Loss: tensor(67.8559, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1578 Loss: 6.433753609657288 Val Loss: tensor(67.9514, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1579 Loss: 6.337803155183792 Val Loss: tensor(67.8392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1580 Loss: 6.293064177036285 Val Loss: tensor(67.8884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1581 Loss: 6.220387935638428 Val Loss: tensor(67.8197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1582 Loss: 6.183862686157227 Val Loss: tensor(67.8497, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1583 Loss: 6.125313013792038 Val Loss: tensor(67.8000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1584 Loss: 6.093017369508743 Val Loss: tensor(67.8233, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1585 Loss: 6.044911980628967 Val Loss: tensor(67.7833, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1586 Loss: 6.018437206745148 Val Loss: tensor(67.8128, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1587 Loss: 5.979888439178467 Val Loss: tensor(67.7732, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1588 Loss: 5.959525376558304 Val Loss: tensor(67.8169, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1589 Loss: 5.927972376346588 Val Loss: tensor(67.7691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1590 Loss: 5.911756068468094 Val Loss: tensor(67.8286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1591 Loss: 5.884386122226715 Val Loss: tensor(67.7713, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1592 Loss: 5.870829224586487 Val Loss: tensor(67.8408, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1593 Loss: 5.846217304468155 Val Loss: tensor(67.7800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1594 Loss: 5.835063815116882 Val Loss: tensor(67.8513, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1595 Loss: 5.81297692656517 Val Loss: tensor(67.7953, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1596 Loss: 5.805024594068527 Val Loss: tensor(67.8610, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1597 Loss: 5.785884261131287 Val Loss: tensor(67.8159, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1598 Loss: 5.7825702130794525 Val Loss: tensor(67.8717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1599 Loss: 5.76721927523613 Val Loss: tensor(67.8412, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1600 Loss: 5.7705855667591095 Val Loss: tensor(67.8848, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1601 Loss: 5.760327726602554 Val Loss: tensor(67.8716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1602 Loss: 5.77314630150795 Val Loss: tensor(67.9001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1603 Loss: 5.769881308078766 Val Loss: tensor(67.9079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1604 Loss: 5.795540988445282 Val Loss: tensor(67.9158, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1605 Loss: 5.80160653591156 Val Loss: tensor(67.9518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1606 Loss: 5.843666136264801 Val Loss: tensor(67.9295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1607 Loss: 5.860991805791855 Val Loss: tensor(68.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1608 Loss: 5.921313613653183 Val Loss: tensor(67.9346, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1609 Loss: 5.94930824637413 Val Loss: tensor(68.0554, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1610 Loss: 6.024033695459366 Val Loss: tensor(67.9181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1611 Loss: 6.056102395057678 Val Loss: tensor(68.0942, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1612 Loss: 6.129389405250549 Val Loss: tensor(67.8629, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1613 Loss: 6.149651944637299 Val Loss: tensor(68.0971, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1614 Loss: 6.189628094434738 Val Loss: tensor(67.7598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1615 Loss: 6.176284492015839 Val Loss: tensor(68.0492, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1616 Loss: 6.146761536598206 Val Loss: tensor(67.6263, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1617 Loss: 6.088535934686661 Val Loss: tensor(67.9619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1618 Loss: 5.982243895530701 Val Loss: tensor(67.5059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1619 Loss: 5.895734339952469 Val Loss: tensor(67.8705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1620 Loss: 5.753288835287094 Val Loss: tensor(67.4352, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1621 Loss: 5.672520011663437 Val Loss: tensor(67.8055, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1622 Loss: 5.547358214855194 Val Loss: tensor(67.4202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1623 Loss: 5.492266058921814 Val Loss: tensor(67.7763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1624 Loss: 5.407096266746521 Val Loss: tensor(67.4434, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1625 Loss: 5.374102175235748 Val Loss: tensor(67.7740, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1626 Loss: 5.322185784578323 Val Loss: tensor(67.4784, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1627 Loss: 5.299560070037842 Val Loss: tensor(67.7844, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1628 Loss: 5.267141371965408 Val Loss: tensor(67.5057, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1629 Loss: 5.247694253921509 Val Loss: tensor(67.7972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1630 Loss: 5.225323647260666 Val Loss: tensor(67.5211, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1631 Loss: 5.207198083400726 Val Loss: tensor(67.8070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1632 Loss: 5.189767926931381 Val Loss: tensor(67.5298, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1633 Loss: 5.172890394926071 Val Loss: tensor(67.8134, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1634 Loss: 5.157383382320404 Val Loss: tensor(67.5377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1635 Loss: 5.141819953918457 Val Loss: tensor(67.8172, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1636 Loss: 5.126317739486694 Val Loss: tensor(67.5466, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1637 Loss: 5.112088233232498 Val Loss: tensor(67.8188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1638 Loss: 5.095530778169632 Val Loss: tensor(67.5561, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1639 Loss: 5.082592576742172 Val Loss: tensor(67.8169, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1640 Loss: 5.064731448888779 Val Loss: tensor(67.5650, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1641 Loss: 5.053031325340271 Val Loss: tensor(67.8113, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1642 Loss: 5.034095197916031 Val Loss: tensor(67.5732, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1643 Loss: 5.023561477661133 Val Loss: tensor(67.8021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1644 Loss: 5.0039428770542145 Val Loss: tensor(67.5814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1645 Loss: 4.99455189704895 Val Loss: tensor(67.7902, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1646 Loss: 4.9746357798576355 Val Loss: tensor(67.5901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1647 Loss: 4.966384261846542 Val Loss: tensor(67.7761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1648 Loss: 4.946402311325073 Val Loss: tensor(67.5999, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1649 Loss: 4.939340651035309 Val Loss: tensor(67.7600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1650 Loss: 4.9193969666957855 Val Loss: tensor(67.6109, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1651 Loss: 4.9135264456272125 Val Loss: tensor(67.7419, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1652 Loss: 4.893671602010727 Val Loss: tensor(67.6233, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1653 Loss: 4.888952404260635 Val Loss: tensor(67.7215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1654 Loss: 4.86916846036911 Val Loss: tensor(67.6374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1655 Loss: 4.8655064702034 Val Loss: tensor(67.6983, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1656 Loss: 4.845741182565689 Val Loss: tensor(67.6532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1657 Loss: 4.84295853972435 Val Loss: tensor(67.6719, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1658 Loss: 4.8231692016124725 Val Loss: tensor(67.6707, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1659 Loss: 4.821071296930313 Val Loss: tensor(67.6421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1660 Loss: 4.801223158836365 Val Loss: tensor(67.6898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1661 Loss: 4.7995592057704926 Val Loss: tensor(67.6089, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1662 Loss: 4.779715985059738 Val Loss: tensor(67.7099, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1663 Loss: 4.778270661830902 Val Loss: tensor(67.5729, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1664 Loss: 4.758570492267609 Val Loss: tensor(67.7307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1665 Loss: 4.757168114185333 Val Loss: tensor(67.5349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1666 Loss: 4.73786336183548 Val Loss: tensor(67.7514, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1667 Loss: 4.736372083425522 Val Loss: tensor(67.4961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1668 Loss: 4.717781037092209 Val Loss: tensor(67.7708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1669 Loss: 4.71604460477829 Val Loss: tensor(67.4580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1670 Loss: 4.69851154088974 Val Loss: tensor(67.7878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1671 Loss: 4.696356385946274 Val Loss: tensor(67.4225, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1672 Loss: 4.680217415094376 Val Loss: tensor(67.8011, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1673 Loss: 4.677439302206039 Val Loss: tensor(67.3913, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1674 Loss: 4.663181126117706 Val Loss: tensor(67.8096, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1675 Loss: 4.659730285406113 Val Loss: tensor(67.3665, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1676 Loss: 4.64812508225441 Val Loss: tensor(67.8127, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1677 Loss: 4.644410610198975 Val Loss: tensor(67.3498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1678 Loss: 4.63689786195755 Val Loss: tensor(67.8113, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1679 Loss: 4.634395807981491 Val Loss: tensor(67.3435, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1680 Loss: 4.633563905954361 Val Loss: tensor(67.8082, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1681 Loss: 4.6356014013290405 Val Loss: tensor(67.3498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1682 Loss: 4.6458273231983185 Val Loss: tensor(67.8074, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1683 Loss: 4.658460110425949 Val Loss: tensor(67.3716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1684 Loss: 4.686382353305817 Val Loss: tensor(67.8138, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1685 Loss: 4.718422204256058 Val Loss: tensor(67.4118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1686 Loss: 4.7717984318733215 Val Loss: tensor(67.8282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1687 Loss: 4.831580609083176 Val Loss: tensor(67.4714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1688 Loss: 4.913319855928421 Val Loss: tensor(67.8365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1689 Loss: 4.995529651641846 Val Loss: tensor(67.5395, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1690 Loss: 5.087149053812027 Val Loss: tensor(67.7961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1691 Loss: 5.14788493514061 Val Loss: tensor(67.5808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1692 Loss: 5.192704498767853 Val Loss: tensor(67.6545, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1693 Loss: 5.152957022190094 Val Loss: tensor(67.5462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1694 Loss: 5.095446735620499 Val Loss: tensor(67.4320, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1695 Loss: 4.9351359605789185 Val Loss: tensor(67.4317, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1696 Loss: 4.810540080070496 Val Loss: tensor(67.2484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1697 Loss: 4.618467003107071 Val Loss: tensor(67.3009, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1698 Loss: 4.508660852909088 Val Loss: tensor(67.1804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1699 Loss: 4.359418034553528 Val Loss: tensor(67.2087, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1700 Loss: 4.288521647453308 Val Loss: tensor(67.1793, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1701 Loss: 4.192038685083389 Val Loss: tensor(67.1540, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1702 Loss: 4.1487248837947845 Val Loss: tensor(67.1882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1703 Loss: 4.094132333993912 Val Loss: tensor(67.1258, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1704 Loss: 4.064857602119446 Val Loss: tensor(67.2056, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1705 Loss: 4.036435186862946 Val Loss: tensor(67.1194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1706 Loss: 4.012421041727066 Val Loss: tensor(67.2293, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1707 Loss: 3.9957067370414734 Val Loss: tensor(67.1254, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1708 Loss: 3.9734144806861877 Val Loss: tensor(67.2466, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1709 Loss: 3.9603098034858704 Val Loss: tensor(67.1357, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1710 Loss: 3.9389189779758453 Val Loss: tensor(67.2541, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1711 Loss: 3.9267796874046326 Val Loss: tensor(67.1449, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1712 Loss: 3.9066914319992065 Val Loss: tensor(67.2582, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1713 Loss: 3.8950068652629852 Val Loss: tensor(67.1533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1714 Loss: 3.8767177760601044 Val Loss: tensor(67.2651, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1715 Loss: 3.865247368812561 Val Loss: tensor(67.1621, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1716 Loss: 3.848711848258972 Val Loss: tensor(67.2741, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1717 Loss: 3.8372671604156494 Val Loss: tensor(67.1688, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1718 Loss: 3.8221585154533386 Val Loss: tensor(67.2821, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1719 Loss: 3.810643970966339 Val Loss: tensor(67.1723, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1720 Loss: 3.7966549694538116 Val Loss: tensor(67.2881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1721 Loss: 3.785077929496765 Val Loss: tensor(67.1733, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1722 Loss: 3.7720406651496887 Val Loss: tensor(67.2927, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1723 Loss: 3.760449171066284 Val Loss: tensor(67.1737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1724 Loss: 3.7483333945274353 Val Loss: tensor(67.2968, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1725 Loss: 3.736796200275421 Val Loss: tensor(67.1748, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1726 Loss: 3.72567880153656 Val Loss: tensor(67.3008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1727 Loss: 3.714331418275833 Val Loss: tensor(67.1774, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1728 Loss: 3.7044091522693634 Val Loss: tensor(67.3047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1729 Loss: 3.6934303045272827 Val Loss: tensor(67.1823, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1730 Loss: 3.6852001547813416 Val Loss: tensor(67.3079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1731 Loss: 3.6749732196331024 Val Loss: tensor(67.1910, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1732 Loss: 3.6693642139434814 Val Loss: tensor(67.3098, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1733 Loss: 3.660675674676895 Val Loss: tensor(67.2061, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1734 Loss: 3.6596004962921143 Val Loss: tensor(67.3100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1735 Loss: 3.6541815102100372 Val Loss: tensor(67.2322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1736 Loss: 3.66141277551651 Val Loss: tensor(67.3085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1737 Loss: 3.6630337834358215 Val Loss: tensor(67.2773, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1738 Loss: 3.686361998319626 Val Loss: tensor(67.3063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1739 Loss: 3.703240245580673 Val Loss: tensor(67.3556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1740 Loss: 3.758609563112259 Val Loss: tensor(67.3095, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1741 Loss: 3.80801197886467 Val Loss: tensor(67.4894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1742 Loss: 3.9254641234874725 Val Loss: tensor(67.3340, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1743 Loss: 4.038412630558014 Val Loss: tensor(67.6973, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1744 Loss: 4.256022900342941 Val Loss: tensor(67.4086, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1745 Loss: 4.4560049176216125 Val Loss: tensor(67.9207, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1746 Loss: 4.736309319734573 Val Loss: tensor(67.5286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1747 Loss: 4.912002623081207 Val Loss: tensor(67.9047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1748 Loss: 4.97962960600853 Val Loss: tensor(67.5432, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1749 Loss: 4.856445252895355 Val Loss: tensor(67.4707, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1750 Loss: 4.594518959522247 Val Loss: tensor(67.2994, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1751 Loss: 4.360328435897827 Val Loss: tensor(67.0307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1752 Loss: 4.138560354709625 Val Loss: tensor(67.0609, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1753 Loss: 4.034003019332886 Val Loss: tensor(67.0506, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1754 Loss: 3.872493326663971 Val Loss: tensor(66.9532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1755 Loss: 3.7550600171089172 Val Loss: tensor(67.1311, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1756 Loss: 3.595129042863846 Val Loss: tensor(66.7609, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1757 Loss: 3.510639876127243 Val Loss: tensor(67.0363, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1758 Loss: 3.4338851869106293 Val Loss: tensor(66.6508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1759 Loss: 3.4055044054985046 Val Loss: tensor(67.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1760 Loss: 3.3866379857063293 Val Loss: tensor(66.6861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1761 Loss: 3.3698287904262543 Val Loss: tensor(67.0277, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1762 Loss: 3.3598527014255524 Val Loss: tensor(66.7349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1763 Loss: 3.3410943150520325 Val Loss: tensor(67.0385, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1764 Loss: 3.329174906015396 Val Loss: tensor(66.7811, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1765 Loss: 3.310134679079056 Val Loss: tensor(67.0355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1766 Loss: 3.299592226743698 Val Loss: tensor(66.8017, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1767 Loss: 3.283160924911499 Val Loss: tensor(67.0202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1768 Loss: 3.273567169904709 Val Loss: tensor(66.8184, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1769 Loss: 3.2612180709838867 Val Loss: tensor(67.0199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1770 Loss: 3.251545161008835 Val Loss: tensor(66.8394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1771 Loss: 3.2420569956302643 Val Loss: tensor(67.0199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1772 Loss: 3.2317398488521576 Val Loss: tensor(66.8505, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1773 Loss: 3.22430083155632 Val Loss: tensor(67.0224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1774 Loss: 3.2135452926158905 Val Loss: tensor(66.8625, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1775 Loss: 3.2079266905784607 Val Loss: tensor(67.0237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1776 Loss: 3.1971762776374817 Val Loss: tensor(66.8707, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1777 Loss: 3.193807601928711 Val Loss: tensor(67.0171, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1778 Loss: 3.1834899187088013 Val Loss: tensor(66.8818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1779 Loss: 3.1830168068408966 Val Loss: tensor(67.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1780 Loss: 3.173996239900589 Val Loss: tensor(66.9025, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1781 Loss: 3.1771128177642822 Val Loss: tensor(67.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1782 Loss: 3.1705912351608276 Val Loss: tensor(66.9294, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1783 Loss: 3.1784436106681824 Val Loss: tensor(66.9876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1784 Loss: 3.176010102033615 Val Loss: tensor(66.9642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1785 Loss: 3.1899845004081726 Val Loss: tensor(66.9740, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1786 Loss: 3.193834215402603 Val Loss: tensor(67.0067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1787 Loss: 3.21494922041893 Val Loss: tensor(66.9604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1788 Loss: 3.2269382774829865 Val Loss: tensor(67.0548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1789 Loss: 3.2543820440769196 Val Loss: tensor(66.9489, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1790 Loss: 3.2743046581745148 Val Loss: tensor(67.1002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1791 Loss: 3.3035386204719543 Val Loss: tensor(66.9366, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1792 Loss: 3.3271601796150208 Val Loss: tensor(67.1300, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1793 Loss: 3.351074904203415 Val Loss: tensor(66.9175, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1794 Loss: 3.3722009658813477 Val Loss: tensor(67.1370, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1795 Loss: 3.3874093294143677 Val Loss: tensor(66.8849, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1796 Loss: 3.4057639241218567 Val Loss: tensor(67.1291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1797 Loss: 3.4172291457653046 Val Loss: tensor(66.8385, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1798 Loss: 3.4414099752902985 Val Loss: tensor(67.1283, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1799 Loss: 3.453629195690155 Val Loss: tensor(66.7853, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1800 Loss: 3.489320784807205 Val Loss: tensor(67.1462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1801 Loss: 3.4926230013370514 Val Loss: tensor(66.7232, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1802 Loss: 3.5302675664424896 Val Loss: tensor(67.1666, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1803 Loss: 3.5024963915348053 Val Loss: tensor(66.6395, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1804 Loss: 3.5225437581539154 Val Loss: tensor(67.1607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1805 Loss: 3.4498605728149414 Val Loss: tensor(66.5313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1806 Loss: 3.443461447954178 Val Loss: tensor(67.1160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1807 Loss: 3.339571475982666 Val Loss: tensor(66.4219, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1808 Loss: 3.3166438341140747 Val Loss: tensor(67.0468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1809 Loss: 3.2111642360687256 Val Loss: tensor(66.3470, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1810 Loss: 3.1844083964824677 Val Loss: tensor(66.9774, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1811 Loss: 3.0971474051475525 Val Loss: tensor(66.3233, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1812 Loss: 3.0717371106147766 Val Loss: tensor(66.9198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1813 Loss: 3.0069164633750916 Val Loss: tensor(66.3420, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1814 Loss: 2.983855038881302 Val Loss: tensor(66.8741, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1815 Loss: 2.937579423189163 Val Loss: tensor(66.3827, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1816 Loss: 2.9166114032268524 Val Loss: tensor(66.8363, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1817 Loss: 2.8831405639648438 Val Loss: tensor(66.4262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1818 Loss: 2.8640697300434113 Val Loss: tensor(66.8027, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1819 Loss: 2.8388919830322266 Val Loss: tensor(66.4633, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1820 Loss: 2.8219971358776093 Val Loss: tensor(66.7735, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1821 Loss: 2.802350878715515 Val Loss: tensor(66.4933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1822 Loss: 2.788029730319977 Val Loss: tensor(66.7505, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1823 Loss: 2.7722397446632385 Val Loss: tensor(66.5190, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1824 Loss: 2.7607935070991516 Val Loss: tensor(66.7337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1825 Loss: 2.7477102279663086 Val Loss: tensor(66.5423, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1826 Loss: 2.7393667697906494 Val Loss: tensor(66.7218, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1827 Loss: 2.7282723784446716 Val Loss: tensor(66.5651, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1828 Loss: 2.723361521959305 Val Loss: tensor(66.7133, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1829 Loss: 2.7139684855937958 Val Loss: tensor(66.5893, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1830 Loss: 2.7131897807121277 Val Loss: tensor(66.7071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1831 Loss: 2.705615282058716 Val Loss: tensor(66.6180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1832 Loss: 2.7102085947990417 Val Loss: tensor(66.7024, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1833 Loss: 2.7051089107990265 Val Loss: tensor(66.6538, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1834 Loss: 2.717041313648224 Val Loss: tensor(66.6989, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1835 Loss: 2.715841293334961 Val Loss: tensor(66.7003, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1836 Loss: 2.73782742023468 Val Loss: tensor(66.6982, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1837 Loss: 2.7427092790603638 Val Loss: tensor(66.7599, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1838 Loss: 2.77758526802063 Val Loss: tensor(66.7031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1839 Loss: 2.790717899799347 Val Loss: tensor(66.8316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1840 Loss: 2.8391616344451904 Val Loss: tensor(66.7163, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1841 Loss: 2.859929323196411 Val Loss: tensor(66.9053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1842 Loss: 2.91682231426239 Val Loss: tensor(66.7355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1843 Loss: 2.937983065843582 Val Loss: tensor(66.9602, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1844 Loss: 2.9924614131450653 Val Loss: tensor(66.7493, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1845 Loss: 3.0032202303409576 Val Loss: tensor(66.9775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1846 Loss: 3.050683408975601 Val Loss: tensor(66.7463, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1847 Loss: 3.051908105611801 Val Loss: tensor(66.9678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1848 Loss: 3.104761481285095 Val Loss: tensor(66.7341, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1849 Loss: 3.1130703389644623 Val Loss: tensor(66.9686, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1850 Loss: 3.1838634312152863 Val Loss: tensor(66.7294, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1851 Loss: 3.2060031592845917 Val Loss: tensor(66.9930, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1852 Loss: 3.278856486082077 Val Loss: tensor(66.7203, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1853 Loss: 3.2879849672317505 Val Loss: tensor(67.0018, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1854 Loss: 3.3125878274440765 Val Loss: tensor(66.6664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1855 Loss: 3.2637137472629547 Val Loss: tensor(66.9439, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1856 Loss: 3.1961435973644257 Val Loss: tensor(66.5490, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1857 Loss: 3.086799383163452 Val Loss: tensor(66.8168, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1858 Loss: 2.9538148045539856 Val Loss: tensor(66.4102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1859 Loss: 2.840250015258789 Val Loss: tensor(66.6763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1860 Loss: 2.7135526835918427 Val Loss: tensor(66.3190, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1861 Loss: 2.6367944180965424 Val Loss: tensor(66.5772, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1862 Loss: 2.5527846813201904 Val Loss: tensor(66.2999, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1863 Loss: 2.51052325963974 Val Loss: tensor(66.5306, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1864 Loss: 2.464755892753601 Val Loss: tensor(66.3278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1865 Loss: 2.441252112388611 Val Loss: tensor(66.5208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1866 Loss: 2.4183928966522217 Val Loss: tensor(66.3677, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1867 Loss: 2.4023557007312775 Val Loss: tensor(66.5280, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1868 Loss: 2.39051416516304 Val Loss: tensor(66.3966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1869 Loss: 2.3768855333328247 Val Loss: tensor(66.5380, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1870 Loss: 2.3696490824222565 Val Loss: tensor(66.4104, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1871 Loss: 2.3569266498088837 Val Loss: tensor(66.5451, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1872 Loss: 2.3515841960906982 Val Loss: tensor(66.4139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1873 Loss: 2.339528888463974 Val Loss: tensor(66.5501, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1874 Loss: 2.334991842508316 Val Loss: tensor(66.4136, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1875 Loss: 2.323668450117111 Val Loss: tensor(66.5549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1876 Loss: 2.3194699585437775 Val Loss: tensor(66.4122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1877 Loss: 2.308976322412491 Val Loss: tensor(66.5596, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1878 Loss: 2.3049196302890778 Val Loss: tensor(66.4085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1879 Loss: 2.295289784669876 Val Loss: tensor(66.5629, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1880 Loss: 2.2913574874401093 Val Loss: tensor(66.4021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1881 Loss: 2.282626658678055 Val Loss: tensor(66.5655, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1882 Loss: 2.278880685567856 Val Loss: tensor(66.3944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1883 Loss: 2.271191954612732 Val Loss: tensor(66.5678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1884 Loss: 2.26774999499321 Val Loss: tensor(66.3866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1885 Loss: 2.2614461183547974 Val Loss: tensor(66.5699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1886 Loss: 2.2585787773132324 Val Loss: tensor(66.3801, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1887 Loss: 2.2543084621429443 Val Loss: tensor(66.5719, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1888 Loss: 2.252589166164398 Val Loss: tensor(66.3771, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1889 Loss: 2.251547545194626 Val Loss: tensor(66.5747, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1890 Loss: 2.2521743178367615 Val Loss: tensor(66.3809, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1891 Loss: 2.2565114200115204 Val Loss: tensor(66.5792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1892 Loss: 2.26190522313118 Val Loss: tensor(66.3962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1893 Loss: 2.275168865919113 Val Loss: tensor(66.5883, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1894 Loss: 2.28978568315506 Val Loss: tensor(66.4291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1895 Loss: 2.3164192140102386 Val Loss: tensor(66.6081, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1896 Loss: 2.3462876677513123 Val Loss: tensor(66.4824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1897 Loss: 2.386738121509552 Val Loss: tensor(66.6444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1898 Loss: 2.4324854612350464 Val Loss: tensor(66.5397, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1899 Loss: 2.470103770494461 Val Loss: tensor(66.6869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1900 Loss: 2.5093294978141785 Val Loss: tensor(66.5487, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1901 Loss: 2.5071361660957336 Val Loss: tensor(66.6886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1902 Loss: 2.4996719658374786 Val Loss: tensor(66.4561, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1903 Loss: 2.4517390429973602 Val Loss: tensor(66.6101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1904 Loss: 2.403263032436371 Val Loss: tensor(66.3123, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1905 Loss: 2.360713243484497 Val Loss: tensor(66.5036, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1906 Loss: 2.319287985563278 Val Loss: tensor(66.2488, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1907 Loss: 2.3120289146900177 Val Loss: tensor(66.4350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1908 Loss: 2.2959260642528534 Val Loss: tensor(66.2955, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1909 Loss: 2.3171888887882233 Val Loss: tensor(66.3899, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1910 Loss: 2.3238651752471924 Val Loss: tensor(66.3955, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1911 Loss: 2.366997539997101 Val Loss: tensor(66.3493, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1912 Loss: 2.39884290099144 Val Loss: tensor(66.5162, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1913 Loss: 2.467129558324814 Val Loss: tensor(66.3178, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1914 Loss: 2.532246708869934 Val Loss: tensor(66.6456, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1915 Loss: 2.628518521785736 Val Loss: tensor(66.3013, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1916 Loss: 2.7302728593349457 Val Loss: tensor(66.7738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1917 Loss: 2.838535338640213 Val Loss: tensor(66.2968, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1918 Loss: 2.957245498895645 Val Loss: tensor(66.8621, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1919 Loss: 3.0243099331855774 Val Loss: tensor(66.2777, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1920 Loss: 3.1029606759548187 Val Loss: tensor(66.8482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1921 Loss: 3.054521292448044 Val Loss: tensor(66.2115, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1922 Loss: 3.034943401813507 Val Loss: tensor(66.7134, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1923 Loss: 2.8565492928028107 Val Loss: tensor(66.1028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1924 Loss: 2.7547605633735657 Val Loss: tensor(66.5196, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1925 Loss: 2.535247713327408 Val Loss: tensor(65.9982, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1926 Loss: 2.4284026622772217 Val Loss: tensor(66.3512, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1927 Loss: 2.265666127204895 Val Loss: tensor(65.9439, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1928 Loss: 2.1992048621177673 Val Loss: tensor(66.2492, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1929 Loss: 2.1159344911575317 Val Loss: tensor(65.9498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1930 Loss: 2.0826559364795685 Val Loss: tensor(66.2085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1931 Loss: 2.050936698913574 Val Loss: tensor(65.9912, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1932 Loss: 2.031175583600998 Val Loss: tensor(66.2018, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1933 Loss: 2.0216240882873535 Val Loss: tensor(66.0331, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1934 Loss: 2.004982888698578 Val Loss: tensor(66.2050, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1935 Loss: 2.0031452775001526 Val Loss: tensor(66.0580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1936 Loss: 1.9873819947242737 Val Loss: tensor(66.2087, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1937 Loss: 1.9888768792152405 Val Loss: tensor(66.0693, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1938 Loss: 1.9747692048549652 Val Loss: tensor(66.2135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1939 Loss: 1.9788262844085693 Val Loss: tensor(66.0764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1940 Loss: 1.9674569964408875 Val Loss: tensor(66.2218, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1941 Loss: 1.9744577407836914 Val Loss: tensor(66.0861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1942 Loss: 1.9663680791854858 Val Loss: tensor(66.2365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1943 Loss: 1.9772958159446716 Val Loss: tensor(66.0995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1944 Loss: 1.9727602005004883 Val Loss: tensor(66.2584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1945 Loss: 1.9892589449882507 Val Loss: tensor(66.1146, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1946 Loss: 1.9885711669921875 Val Loss: tensor(66.2882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1947 Loss: 2.0131552517414093 Val Loss: tensor(66.1319, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1948 Loss: 2.0167214572429657 Val Loss: tensor(66.3285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1949 Loss: 2.0528154969215393 Val Loss: tensor(66.1549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1950 Loss: 2.0611365735530853 Val Loss: tensor(66.3806, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1951 Loss: 2.1125549376010895 Val Loss: tensor(66.1858, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1952 Loss: 2.125538855791092 Val Loss: tensor(66.4423, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1953 Loss: 2.1950181126594543 Val Loss: tensor(66.2247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1954 Loss: 2.2103996574878693 Val Loss: tensor(66.5054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1955 Loss: 2.2964751422405243 Val Loss: tensor(66.2664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1956 Loss: 2.3077019453048706 Val Loss: tensor(66.5526, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1957 Loss: 2.4000335037708282 Val Loss: tensor(66.2988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1958 Loss: 2.395459830760956 Val Loss: tensor(66.5580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1959 Loss: 2.4724519550800323 Val Loss: tensor(66.3039, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1960 Loss: 2.439892679452896 Val Loss: tensor(66.5017, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1961 Loss: 2.4758321046829224 Val Loss: tensor(66.2696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1962 Loss: 2.412814199924469 Val Loss: tensor(66.3906, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1963 Loss: 2.394820749759674 Val Loss: tensor(66.2022, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1964 Loss: 2.3145304322242737 Val Loss: tensor(66.2616, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1965 Loss: 2.2535850405693054 Val Loss: tensor(66.1228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1966 Loss: 2.176700085401535 Val Loss: tensor(66.1530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1967 Loss: 2.099102258682251 Val Loss: tensor(66.0520, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1968 Loss: 2.0402124226093292 Val Loss: tensor(66.0798, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1969 Loss: 1.969312697649002 Val Loss: tensor(66.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1970 Loss: 1.9317587316036224 Val Loss: tensor(66.0380, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1971 Loss: 1.8781039714813232 Val Loss: tensor(65.9738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1972 Loss: 1.8574083149433136 Val Loss: tensor(66.0186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1973 Loss: 1.8207546174526215 Val Loss: tensor(65.9661, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1974 Loss: 1.8103377521038055 Val Loss: tensor(66.0134, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1975 Loss: 1.7860596776008606 Val Loss: tensor(65.9695, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1976 Loss: 1.7807714343070984 Val Loss: tensor(66.0156, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1977 Loss: 1.7642357349395752 Val Loss: tensor(65.9760, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1978 Loss: 1.7611104249954224 Val Loss: tensor(66.0207, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1979 Loss: 1.7491394579410553 Val Loss: tensor(65.9808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1980 Loss: 1.7468598783016205 Val Loss: tensor(66.0262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1981 Loss: 1.7375880777835846 Val Loss: tensor(65.9825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1982 Loss: 1.735752135515213 Val Loss: tensor(66.0309, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1983 Loss: 1.7281438410282135 Val Loss: tensor(65.9810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1984 Loss: 1.7267740070819855 Val Loss: tensor(66.0348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1985 Loss: 1.720306783914566 Val Loss: tensor(65.9766, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1986 Loss: 1.7196649312973022 Val Loss: tensor(66.0382, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1987 Loss: 1.7141209840774536 Val Loss: tensor(65.9693, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1988 Loss: 1.714586615562439 Val Loss: tensor(66.0422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1989 Loss: 1.7100310027599335 Val Loss: tensor(65.9598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1990 Loss: 1.7121638357639313 Val Loss: tensor(66.0481, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1991 Loss: 1.7089123725891113 Val Loss: tensor(65.9484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1992 Loss: 1.7135337889194489 Val Loss: tensor(66.0575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1993 Loss: 1.7122454047203064 Val Loss: tensor(65.9359, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1994 Loss: 1.72060164809227 Val Loss: tensor(66.0721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1995 Loss: 1.7223871052265167 Val Loss: tensor(65.9232, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1996 Loss: 1.73635533452034 Val Loss: tensor(66.0941, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1997 Loss: 1.742948979139328 Val Loss: tensor(65.9118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1998 Loss: 1.7652938961982727 Val Loss: tensor(66.1259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1999 Loss: 1.779053270816803 Val Loss: tensor(65.9041, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2000 Loss: 1.8134284019470215 Val Loss: tensor(66.1696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2001 Loss: 1.837010532617569 Val Loss: tensor(65.9029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2002 Loss: 1.887339025735855 Val Loss: tensor(66.2247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2003 Loss: 1.9221188426017761 Val Loss: tensor(65.9106, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2004 Loss: 1.9902717471122742 Val Loss: tensor(66.2846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2005 Loss: 2.03276926279068 Val Loss: tensor(65.9255, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2006 Loss: 2.1139597594738007 Val Loss: tensor(66.3321, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2007 Loss: 2.151355057954788 Val Loss: tensor(65.9364, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2008 Loss: 2.2297731935977936 Val Loss: tensor(66.3418, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2009 Loss: 2.2403458654880524 Val Loss: tensor(65.9244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2010 Loss: 2.293352425098419 Val Loss: tensor(66.2984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2011 Loss: 2.2591612339019775 Val Loss: tensor(65.8763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2012 Loss: 2.2725119590759277 Val Loss: tensor(66.2185, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2013 Loss: 2.19564750790596 Val Loss: tensor(65.7998, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2014 Loss: 2.1738681197166443 Val Loss: tensor(66.1395, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2015 Loss: 2.0774512588977814 Val Loss: tensor(65.7132, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2016 Loss: 2.0396839678287506 Val Loss: tensor(66.0840, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2017 Loss: 1.9548605382442474 Val Loss: tensor(65.6341, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2018 Loss: 1.9249414205551147 Val Loss: tensor(66.0544, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2019 Loss: 1.874938815832138 Val Loss: tensor(65.5814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2020 Loss: 1.8678689301013947 Val Loss: tensor(66.0531, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2021 Loss: 1.8560675084590912 Val Loss: tensor(65.5666, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2022 Loss: 1.8724270462989807 Val Loss: tensor(66.0753, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2023 Loss: 1.8854436576366425 Val Loss: tensor(65.5809, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2024 Loss: 1.9176198542118073 Val Loss: tensor(66.1009, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2025 Loss: 1.9373462498188019 Val Loss: tensor(65.6058, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2026 Loss: 1.9735981822013855 Val Loss: tensor(66.1043, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2027 Loss: 1.9834569692611694 Val Loss: tensor(65.6226, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2028 Loss: 2.0080951154232025 Val Loss: tensor(66.0625, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2029 Loss: 1.995089828968048 Val Loss: tensor(65.6166, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2030 Loss: 1.9929759502410889 Val Loss: tensor(65.9650, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2031 Loss: 1.952976018190384 Val Loss: tensor(65.5835, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2032 Loss: 1.9199020266532898 Val Loss: tensor(65.8255, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2033 Loss: 1.8626873195171356 Val Loss: tensor(65.5343, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2034 Loss: 1.8109161853790283 Val Loss: tensor(65.6786, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2035 Loss: 1.754426747560501 Val Loss: tensor(65.4879, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2036 Loss: 1.7028149664402008 Val Loss: tensor(65.5598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2037 Loss: 1.6599921584129333 Val Loss: tensor(65.4585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2038 Loss: 1.6199774146080017 Val Loss: tensor(65.4856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2039 Loss: 1.5931412875652313 Val Loss: tensor(65.4481, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2040 Loss: 1.5659000873565674 Val Loss: tensor(65.4517, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2041 Loss: 1.551081120967865 Val Loss: tensor(65.4511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2042 Loss: 1.5329820811748505 Val Loss: tensor(65.4447, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2043 Loss: 1.5253972113132477 Val Loss: tensor(65.4608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2044 Loss: 1.5126840770244598 Val Loss: tensor(65.4512, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2045 Loss: 1.5090473890304565 Val Loss: tensor(65.4723, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2046 Loss: 1.499275267124176 Val Loss: tensor(65.4617, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2047 Loss: 1.497859925031662 Val Loss: tensor(65.4833, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2048 Loss: 1.4897263944149017 Val Loss: tensor(65.4718, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2049 Loss: 1.4898040890693665 Val Loss: tensor(65.4933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2050 Loss: 1.4827329814434052 Val Loss: tensor(65.4799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2051 Loss: 1.4841304123401642 Val Loss: tensor(65.5026, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2052 Loss: 1.4779956936836243 Val Loss: tensor(65.4853, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2053 Loss: 1.4808497726917267 Val Loss: tensor(65.5119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2054 Loss: 1.4758729040622711 Val Loss: tensor(65.4881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2055 Loss: 1.4806550741195679 Val Loss: tensor(65.5228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2056 Loss: 1.4774268567562103 Val Loss: tensor(65.4884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2057 Loss: 1.485003411769867 Val Loss: tensor(65.5375, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2058 Loss: 1.4847321212291718 Val Loss: tensor(65.4877, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2059 Loss: 1.4965856671333313 Val Loss: tensor(65.5592, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2060 Loss: 1.5015028417110443 Val Loss: tensor(65.4880, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2061 Loss: 1.5201513171195984 Val Loss: tensor(65.5934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2062 Loss: 1.5341657996177673 Val Loss: tensor(65.4932, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2063 Loss: 1.5637149214744568 Val Loss: tensor(65.6470, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2064 Loss: 1.5931905806064606 Val Loss: tensor(65.5099, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2065 Loss: 1.6398070454597473 Val Loss: tensor(65.7295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2066 Loss: 1.6934778988361359 Val Loss: tensor(65.5482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2067 Loss: 1.7640072107315063 Val Loss: tensor(65.8475, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2068 Loss: 1.8482442200183868 Val Loss: tensor(65.6164, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2069 Loss: 1.9420864582061768 Val Loss: tensor(65.9874, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2070 Loss: 2.0450115501880646 Val Loss: tensor(65.7003, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2071 Loss: 2.1343172192573547 Val Loss: tensor(66.0892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2072 Loss: 2.207919120788574 Val Loss: tensor(65.7361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2073 Loss: 2.2339504957199097 Val Loss: tensor(66.0617, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2074 Loss: 2.228854537010193 Val Loss: tensor(65.6562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2075 Loss: 2.1725027561187744 Val Loss: tensor(65.9054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2076 Loss: 2.108575850725174 Val Loss: tensor(65.5102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2077 Loss: 2.027974247932434 Val Loss: tensor(65.7646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2078 Loss: 1.954394280910492 Val Loss: tensor(65.4069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2079 Loss: 1.88905268907547 Val Loss: tensor(65.7170, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2080 Loss: 1.8199809789657593 Val Loss: tensor(65.3429, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2081 Loss: 1.7642445266246796 Val Loss: tensor(65.6916, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2082 Loss: 1.6957897245883942 Val Loss: tensor(65.2689, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2083 Loss: 1.6459856629371643 Val Loss: tensor(65.6369, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2084 Loss: 1.5842947661876678 Val Loss: tensor(65.1956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2085 Loss: 1.5477725565433502 Val Loss: tensor(65.5715, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2086 Loss: 1.5025483965873718 Val Loss: tensor(65.1513, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2087 Loss: 1.4813984632492065 Val Loss: tensor(65.5215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2088 Loss: 1.4526340067386627 Val Loss: tensor(65.1396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2089 Loss: 1.4418047964572906 Val Loss: tensor(65.4910, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2090 Loss: 1.4236924648284912 Val Loss: tensor(65.1482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2091 Loss: 1.4177702963352203 Val Loss: tensor(65.4738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2092 Loss: 1.4055296778678894 Val Loss: tensor(65.1636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2093 Loss: 1.4019315242767334 Val Loss: tensor(65.4650, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2094 Loss: 1.3931303322315216 Val Loss: tensor(65.1767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2095 Loss: 1.3914775550365448 Val Loss: tensor(65.4604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2096 Loss: 1.3849875628948212 Val Loss: tensor(65.1844, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2097 Loss: 1.3856079578399658 Val Loss: tensor(65.4583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2098 Loss: 1.3809150755405426 Val Loss: tensor(65.1878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2099 Loss: 1.3840793669223785 Val Loss: tensor(65.4593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2100 Loss: 1.3809851706027985 Val Loss: tensor(65.1892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2101 Loss: 1.3869478106498718 Val Loss: tensor(65.4643, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2102 Loss: 1.3854896426200867 Val Loss: tensor(65.1898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2103 Loss: 1.3947515785694122 Val Loss: tensor(65.4741, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2104 Loss: 1.3952156007289886 Val Loss: tensor(65.1904, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2105 Loss: 1.4083597958087921 Val Loss: tensor(65.4889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2106 Loss: 1.411274641752243 Val Loss: tensor(65.1915, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2107 Loss: 1.4288980960845947 Val Loss: tensor(65.5076, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2108 Loss: 1.4348194003105164 Val Loss: tensor(65.1936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2109 Loss: 1.4572730362415314 Val Loss: tensor(65.5296, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2110 Loss: 1.4664858877658844 Val Loss: tensor(65.1966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2111 Loss: 1.4935537278652191 Val Loss: tensor(65.5534, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2112 Loss: 1.505760133266449 Val Loss: tensor(65.2001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2113 Loss: 1.5363934934139252 Val Loss: tensor(65.5768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2114 Loss: 1.5506533980369568 Val Loss: tensor(65.2029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2115 Loss: 1.5831220149993896 Val Loss: tensor(65.5967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2116 Loss: 1.5983673632144928 Val Loss: tensor(65.2054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2117 Loss: 1.630619615316391 Val Loss: tensor(65.6113, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2118 Loss: 1.646533340215683 Val Loss: tensor(65.2101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2119 Loss: 1.6763575673103333 Val Loss: tensor(65.6207, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2120 Loss: 1.6934272646903992 Val Loss: tensor(65.2213, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2121 Loss: 1.7177930772304535 Val Loss: tensor(65.6246, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2122 Loss: 1.735464185476303 Val Loss: tensor(65.2405, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2123 Loss: 1.749866545200348 Val Loss: tensor(65.6188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2124 Loss: 1.763886272907257 Val Loss: tensor(65.2623, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2125 Loss: 1.7627344727516174 Val Loss: tensor(65.5925, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2126 Loss: 1.764473170042038 Val Loss: tensor(65.2735, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2127 Loss: 1.7438145279884338 Val Loss: tensor(65.5341, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2128 Loss: 1.7250033020973206 Val Loss: tensor(65.2615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2129 Loss: 1.686581552028656 Val Loss: tensor(65.4418, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2130 Loss: 1.6472744345664978 Val Loss: tensor(65.2247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2131 Loss: 1.60024294257164 Val Loss: tensor(65.3326, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2132 Loss: 1.550776720046997 Val Loss: tensor(65.1769, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2133 Loss: 1.5070306658744812 Val Loss: tensor(65.2333, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2134 Loss: 1.4600442349910736 Val Loss: tensor(65.1352, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2135 Loss: 1.4270022213459015 Val Loss: tensor(65.1618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2136 Loss: 1.3894788026809692 Val Loss: tensor(65.1074, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2137 Loss: 1.36808842420578 Val Loss: tensor(65.1190, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2138 Loss: 1.3407815396785736 Val Loss: tensor(65.0920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2139 Loss: 1.328526884317398 Val Loss: tensor(65.0964, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2140 Loss: 1.3093708753585815 Val Loss: tensor(65.0842, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2141 Loss: 1.3033845126628876 Val Loss: tensor(65.0854, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2142 Loss: 1.2900906801223755 Val Loss: tensor(65.0807, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2143 Loss: 1.2883583307266235 Val Loss: tensor(65.0807, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2144 Loss: 1.27932870388031 Val Loss: tensor(65.0798, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2145 Loss: 1.2808004319667816 Val Loss: tensor(65.0800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2146 Loss: 1.2752296328544617 Val Loss: tensor(65.0809, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2147 Loss: 1.2795986235141754 Val Loss: tensor(65.0829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2148 Loss: 1.2771941125392914 Val Loss: tensor(65.0840, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2149 Loss: 1.2848181426525116 Val Loss: tensor(65.0895, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2150 Loss: 1.2856789529323578 Val Loss: tensor(65.0898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2151 Loss: 1.297391265630722 Val Loss: tensor(65.1001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2152 Loss: 1.3017187416553497 Val Loss: tensor(65.0992, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2153 Loss: 1.3186397850513458 Val Loss: tensor(65.1149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2154 Loss: 1.326286792755127 Val Loss: tensor(65.1132, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2155 Loss: 1.3491836488246918 Val Loss: tensor(65.1329, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2156 Loss: 1.3588183224201202 Val Loss: tensor(65.1303, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2157 Loss: 1.3869620561599731 Val Loss: tensor(65.1498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2158 Loss: 1.3952912092208862 Val Loss: tensor(65.1453, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2159 Loss: 1.4251852631568909 Val Loss: tensor(65.1571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2160 Loss: 1.4277442395687103 Val Loss: tensor(65.1487, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2161 Loss: 1.4539886116981506 Val Loss: tensor(65.1470, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2162 Loss: 1.4484898149967194 Val Loss: tensor(65.1330, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2163 Loss: 1.4677595496177673 Val Loss: tensor(65.1216, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2164 Loss: 1.457031011581421 Val Loss: tensor(65.1001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2165 Loss: 1.4712475538253784 Val Loss: tensor(65.0958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2166 Loss: 1.4607944190502167 Val Loss: tensor(65.0597, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2167 Loss: 1.4745542407035828 Val Loss: tensor(65.0860, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2168 Loss: 1.4675599336624146 Val Loss: tensor(65.0191, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2169 Loss: 1.4835197925567627 Val Loss: tensor(65.0977, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2170 Loss: 1.4791030287742615 Val Loss: tensor(64.9780, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2171 Loss: 1.496599167585373 Val Loss: tensor(65.1241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2172 Loss: 1.4915776252746582 Val Loss: tensor(64.9336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2173 Loss: 1.5082554817199707 Val Loss: tensor(65.1549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2174 Loss: 1.4991559982299805 Val Loss: tensor(64.8846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2175 Loss: 1.512655794620514 Val Loss: tensor(65.1812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2176 Loss: 1.4970229268074036 Val Loss: tensor(64.8314, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2177 Loss: 1.506116807460785 Val Loss: tensor(65.1987, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2178 Loss: 1.4834425449371338 Val Loss: tensor(64.7761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2179 Loss: 1.4884250462055206 Val Loss: tensor(65.2065, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2180 Loss: 1.4603938460350037 Val Loss: tensor(64.7214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2181 Loss: 1.4627995193004608 Val Loss: tensor(65.2066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2182 Loss: 1.4325732290744781 Val Loss: tensor(64.6701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2183 Loss: 1.4343666434288025 Val Loss: tensor(65.2021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2184 Loss: 1.4053003787994385 Val Loss: tensor(64.6249, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2185 Loss: 1.4081060290336609 Val Loss: tensor(65.1957, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2186 Loss: 1.3827314376831055 Val Loss: tensor(64.5878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2187 Loss: 1.3874353468418121 Val Loss: tensor(65.1891, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2188 Loss: 1.3670916259288788 Val Loss: tensor(64.5594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2189 Loss: 1.3738881051540375 Val Loss: tensor(65.1830, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2190 Loss: 1.3588115572929382 Val Loss: tensor(64.5395, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2191 Loss: 1.3672653436660767 Val Loss: tensor(65.1760, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2192 Loss: 1.3568967878818512 Val Loss: tensor(64.5270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2193 Loss: 1.3660828769207 Val Loss: tensor(65.1656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2194 Loss: 1.3591974079608917 Val Loss: tensor(64.5210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2195 Loss: 1.3676833808422089 Val Loss: tensor(65.1477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2196 Loss: 1.362568885087967 Val Loss: tensor(64.5204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2197 Loss: 1.3685866594314575 Val Loss: tensor(65.1189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2198 Loss: 1.363255351781845 Val Loss: tensor(64.5236, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2199 Loss: 1.3651623725891113 Val Loss: tensor(65.0767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2200 Loss: 1.357815444469452 Val Loss: tensor(64.5287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2201 Loss: 1.3547593355178833 Val Loss: tensor(65.0214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2202 Loss: 1.3445030748844147 Val Loss: tensor(64.5343, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2203 Loss: 1.3370243608951569 Val Loss: tensor(64.9566, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2204 Loss: 1.3242447972297668 Val Loss: tensor(64.5403, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2205 Loss: 1.3142525255680084 Val Loss: tensor(64.8886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2206 Loss: 1.3005186319351196 Val Loss: tensor(64.5474, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2207 Loss: 1.2904375195503235 Val Loss: tensor(64.8242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2208 Loss: 1.277725338935852 Val Loss: tensor(64.5572, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2209 Loss: 1.269646555185318 Val Loss: tensor(64.7685, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2210 Loss: 1.2595672011375427 Val Loss: tensor(64.5704, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2211 Loss: 1.2547720074653625 Val Loss: tensor(64.7240, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2212 Loss: 1.2482290267944336 Val Loss: tensor(64.5871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2213 Loss: 1.2472298741340637 Val Loss: tensor(64.6906, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2214 Loss: 1.2444856464862823 Val Loss: tensor(64.6069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2215 Loss: 1.2473227381706238 Val Loss: tensor(64.6668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2216 Loss: 1.2483303844928741 Val Loss: tensor(64.6293, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2217 Loss: 1.2548770904541016 Val Loss: tensor(64.6505, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2218 Loss: 1.2594042122364044 Val Loss: tensor(64.6535, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2219 Loss: 1.2694810032844543 Val Loss: tensor(64.6394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2220 Loss: 1.2772904336452484 Val Loss: tensor(64.6791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2221 Loss: 1.2906388640403748 Val Loss: tensor(64.6314, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2222 Loss: 1.301378220319748 Val Loss: tensor(64.7051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2223 Loss: 1.3176013827323914 Val Loss: tensor(64.6250, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2224 Loss: 1.3307529091835022 Val Loss: tensor(64.7305, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2225 Loss: 1.3491324484348297 Val Loss: tensor(64.6181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2226 Loss: 1.3639424741268158 Val Loss: tensor(64.7532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2227 Loss: 1.3832583129405975 Val Loss: tensor(64.6093, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2228 Loss: 1.3986729681491852 Val Loss: tensor(64.7714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2229 Loss: 1.4169707298278809 Val Loss: tensor(64.5969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2230 Loss: 1.4316754639148712 Val Loss: tensor(64.7835, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2231 Loss: 1.446214348077774 Val Loss: tensor(64.5800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2232 Loss: 1.4585406184196472 Val Loss: tensor(64.7882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2233 Loss: 1.4658271968364716 Val Loss: tensor(64.5579, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2234 Loss: 1.473752498626709 Val Loss: tensor(64.7849, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2235 Loss: 1.4700863063335419 Val Loss: tensor(64.5302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2236 Loss: 1.4712918996810913 Val Loss: tensor(64.7726, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2237 Loss: 1.4539133608341217 Val Loss: tensor(64.4966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2238 Loss: 1.4467298984527588 Val Loss: tensor(64.7498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2239 Loss: 1.415580302476883 Val Loss: tensor(64.4573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2240 Loss: 1.4005332291126251 Val Loss: tensor(64.7157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2241 Loss: 1.3594712316989899 Val Loss: tensor(64.4147, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2242 Loss: 1.3400476574897766 Val Loss: tensor(64.6723, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2243 Loss: 1.295823335647583 Val Loss: tensor(64.3739, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2244 Loss: 1.2770408987998962 Val Loss: tensor(64.6256, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2245 Loss: 1.2362259030342102 Val Loss: tensor(64.3408, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2246 Loss: 1.221697449684143 Val Loss: tensor(64.5823, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2247 Loss: 1.1879225075244904 Val Loss: tensor(64.3180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2248 Loss: 1.1787604093551636 Val Loss: tensor(64.5465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2249 Loss: 1.1526204347610474 Val Loss: tensor(64.3047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2250 Loss: 1.1481414139270782 Val Loss: tensor(64.5194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2251 Loss: 1.1285537779331207 Val Loss: tensor(64.2976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2252 Loss: 1.1276638805866241 Val Loss: tensor(64.5006, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2253 Loss: 1.1131510138511658 Val Loss: tensor(64.2937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2254 Loss: 1.1149027049541473 Val Loss: tensor(64.4888, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2255 Loss: 1.1041750609874725 Val Loss: tensor(64.2907, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2256 Loss: 1.1081060767173767 Val Loss: tensor(64.4826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2257 Loss: 1.1002652645111084 Val Loss: tensor(64.2875, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2258 Loss: 1.106222540140152 Val Loss: tensor(64.4808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2259 Loss: 1.1007717549800873 Val Loss: tensor(64.2838, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2260 Loss: 1.1089164018630981 Val Loss: tensor(64.4832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2261 Loss: 1.105694055557251 Val Loss: tensor(64.2801, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2262 Loss: 1.1164015531539917 Val Loss: tensor(64.4895, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2263 Loss: 1.1156043708324432 Val Loss: tensor(64.2769, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2264 Loss: 1.1294668018817902 Val Loss: tensor(64.5006, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2265 Loss: 1.1315976083278656 Val Loss: tensor(64.2749, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2266 Loss: 1.1493645310401917 Val Loss: tensor(64.5173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2267 Loss: 1.1552619338035583 Val Loss: tensor(64.2752, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2268 Loss: 1.1777836084365845 Val Loss: tensor(64.5405, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2269 Loss: 1.1885460913181305 Val Loss: tensor(64.2789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2270 Loss: 1.2165608406066895 Val Loss: tensor(64.5709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2271 Loss: 1.2333254218101501 Val Loss: tensor(64.2871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2272 Loss: 1.2670396864414215 Val Loss: tensor(64.6084, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2273 Loss: 1.2903740108013153 Val Loss: tensor(64.3000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2274 Loss: 1.3287054300308228 Val Loss: tensor(64.6509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2275 Loss: 1.3575615882873535 Val Loss: tensor(64.3164, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2276 Loss: 1.3971292674541473 Val Loss: tensor(64.6930, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2277 Loss: 1.427352249622345 Val Loss: tensor(64.3323, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2278 Loss: 1.4616520702838898 Val Loss: tensor(64.7248, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2279 Loss: 1.4850851595401764 Val Loss: tensor(64.3394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2280 Loss: 1.5054344236850739 Val Loss: tensor(64.7337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2281 Loss: 1.5115634500980377 Val Loss: tensor(64.3282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2282 Loss: 1.5108951926231384 Val Loss: tensor(64.7094, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2283 Loss: 1.4925579726696014 Val Loss: tensor(64.2926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2284 Loss: 1.4706698656082153 Val Loss: tensor(64.6514, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2285 Loss: 1.4300835728645325 Val Loss: tensor(64.2361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2286 Loss: 1.395271360874176 Val Loss: tensor(64.5728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2287 Loss: 1.3436749279499054 Val Loss: tensor(64.1713, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2288 Loss: 1.3076775968074799 Val Loss: tensor(64.4936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2289 Loss: 1.2577925324440002 Val Loss: tensor(64.1132, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2290 Loss: 1.2288815081119537 Val Loss: tensor(64.4294, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2291 Loss: 1.1883914470672607 Val Loss: tensor(64.0699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2292 Loss: 1.1690113544464111 Val Loss: tensor(64.3855, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2293 Loss: 1.1396335363388062 Val Loss: tensor(64.0414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2294 Loss: 1.1284503042697906 Val Loss: tensor(64.3597, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2295 Loss: 1.1085461974143982 Val Loss: tensor(64.0230, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2296 Loss: 1.103218287229538 Val Loss: tensor(64.3466, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2297 Loss: 1.0902608633041382 Val Loss: tensor(64.0094, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2298 Loss: 1.0889734625816345 Val Loss: tensor(64.3423, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2299 Loss: 1.080809772014618 Val Loss: tensor(63.9973, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2300 Loss: 1.0825663208961487 Val Loss: tensor(64.3440, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2301 Loss: 1.077681839466095 Val Loss: tensor(63.9850, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2302 Loss: 1.0821104943752289 Val Loss: tensor(64.3503, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2303 Loss: 1.079555720090866 Val Loss: tensor(63.9722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2304 Loss: 1.0866540968418121 Val Loss: tensor(64.3604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2305 Loss: 1.0858959257602692 Val Loss: tensor(63.9593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2306 Loss: 1.0959026217460632 Val Loss: tensor(64.3736, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2307 Loss: 1.0966554880142212 Val Loss: tensor(63.9470, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2308 Loss: 1.1099111437797546 Val Loss: tensor(64.3896, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2309 Loss: 1.1120510399341583 Val Loss: tensor(63.9361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2310 Loss: 1.1289015710353851 Val Loss: tensor(64.4080, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2311 Loss: 1.132427603006363 Val Loss: tensor(63.9279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2312 Loss: 1.1530581712722778 Val Loss: tensor(64.4279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2313 Loss: 1.1579117476940155 Val Loss: tensor(63.9236, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2314 Loss: 1.18207648396492 Val Loss: tensor(64.4477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2315 Loss: 1.1879797577857971 Val Loss: tensor(63.9243, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2316 Loss: 1.2144398987293243 Val Loss: tensor(64.4644, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2317 Loss: 1.2204508483409882 Val Loss: tensor(63.9302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2318 Loss: 1.2462301552295685 Val Loss: tensor(64.4731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2319 Loss: 1.2500136494636536 Val Loss: tensor(63.9386, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2320 Loss: 1.269945353269577 Val Loss: tensor(64.4658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2321 Loss: 1.2672746777534485 Val Loss: tensor(63.9425, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2322 Loss: 1.2752783000469208 Val Loss: tensor(64.4337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2323 Loss: 1.261267989873886 Val Loss: tensor(63.9332, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2324 Loss: 1.254313975572586 Val Loss: tensor(64.3729, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2325 Loss: 1.227245569229126 Val Loss: tensor(63.9067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2326 Loss: 1.2090406715869904 Val Loss: tensor(64.2912, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2327 Loss: 1.173449695110321 Val Loss: tensor(63.8705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2328 Loss: 1.1524176001548767 Val Loss: tensor(64.2058, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2329 Loss: 1.1163675487041473 Val Loss: tensor(63.8386, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2330 Loss: 1.0993443727493286 Val Loss: tensor(64.1321, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2331 Loss: 1.0688410997390747 Val Loss: tensor(63.8194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2332 Loss: 1.0580176413059235 Val Loss: tensor(64.0753, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2333 Loss: 1.0349151194095612 Val Loss: tensor(63.8123, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2334 Loss: 1.0293737649917603 Val Loss: tensor(64.0342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2335 Loss: 1.0129058957099915 Val Loss: tensor(63.8125, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2336 Loss: 1.0110029578208923 Val Loss: tensor(64.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2337 Loss: 0.9996577203273773 Val Loss: tensor(63.8156, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2338 Loss: 1.0001893043518066 Val Loss: tensor(63.9851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2339 Loss: 0.992674857378006 Val Loss: tensor(63.8197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2340 Loss: 0.9949437975883484 Val Loss: tensor(63.9719, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2341 Loss: 0.9903993010520935 Val Loss: tensor(63.8242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2342 Loss: 0.9940474331378937 Val Loss: tensor(63.9639, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2343 Loss: 0.9919962286949158 Val Loss: tensor(63.8293, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2344 Loss: 0.9968938529491425 Val Loss: tensor(63.9608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2345 Loss: 0.9972540438175201 Val Loss: tensor(63.8351, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2346 Loss: 1.0034281313419342 Val Loss: tensor(63.9623, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2347 Loss: 1.0064679980278015 Val Loss: tensor(63.8416, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2348 Loss: 1.0141336619853973 Val Loss: tensor(63.9690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2349 Loss: 1.0206742882728577 Val Loss: tensor(63.8494, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2350 Loss: 1.0303253531455994 Val Loss: tensor(63.9826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2351 Loss: 1.042112946510315 Val Loss: tensor(63.8597, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2352 Loss: 1.0546909868717194 Val Loss: tensor(64.0064, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2353 Loss: 1.0751696527004242 Val Loss: tensor(63.8759, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2354 Loss: 1.09261092543602 Val Loss: tensor(64.0468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2355 Loss: 1.128356158733368 Val Loss: tensor(63.9047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2356 Loss: 1.1544860005378723 Val Loss: tensor(64.1153, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2357 Loss: 1.217739313840866 Val Loss: tensor(63.9594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2358 Loss: 1.2601313591003418 Val Loss: tensor(64.2304, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2359 Loss: 1.3709332644939423 Val Loss: tensor(64.0641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2360 Loss: 1.4422672390937805 Val Loss: tensor(64.4123, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2361 Loss: 1.6191062927246094 Val Loss: tensor(64.2423, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2362 Loss: 1.725986659526825 Val Loss: tensor(64.6361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2363 Loss: 1.9258012175559998 Val Loss: tensor(64.4353, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2364 Loss: 2.006132036447525 Val Loss: tensor(64.7132, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2365 Loss: 2.0386467278003693 Val Loss: tensor(64.3910, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2366 Loss: 1.9434210062026978 Val Loss: tensor(64.3470, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2367 Loss: 1.7334272265434265 Val Loss: tensor(63.9977, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2368 Loss: 1.5443127751350403 Val Loss: tensor(63.7494, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2369 Loss: 1.345994919538498 Val Loss: tensor(63.6539, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2370 Loss: 1.2535622119903564 Val Loss: tensor(63.5278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2371 Loss: 1.1602199375629425 Val Loss: tensor(63.5761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2372 Loss: 1.1332367062568665 Val Loss: tensor(63.5694, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2373 Loss: 1.0844405889511108 Val Loss: tensor(63.5315, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2374 Loss: 1.0628143548965454 Val Loss: tensor(63.5943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2375 Loss: 1.0212382078170776 Val Loss: tensor(63.4747, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2376 Loss: 1.0018846094608307 Val Loss: tensor(63.5901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2377 Loss: 0.9708893001079559 Val Loss: tensor(63.4523, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2378 Loss: 0.9607300162315369 Val Loss: tensor(63.5697, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2379 Loss: 0.9436384439468384 Val Loss: tensor(63.4492, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2380 Loss: 0.9410847425460815 Val Loss: tensor(63.5620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2381 Loss: 0.933462381362915 Val Loss: tensor(63.4506, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2382 Loss: 0.9339210391044617 Val Loss: tensor(63.5608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2383 Loss: 0.930754154920578 Val Loss: tensor(63.4475, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2384 Loss: 0.9315727353096008 Val Loss: tensor(63.5694, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2385 Loss: 0.930228978395462 Val Loss: tensor(63.4464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2386 Loss: 0.9314272999763489 Val Loss: tensor(63.5841, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2387 Loss: 0.9308426678180695 Val Loss: tensor(63.4422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2388 Loss: 0.9333881735801697 Val Loss: tensor(63.5939, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2389 Loss: 0.9334007501602173 Val Loss: tensor(63.4331, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2390 Loss: 0.9378582537174225 Val Loss: tensor(63.6038, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2391 Loss: 0.9386686086654663 Val Loss: tensor(63.4209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2392 Loss: 0.9457094073295593 Val Loss: tensor(63.6147, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2393 Loss: 0.9476734101772308 Val Loss: tensor(63.4077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2394 Loss: 0.958323746919632 Val Loss: tensor(63.6325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2395 Loss: 0.9619772136211395 Val Loss: tensor(63.3981, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2396 Loss: 0.9777994751930237 Val Loss: tensor(63.6583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2397 Loss: 0.9839847683906555 Val Loss: tensor(63.3912, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2398 Loss: 1.006872147321701 Val Loss: tensor(63.6909, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2399 Loss: 1.0169021487236023 Val Loss: tensor(63.3896, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2400 Loss: 1.0491212606430054 Val Loss: tensor(63.7321, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2401 Loss: 1.0645319521427155 Val Loss: tensor(63.3956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2402 Loss: 1.108374685049057 Val Loss: tensor(63.7814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2403 Loss: 1.1304314732551575 Val Loss: tensor(63.4105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2404 Loss: 1.187232792377472 Val Loss: tensor(63.8373, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2405 Loss: 1.215519368648529 Val Loss: tensor(63.4354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2406 Loss: 1.2834000885486603 Val Loss: tensor(63.8940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2407 Loss: 1.3134806454181671 Val Loss: tensor(63.4658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2408 Loss: 1.3840910494327545 Val Loss: tensor(63.9383, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2409 Loss: 1.4049330949783325 Val Loss: tensor(63.4906, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2410 Loss: 1.461833506822586 Val Loss: tensor(63.9516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2411 Loss: 1.4569066762924194 Val Loss: tensor(63.4937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2412 Loss: 1.4813095927238464 Val Loss: tensor(63.9191, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2413 Loss: 1.4387720823287964 Val Loss: tensor(63.4650, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2414 Loss: 1.4238772094249725 Val Loss: tensor(63.8430, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2415 Loss: 1.350595623254776 Val Loss: tensor(63.4103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2416 Loss: 1.3106075525283813 Val Loss: tensor(63.7454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2417 Loss: 1.2306171357631683 Val Loss: tensor(63.3473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2418 Loss: 1.1888113915920258 Val Loss: tensor(63.6545, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2419 Loss: 1.1238843202590942 Val Loss: tensor(63.2925, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2420 Loss: 1.0940077006816864 Val Loss: tensor(63.5869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2421 Loss: 1.051282674074173 Val Loss: tensor(63.2519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2422 Loss: 1.0343676209449768 Val Loss: tensor(63.5448, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2423 Loss: 1.0103676617145538 Val Loss: tensor(63.2247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2424 Loss: 1.002159208059311 Val Loss: tensor(63.5230, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2425 Loss: 0.9907538294792175 Val Loss: tensor(63.2076, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2426 Loss: 0.9874368906021118 Val Loss: tensor(63.5157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2427 Loss: 0.9837106764316559 Val Loss: tensor(63.1957, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2428 Loss: 0.9834765493869781 Val Loss: tensor(63.5179, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2429 Loss: 0.9843946397304535 Val Loss: tensor(63.1853, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2430 Loss: 0.9868887066841125 Val Loss: tensor(63.5267, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2431 Loss: 0.990741103887558 Val Loss: tensor(63.1750, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2432 Loss: 0.9962742626667023 Val Loss: tensor(63.5399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2433 Loss: 1.0020798444747925 Val Loss: tensor(63.1656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2434 Loss: 1.0111164152622223 Val Loss: tensor(63.5566, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2435 Loss: 1.0182176530361176 Val Loss: tensor(63.1585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2436 Loss: 1.0311394333839417 Val Loss: tensor(63.5759, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2437 Loss: 1.0388689041137695 Val Loss: tensor(63.1549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2438 Loss: 1.0556810796260834 Val Loss: tensor(63.5967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2439 Loss: 1.0632047355175018 Val Loss: tensor(63.1542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2440 Loss: 1.0833284854888916 Val Loss: tensor(63.6169, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2441 Loss: 1.0895011126995087 Val Loss: tensor(63.1553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2442 Loss: 1.111726462841034 Val Loss: tensor(63.6332, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2443 Loss: 1.114995688199997 Val Loss: tensor(63.1556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2444 Loss: 1.137501299381256 Val Loss: tensor(63.6414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2445 Loss: 1.1362162232398987 Val Loss: tensor(63.1526, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2446 Loss: 1.1568577587604523 Val Loss: tensor(63.6381, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2447 Loss: 1.1497780978679657 Val Loss: tensor(63.1442, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2448 Loss: 1.1667174994945526 Val Loss: tensor(63.6226, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2449 Loss: 1.1535252034664154 Val Loss: tensor(63.1302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2450 Loss: 1.1657208502292633 Val Loss: tensor(63.5966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2451 Loss: 1.1472067534923553 Val Loss: tensor(63.1117, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2452 Loss: 1.1546510756015778 Val Loss: tensor(63.5640, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2453 Loss: 1.132394790649414 Val Loss: tensor(63.0910, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2454 Loss: 1.135812222957611 Val Loss: tensor(63.5295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2455 Loss: 1.1117304861545563 Val Loss: tensor(63.0698, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2456 Loss: 1.1121598184108734 Val Loss: tensor(63.4964, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2457 Loss: 1.0880519151687622 Val Loss: tensor(63.0494, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2458 Loss: 1.0865886807441711 Val Loss: tensor(63.4664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2459 Loss: 1.0639743208885193 Val Loss: tensor(63.0303, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2460 Loss: 1.061598151922226 Val Loss: tensor(63.4408, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2461 Loss: 1.041687160730362 Val Loss: tensor(63.0127, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2462 Loss: 1.039259135723114 Val Loss: tensor(63.4199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2463 Loss: 1.023059755563736 Val Loss: tensor(62.9967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2464 Loss: 1.0213162004947662 Val Loss: tensor(63.4042, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2465 Loss: 1.009826898574829 Val Loss: tensor(62.9832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2466 Loss: 1.0094423294067383 Val Loss: tensor(63.3947, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2467 Loss: 1.0037657022476196 Val Loss: tensor(62.9735, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2468 Loss: 1.0053642988204956 Val Loss: tensor(63.3923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2469 Loss: 1.0068193078041077 Val Loss: tensor(62.9700, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2470 Loss: 1.0108833014965057 Val Loss: tensor(63.3979, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2471 Loss: 1.020885318517685 Val Loss: tensor(62.9748, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2472 Loss: 1.027490884065628 Val Loss: tensor(63.4106, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2473 Loss: 1.046886295080185 Val Loss: tensor(62.9881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2474 Loss: 1.0553016662597656 Val Loss: tensor(63.4267, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2475 Loss: 1.0833279192447662 Val Loss: tensor(63.0073, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2476 Loss: 1.0921882688999176 Val Loss: tensor(63.4387, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2477 Loss: 1.125901460647583 Val Loss: tensor(63.0253, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2478 Loss: 1.1345238983631134 Val Loss: tensor(63.4374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2479 Loss: 1.1701837480068207 Val Loss: tensor(63.0367, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2480 Loss: 1.1801498532295227 Val Loss: tensor(63.4188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2481 Loss: 1.2154633700847626 Val Loss: tensor(63.0437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2482 Loss: 1.2288463413715363 Val Loss: tensor(63.3843, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2483 Loss: 1.2620081901550293 Val Loss: tensor(63.0539, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2484 Loss: 1.2765282690525055 Val Loss: tensor(63.3338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2485 Loss: 1.3021955490112305 Val Loss: tensor(63.0695, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2486 Loss: 1.3087559044361115 Val Loss: tensor(63.2619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2487 Loss: 1.317683458328247 Val Loss: tensor(63.0824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2488 Loss: 1.3053601682186127 Val Loss: tensor(63.1657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2489 Loss: 1.2914234399795532 Val Loss: tensor(63.0818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2490 Loss: 1.256949245929718 Val Loss: tensor(63.0547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2491 Loss: 1.225059151649475 Val Loss: tensor(63.0647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2492 Loss: 1.1775439381599426 Val Loss: tensor(62.9469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2493 Loss: 1.1411854326725006 Val Loss: tensor(63.0375, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2494 Loss: 1.0949099361896515 Val Loss: tensor(62.8576, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2495 Loss: 1.0652838945388794 Val Loss: tensor(63.0101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2496 Loss: 1.029375582933426 Val Loss: tensor(62.7928, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2497 Loss: 1.0096985399723053 Val Loss: tensor(62.9885, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2498 Loss: 0.9857966601848602 Val Loss: tensor(62.7509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2499 Loss: 0.9741616249084473 Val Loss: tensor(62.9738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2500 Loss: 0.9598541557788849 Val Loss: tensor(62.7272, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2501 Loss: 0.9534389078617096 Val Loss: tensor(62.9647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2502 Loss: 0.9455433189868927 Val Loss: tensor(62.7157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2503 Loss: 0.9423932135105133 Val Loss: tensor(62.9598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2504 Loss: 0.9384198486804962 Val Loss: tensor(62.7100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2505 Loss: 0.9376547038555145 Val Loss: tensor(62.9586, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2506 Loss: 0.9360400140285492 Val Loss: tensor(62.7062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2507 Loss: 0.9374384582042694 Val Loss: tensor(62.9604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2508 Loss: 0.9373211562633514 Val Loss: tensor(62.7030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2509 Loss: 0.9409410357475281 Val Loss: tensor(62.9650, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2510 Loss: 0.9418683350086212 Val Loss: tensor(62.7001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2511 Loss: 0.9478407800197601 Val Loss: tensor(62.9720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2512 Loss: 0.9495759308338165 Val Loss: tensor(62.6978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2513 Loss: 0.9580188393592834 Val Loss: tensor(62.9807, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2514 Loss: 0.9603417217731476 Val Loss: tensor(62.6961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2515 Loss: 0.9713187515735626 Val Loss: tensor(62.9908, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2516 Loss: 0.9739915430545807 Val Loss: tensor(62.6951, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2517 Loss: 0.9874602854251862 Val Loss: tensor(63.0015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2518 Loss: 0.990139901638031 Val Loss: tensor(62.6948, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2519 Loss: 1.0059167444705963 Val Loss: tensor(63.0118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2520 Loss: 1.0080814957618713 Val Loss: tensor(62.6946, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2521 Loss: 1.0257684290409088 Val Loss: tensor(63.0203, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2522 Loss: 1.0267156660556793 Val Loss: tensor(62.6943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2523 Loss: 1.0455530881881714 Val Loss: tensor(63.0251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2524 Loss: 1.0443991720676422 Val Loss: tensor(62.6928, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2525 Loss: 1.0633523762226105 Val Loss: tensor(63.0244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2526 Loss: 1.0590965747833252 Val Loss: tensor(62.6894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2527 Loss: 1.0768709778785706 Val Loss: tensor(63.0166, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2528 Loss: 1.0685913860797882 Val Loss: tensor(62.6833, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2529 Loss: 1.0838299691677094 Val Loss: tensor(63.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2530 Loss: 1.0708443224430084 Val Loss: tensor(62.6738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2531 Loss: 1.082476258277893 Val Loss: tensor(62.9770, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2532 Loss: 1.064634919166565 Val Loss: tensor(62.6608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2533 Loss: 1.0721005499362946 Val Loss: tensor(62.9463, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2534 Loss: 1.049914687871933 Val Loss: tensor(62.6447, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2535 Loss: 1.0534096956253052 Val Loss: tensor(62.9103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2536 Loss: 1.0280417203903198 Val Loss: tensor(62.6262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2537 Loss: 1.0284368693828583 Val Loss: tensor(62.8716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2538 Loss: 1.0015538930892944 Val Loss: tensor(62.6064, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2539 Loss: 1.000163495540619 Val Loss: tensor(62.8327, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2540 Loss: 0.9735509157180786 Val Loss: tensor(62.5865, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2541 Loss: 0.9717639088630676 Val Loss: tensor(62.7956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2542 Loss: 0.946908712387085 Val Loss: tensor(62.5675, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2543 Loss: 0.9458309710025787 Val Loss: tensor(62.7619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2544 Loss: 0.9237585961818695 Val Loss: tensor(62.5500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2545 Loss: 0.9241126477718353 Val Loss: tensor(62.7328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2546 Loss: 0.9053220152854919 Val Loss: tensor(62.5346, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2547 Loss: 0.9075116515159607 Val Loss: tensor(62.7087, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2548 Loss: 0.8921831548213959 Val Loss: tensor(62.5218, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2549 Loss: 0.8964665234088898 Val Loss: tensor(62.6903, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2550 Loss: 0.8846778273582458 Val Loss: tensor(62.5124, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2551 Loss: 0.8913339674472809 Val Loss: tensor(62.6783, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2552 Loss: 0.883359968662262 Val Loss: tensor(62.5075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2553 Loss: 0.8927792608737946 Val Loss: tensor(62.6738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2554 Loss: 0.8892152011394501 Val Loss: tensor(62.5088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2555 Loss: 0.9018669724464417 Val Loss: tensor(62.6781, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2556 Loss: 0.9036413729190826 Val Loss: tensor(62.5185, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2557 Loss: 0.9197037220001221 Val Loss: tensor(62.6915, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2558 Loss: 0.9276452958583832 Val Loss: tensor(62.5372, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2559 Loss: 0.9463201463222504 Val Loss: tensor(62.7114, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2560 Loss: 0.9600976407527924 Val Loss: tensor(62.5628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2561 Loss: 0.9789721667766571 Val Loss: tensor(62.7297, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2562 Loss: 0.995818555355072 Val Loss: tensor(62.5872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2563 Loss: 1.011566013097763 Val Loss: tensor(62.7338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2564 Loss: 1.0269869267940521 Val Loss: tensor(62.6000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2565 Loss: 1.0386178493499756 Val Loss: tensor(62.7151, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2566 Loss: 1.0505065321922302 Val Loss: tensor(62.5991, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2567 Loss: 1.0613453686237335 Val Loss: tensor(62.6790, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2568 Loss: 1.0729461312294006 Val Loss: tensor(62.5956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2569 Loss: 1.0871561467647552 Val Loss: tensor(62.6397, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2570 Loss: 1.1038272380828857 Val Loss: tensor(62.6030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2571 Loss: 1.121431976556778 Val Loss: tensor(62.6063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2572 Loss: 1.1450304090976715 Val Loss: tensor(62.6235, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2573 Loss: 1.1612818539142609 Val Loss: tensor(62.5773, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2574 Loss: 1.1882497370243073 Val Loss: tensor(62.6465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2575 Loss: 1.1959796845912933 Val Loss: tensor(62.5466, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2576 Loss: 1.2192846238613129 Val Loss: tensor(62.6571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2577 Loss: 1.2113425731658936 Val Loss: tensor(62.5093, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2578 Loss: 1.2240759134292603 Val Loss: tensor(62.6450, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2579 Loss: 1.1963253915309906 Val Loss: tensor(62.4634, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2580 Loss: 1.1954422891139984 Val Loss: tensor(62.6088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2581 Loss: 1.1499103605747223 Val Loss: tensor(62.4097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2582 Loss: 1.1378175020217896 Val Loss: tensor(62.5568, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2583 Loss: 1.0830475687980652 Val Loss: tensor(62.3519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2584 Loss: 1.0655733942985535 Val Loss: tensor(62.4999, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2585 Loss: 1.012256145477295 Val Loss: tensor(62.2943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2586 Loss: 0.9949986934661865 Val Loss: tensor(62.4477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2587 Loss: 0.95085209608078 Val Loss: tensor(62.2420, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2588 Loss: 0.9369987547397614 Val Loss: tensor(62.4057, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2589 Loss: 0.9048150181770325 Val Loss: tensor(62.1978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2590 Loss: 0.8951870501041412 Val Loss: tensor(62.3759, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2591 Loss: 0.8740013837814331 Val Loss: tensor(62.1622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2592 Loss: 0.8681544959545135 Val Loss: tensor(62.3579, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2593 Loss: 0.8555153012275696 Val Loss: tensor(62.1340, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2594 Loss: 0.8527481555938721 Val Loss: tensor(62.3506, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2595 Loss: 0.846364364027977 Val Loss: tensor(62.1112, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2596 Loss: 0.8462408483028412 Val Loss: tensor(62.3526, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2597 Loss: 0.8446643799543381 Val Loss: tensor(62.0926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2598 Loss: 0.8472208678722382 Val Loss: tensor(62.3636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2599 Loss: 0.8499622941017151 Val Loss: tensor(62.0781, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2600 Loss: 0.8557659983634949 Val Loss: tensor(62.3837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2601 Loss: 0.8631585538387299 Val Loss: tensor(62.0690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2602 Loss: 0.8733574748039246 Val Loss: tensor(62.4143, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2603 Loss: 0.8863784074783325 Val Loss: tensor(62.0682, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2604 Loss: 0.9026904106140137 Val Loss: tensor(62.4574, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2605 Loss: 0.9228403270244598 Val Loss: tensor(62.0795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2606 Loss: 0.9471747279167175 Val Loss: tensor(62.5140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2607 Loss: 0.9759409725666046 Val Loss: tensor(62.1072, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2608 Loss: 1.0096955299377441 Val Loss: tensor(62.5818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2609 Loss: 1.0472595691680908 Val Loss: tensor(62.1533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2610 Loss: 1.0898889005184174 Val Loss: tensor(62.6511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2611 Loss: 1.1328978836536407 Val Loss: tensor(62.2131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2612 Loss: 1.1808688640594482 Val Loss: tensor(62.7025, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2613 Loss: 1.2205676436424255 Val Loss: tensor(62.2736, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2614 Loss: 1.2678948938846588 Val Loss: tensor(62.7121, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2615 Loss: 1.2913925349712372 Val Loss: tensor(62.3187, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2616 Loss: 1.3311097621917725 Val Loss: tensor(62.6633, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2617 Loss: 1.326007753610611 Val Loss: tensor(62.3385, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2618 Loss: 1.3498916625976562 Val Loss: tensor(62.5546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2619 Loss: 1.3098748922348022 Val Loss: tensor(62.3316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2620 Loss: 1.309817612171173 Val Loss: tensor(62.4001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2621 Loss: 1.2408463954925537 Val Loss: tensor(62.2991, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2622 Loss: 1.2163448631763458 Val Loss: tensor(62.2276, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2623 Loss: 1.1381679773330688 Val Loss: tensor(62.2468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2624 Loss: 1.1002630293369293 Val Loss: tensor(62.0714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2625 Loss: 1.034483551979065 Val Loss: tensor(62.1875, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2626 Loss: 0.9974224865436554 Val Loss: tensor(61.9553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2627 Loss: 0.95329549908638 Val Loss: tensor(62.1346, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2628 Loss: 0.9247810542583466 Val Loss: tensor(61.8837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2629 Loss: 0.8992070555686951 Val Loss: tensor(62.0933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2630 Loss: 0.8797287344932556 Val Loss: tensor(61.8471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2631 Loss: 0.8658232092857361 Val Loss: tensor(62.0628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2632 Loss: 0.8529115170240402 Val Loss: tensor(61.8325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2633 Loss: 0.8453707098960876 Val Loss: tensor(62.0402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2634 Loss: 0.8364804238080978 Val Loss: tensor(61.8289, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2635 Loss: 0.8325487077236176 Val Loss: tensor(62.0236, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2636 Loss: 0.8260290026664734 Val Loss: tensor(61.8294, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2637 Loss: 0.8246183693408966 Val Loss: tensor(62.0125, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2638 Loss: 0.8195787370204926 Val Loss: tensor(61.8308, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2639 Loss: 0.8203640282154083 Val Loss: tensor(62.0068, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2640 Loss: 0.8164071440696716 Val Loss: tensor(61.8320, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2641 Loss: 0.8193559050559998 Val Loss: tensor(62.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2642 Loss: 0.8163104951381683 Val Loss: tensor(61.8328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2643 Loss: 0.8214791566133499 Val Loss: tensor(62.0097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2644 Loss: 0.8193786144256592 Val Loss: tensor(61.8328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2645 Loss: 0.8269561231136322 Val Loss: tensor(62.0167, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2646 Loss: 0.8260018527507782 Val Loss: tensor(61.8327, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2647 Loss: 0.8363546133041382 Val Loss: tensor(62.0272, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2648 Loss: 0.8369540870189667 Val Loss: tensor(61.8331, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2649 Loss: 0.8506608605384827 Val Loss: tensor(62.0420, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2650 Loss: 0.8534373193979263 Val Loss: tensor(61.8347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2651 Loss: 0.8713296055793762 Val Loss: tensor(62.0619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2652 Loss: 0.8771455734968185 Val Loss: tensor(61.8388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2653 Loss: 0.9002010524272919 Val Loss: tensor(62.0881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2654 Loss: 0.9101012349128723 Val Loss: tensor(61.8464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2655 Loss: 0.939201295375824 Val Loss: tensor(62.1219, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2656 Loss: 0.9542063474655151 Val Loss: tensor(61.8590, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2657 Loss: 0.989663690328598 Val Loss: tensor(62.1639, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2658 Loss: 1.0101943910121918 Val Loss: tensor(61.8768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2659 Loss: 1.0510129034519196 Val Loss: tensor(62.2129, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2660 Loss: 1.0759906470775604 Val Loss: tensor(61.8985, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2661 Loss: 1.118885487318039 Val Loss: tensor(62.2659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2662 Loss: 1.144795000553131 Val Loss: tensor(61.9212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2663 Loss: 1.1839150786399841 Val Loss: tensor(62.3176, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2664 Loss: 1.2049623131752014 Val Loss: tensor(61.9401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2665 Loss: 1.2334218323230743 Val Loss: tensor(62.3620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2666 Loss: 1.244037687778473 Val Loss: tensor(61.9511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2667 Loss: 1.2577045857906342 Val Loss: tensor(62.3956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2668 Loss: 1.2563820779323578 Val Loss: tensor(61.9517, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2669 Loss: 1.2571634650230408 Val Loss: tensor(62.4169, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2670 Loss: 1.247445434331894 Val Loss: tensor(61.9401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2671 Loss: 1.2416173219680786 Val Loss: tensor(62.4249, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2672 Loss: 1.2285139560699463 Val Loss: tensor(61.9154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2673 Loss: 1.221573829650879 Val Loss: tensor(62.4184, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2674 Loss: 1.207450121641159 Val Loss: tensor(61.8785, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2675 Loss: 1.2007805407047272 Val Loss: tensor(62.3967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2676 Loss: 1.1850960850715637 Val Loss: tensor(61.8341, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2677 Loss: 1.1770696938037872 Val Loss: tensor(62.3602, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2678 Loss: 1.1581852436065674 Val Loss: tensor(61.7881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2679 Loss: 1.1468467712402344 Val Loss: tensor(62.3100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2680 Loss: 1.1236777007579803 Val Loss: tensor(61.7446, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2681 Loss: 1.1085934042930603 Val Loss: tensor(62.2485, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2682 Loss: 1.0815395712852478 Val Loss: tensor(61.7053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2683 Loss: 1.0639915466308594 Val Loss: tensor(62.1798, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2684 Loss: 1.0349749028682709 Val Loss: tensor(61.6707, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2685 Loss: 1.0171244144439697 Val Loss: tensor(62.1095, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2686 Loss: 0.9886864423751831 Val Loss: tensor(61.6409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2687 Loss: 0.9724936187267303 Val Loss: tensor(62.0426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2688 Loss: 0.9467531740665436 Val Loss: tensor(61.6160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2689 Loss: 0.9334078729152679 Val Loss: tensor(61.9828, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2690 Loss: 0.9115341603755951 Val Loss: tensor(61.5954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2691 Loss: 0.9014608860015869 Val Loss: tensor(61.9321, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2692 Loss: 0.8836893737316132 Val Loss: tensor(61.5780, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2693 Loss: 0.8767533302307129 Val Loss: tensor(61.8907, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2694 Loss: 0.862656831741333 Val Loss: tensor(61.5629, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2695 Loss: 0.8584805130958557 Val Loss: tensor(61.8577, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2696 Loss: 0.847389817237854 Val Loss: tensor(61.5491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2697 Loss: 0.8455248177051544 Val Loss: tensor(61.8319, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2698 Loss: 0.836748331785202 Val Loss: tensor(61.5360, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2699 Loss: 0.8368055820465088 Val Loss: tensor(61.8119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2700 Loss: 0.8297409415245056 Val Loss: tensor(61.5232, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2701 Loss: 0.831457793712616 Val Loss: tensor(61.7967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2702 Loss: 0.8256456255912781 Val Loss: tensor(61.5105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2703 Loss: 0.8288511335849762 Val Loss: tensor(61.7856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2704 Loss: 0.8239725530147552 Val Loss: tensor(61.4980, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2705 Loss: 0.8286080658435822 Val Loss: tensor(61.7778, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2706 Loss: 0.8244430422782898 Val Loss: tensor(61.4856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2707 Loss: 0.830534428358078 Val Loss: tensor(61.7731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2708 Loss: 0.8269323110580444 Val Loss: tensor(61.4734, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2709 Loss: 0.8345524370670319 Val Loss: tensor(61.7712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2710 Loss: 0.8314731121063232 Val Loss: tensor(61.4615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2711 Loss: 0.8407284617424011 Val Loss: tensor(61.7719, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2712 Loss: 0.838152676820755 Val Loss: tensor(61.4499, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2713 Loss: 0.8491944372653961 Val Loss: tensor(61.7751, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2714 Loss: 0.8471494317054749 Val Loss: tensor(61.4393, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2715 Loss: 0.8600809276103973 Val Loss: tensor(61.7805, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2716 Loss: 0.8585826754570007 Val Loss: tensor(61.4296, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2717 Loss: 0.8734553158283234 Val Loss: tensor(61.7880, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2718 Loss: 0.8725027740001678 Val Loss: tensor(61.4216, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2719 Loss: 0.8891735672950745 Val Loss: tensor(61.7968, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2720 Loss: 0.8886508345603943 Val Loss: tensor(61.4154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2721 Loss: 0.9066761434078217 Val Loss: tensor(61.8059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2722 Loss: 0.9062227308750153 Val Loss: tensor(61.4112, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2723 Loss: 0.9247380793094635 Val Loss: tensor(61.8132, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2724 Loss: 0.9236386120319366 Val Loss: tensor(61.4085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2725 Loss: 0.9412875175476074 Val Loss: tensor(61.8163, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2726 Loss: 0.9384059906005859 Val Loss: tensor(61.4063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2727 Loss: 0.9535257816314697 Val Loss: tensor(61.8119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2728 Loss: 0.9474852085113525 Val Loss: tensor(61.4028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2729 Loss: 0.9586065709590912 Val Loss: tensor(61.7973, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2730 Loss: 0.9482522308826447 Val Loss: tensor(61.3962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2731 Loss: 0.9547076523303986 Val Loss: tensor(61.7721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2732 Loss: 0.9398122131824493 Val Loss: tensor(61.3860, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2733 Loss: 0.9421364665031433 Val Loss: tensor(61.7378, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2734 Loss: 0.9236745834350586 Val Loss: tensor(61.3728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2735 Loss: 0.9233475625514984 Val Loss: tensor(61.6984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2736 Loss: 0.9031957685947418 Val Loss: tensor(61.3582, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2737 Loss: 0.9018614590167999 Val Loss: tensor(61.6583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2738 Loss: 0.8820085525512695 Val Loss: tensor(61.3437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2739 Loss: 0.8809628188610077 Val Loss: tensor(61.6207, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2740 Loss: 0.8629136979579926 Val Loss: tensor(61.3302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2741 Loss: 0.8629596531391144 Val Loss: tensor(61.5882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2742 Loss: 0.8476027250289917 Val Loss: tensor(61.3181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2743 Loss: 0.8492293059825897 Val Loss: tensor(61.5618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2744 Loss: 0.8369458615779877 Val Loss: tensor(61.3082, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2745 Loss: 0.8406094312667847 Val Loss: tensor(61.5427, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2746 Loss: 0.8316737562417984 Val Loss: tensor(61.3014, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2747 Loss: 0.8379713296890259 Val Loss: tensor(61.5316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2748 Loss: 0.8327853232622147 Val Loss: tensor(61.2995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2749 Loss: 0.8427377045154572 Val Loss: tensor(61.5301, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2750 Loss: 0.8423052728176117 Val Loss: tensor(61.3052, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2751 Loss: 0.8576317727565765 Val Loss: tensor(61.5409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2752 Loss: 0.8640260398387909 Val Loss: tensor(61.3236, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2753 Loss: 0.8876827657222748 Val Loss: tensor(61.5696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2754 Loss: 0.9048769176006317 Val Loss: tensor(61.3632, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2755 Loss: 0.9416742324829102 Val Loss: tensor(61.6260, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2756 Loss: 0.9763746857643127 Val Loss: tensor(61.4381, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2757 Loss: 1.0333847105503082 Val Loss: tensor(61.7249, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2758 Loss: 1.094810962677002 Val Loss: tensor(61.5668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2759 Loss: 1.1792466044425964 Val Loss: tensor(61.8808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2760 Loss: 1.2734207808971405 Val Loss: tensor(61.7568, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2761 Loss: 1.3818414211273193 Val Loss: tensor(62.0822, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2762 Loss: 1.4931751787662506 Val Loss: tensor(61.9615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2763 Loss: 1.5873206853866577 Val Loss: tensor(62.2389, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2764 Loss: 1.6598298847675323 Val Loss: tensor(62.0462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2765 Loss: 1.6679518520832062 Val Loss: tensor(62.1939, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2766 Loss: 1.646536260843277 Val Loss: tensor(61.9121, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2767 Loss: 1.5522686541080475 Val Loss: tensor(61.9295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2768 Loss: 1.4529909789562225 Val Loss: tensor(61.6603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2769 Loss: 1.3254858553409576 Val Loss: tensor(61.6489, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2770 Loss: 1.2048083245754242 Val Loss: tensor(61.4441, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2771 Loss: 1.0995541512966156 Val Loss: tensor(61.4769, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2772 Loss: 0.9995565712451935 Val Loss: tensor(61.3024, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2773 Loss: 0.9322333633899689 Val Loss: tensor(61.3915, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2774 Loss: 0.8679057955741882 Val Loss: tensor(61.2254, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2775 Loss: 0.8334772884845734 Val Loss: tensor(61.3438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2776 Loss: 0.8002455681562424 Val Loss: tensor(61.1926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2777 Loss: 0.7853356003761292 Val Loss: tensor(61.3044, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2778 Loss: 0.7715808749198914 Val Loss: tensor(61.1773, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2779 Loss: 0.7647013664245605 Val Loss: tensor(61.2692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2780 Loss: 0.7605449855327606 Val Loss: tensor(61.1632, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2781 Loss: 0.755706712603569 Val Loss: tensor(61.2395, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2782 Loss: 0.7553901076316833 Val Loss: tensor(61.1479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2783 Loss: 0.750767171382904 Val Loss: tensor(61.2177, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2784 Loss: 0.7517018616199493 Val Loss: tensor(61.1339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2785 Loss: 0.7471988946199417 Val Loss: tensor(61.2030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2786 Loss: 0.7484846711158752 Val Loss: tensor(61.1219, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2787 Loss: 0.7443878650665283 Val Loss: tensor(61.1918, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2788 Loss: 0.7457778006792068 Val Loss: tensor(61.1111, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2789 Loss: 0.742186114192009 Val Loss: tensor(61.1824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2790 Loss: 0.7436170727014542 Val Loss: tensor(61.0999, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2791 Loss: 0.7405616044998169 Val Loss: tensor(61.1744, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2792 Loss: 0.7419957220554352 Val Loss: tensor(61.0882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2793 Loss: 0.7394382208585739 Val Loss: tensor(61.1682, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2794 Loss: 0.7408678084611893 Val Loss: tensor(61.0764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2795 Loss: 0.7387858182191849 Val Loss: tensor(61.1642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2796 Loss: 0.7401806712150574 Val Loss: tensor(61.0652, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2797 Loss: 0.73859803378582 Val Loss: tensor(61.1618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2798 Loss: 0.7399691492319107 Val Loss: tensor(61.0542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2799 Loss: 0.738935798406601 Val Loss: tensor(61.1604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2800 Loss: 0.7403154224157333 Val Loss: tensor(61.0427, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2801 Loss: 0.7399233281612396 Val Loss: tensor(61.1594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2802 Loss: 0.741364985704422 Val Loss: tensor(61.0308, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2803 Loss: 0.7418051809072495 Val Loss: tensor(61.1593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2804 Loss: 0.74341881275177 Val Loss: tensor(61.0188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2805 Loss: 0.7449896335601807 Val Loss: tensor(61.1608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2806 Loss: 0.7469704449176788 Val Loss: tensor(61.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2807 Loss: 0.750149130821228 Val Loss: tensor(61.1647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2808 Loss: 0.7528792321681976 Val Loss: tensor(60.9980, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2809 Loss: 0.7584561407566071 Val Loss: tensor(61.1721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2810 Loss: 0.7625720351934433 Val Loss: tensor(60.9920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2811 Loss: 0.7718376815319061 Val Loss: tensor(61.1852, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2812 Loss: 0.7784532904624939 Val Loss: tensor(60.9924, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2813 Loss: 0.7934741079807281 Val Loss: tensor(61.2071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2814 Loss: 0.8044470995664597 Val Loss: tensor(61.0039, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2815 Loss: 0.8284915685653687 Val Loss: tensor(61.2431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2816 Loss: 0.8467761129140854 Val Loss: tensor(61.0336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2817 Loss: 0.8846334517002106 Val Loss: tensor(61.3005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2818 Loss: 0.9142147302627563 Val Loss: tensor(61.0905, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2819 Loss: 0.9718817174434662 Val Loss: tensor(61.3876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2820 Loss: 1.0164088010787964 Val Loss: tensor(61.1816, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2821 Loss: 1.0981853306293488 Val Loss: tensor(61.5072, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2822 Loss: 1.1562389135360718 Val Loss: tensor(61.3003, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2823 Loss: 1.2567203342914581 Val Loss: tensor(61.6401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2824 Loss: 1.3143713474273682 Val Loss: tensor(61.4078, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2825 Loss: 1.408930629491806 Val Loss: tensor(61.7316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2826 Loss: 1.4401379525661469 Val Loss: tensor(61.4419, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2827 Loss: 1.4938183426856995 Val Loss: tensor(61.7225, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2828 Loss: 1.480581670999527 Val Loss: tensor(61.3793, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2829 Loss: 1.481072872877121 Val Loss: tensor(61.6266, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2830 Loss: 1.4286692142486572 Val Loss: tensor(61.2719, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2831 Loss: 1.3902751803398132 Val Loss: tensor(61.5211, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2832 Loss: 1.314785212278366 Val Loss: tensor(61.1772, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2833 Loss: 1.2501487731933594 Val Loss: tensor(61.4367, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2834 Loss: 1.164537400007248 Val Loss: tensor(61.0933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2835 Loss: 1.0864676237106323 Val Loss: tensor(61.3441, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2836 Loss: 1.0038618445396423 Val Loss: tensor(60.9993, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2837 Loss: 0.9375295639038086 Val Loss: tensor(61.2367, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2838 Loss: 0.8758170306682587 Val Loss: tensor(60.9114, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2839 Loss: 0.8377260267734528 Val Loss: tensor(61.1452, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2840 Loss: 0.8047867715358734 Val Loss: tensor(60.8508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2841 Loss: 0.7891710251569748 Val Loss: tensor(61.0837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2842 Loss: 0.7775039523839951 Val Loss: tensor(60.8125, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2843 Loss: 0.7719254195690155 Val Loss: tensor(61.0448, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2844 Loss: 0.7707395255565643 Val Loss: tensor(60.7876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2845 Loss: 0.7677701115608215 Val Loss: tensor(61.0214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2846 Loss: 0.7705294489860535 Val Loss: tensor(60.7689, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2847 Loss: 0.7679348587989807 Val Loss: tensor(61.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2848 Loss: 0.7720298916101456 Val Loss: tensor(60.7503, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2849 Loss: 0.7698187828063965 Val Loss: tensor(61.0026, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2850 Loss: 0.774594634771347 Val Loss: tensor(60.7295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2851 Loss: 0.7731542736291885 Val Loss: tensor(61.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2852 Loss: 0.7785343825817108 Val Loss: tensor(60.7090, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2853 Loss: 0.7782282531261444 Val Loss: tensor(61.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2854 Loss: 0.7841343283653259 Val Loss: tensor(60.6922, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2855 Loss: 0.7854048162698746 Val Loss: tensor(61.0106, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2856 Loss: 0.7917904555797577 Val Loss: tensor(60.6802, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2857 Loss: 0.7951279729604721 Val Loss: tensor(61.0251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2858 Loss: 0.802094042301178 Val Loss: tensor(60.6720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2859 Loss: 0.8079433292150497 Val Loss: tensor(61.0454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2860 Loss: 0.8157391995191574 Val Loss: tensor(60.6664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2861 Loss: 0.8245652318000793 Val Loss: tensor(61.0708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2862 Loss: 0.8334183692932129 Val Loss: tensor(60.6628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2863 Loss: 0.8457735478878021 Val Loss: tensor(61.1011, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2864 Loss: 0.855804055929184 Val Loss: tensor(60.6613, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2865 Loss: 0.8722720742225647 Val Loss: tensor(61.1368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2866 Loss: 0.8834137916564941 Val Loss: tensor(60.6624, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2867 Loss: 0.9044375419616699 Val Loss: tensor(61.1777, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2868 Loss: 0.9163168370723724 Val Loss: tensor(60.6660, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2869 Loss: 0.9419427812099457 Val Loss: tensor(61.2225, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2870 Loss: 0.9537452161312103 Val Loss: tensor(60.6709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2871 Loss: 0.9832724630832672 Val Loss: tensor(61.2678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2872 Loss: 0.9934998750686646 Val Loss: tensor(60.6757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2873 Loss: 1.0251193642616272 Val Loss: tensor(61.3081, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2874 Loss: 1.0315763652324677 Val Loss: tensor(60.6786, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2875 Loss: 1.0621688067913055 Val Loss: tensor(61.3366, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2876 Loss: 1.0622055232524872 Val Loss: tensor(60.6781, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2877 Loss: 1.08748197555542 Val Loss: tensor(61.3458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2878 Loss: 1.078555852174759 Val Loss: tensor(60.6737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2879 Loss: 1.0938144624233246 Val Loss: tensor(61.3300, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2880 Loss: 1.074452668428421 Val Loss: tensor(60.6653, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2881 Loss: 1.0763976871967316 Val Loss: tensor(61.2872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2882 Loss: 1.0472490191459656 Val Loss: tensor(60.6524, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2883 Loss: 1.0361989736557007 Val Loss: tensor(61.2215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2884 Loss: 1.0009735524654388 Val Loss: tensor(60.6350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2885 Loss: 0.9821860790252686 Val Loss: tensor(61.1442, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2886 Loss: 0.9474027454853058 Val Loss: tensor(60.6165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2887 Loss: 0.9289147257804871 Val Loss: tensor(61.0700, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2888 Loss: 0.9015088677406311 Val Loss: tensor(60.6033, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2889 Loss: 0.8900623023509979 Val Loss: tensor(61.0109, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2890 Loss: 0.8745412528514862 Val Loss: tensor(60.6024, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2891 Loss: 0.8734038174152374 Val Loss: tensor(60.9734, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2892 Loss: 0.8712810575962067 Val Loss: tensor(60.6183, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2893 Loss: 0.8809818625450134 Val Loss: tensor(60.9587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2894 Loss: 0.8920579254627228 Val Loss: tensor(60.6533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2895 Loss: 0.9115779101848602 Val Loss: tensor(60.9653, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2896 Loss: 0.9348668158054352 Val Loss: tensor(60.7064, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2897 Loss: 0.961391270160675 Val Loss: tensor(60.9866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2898 Loss: 0.9939407110214233 Val Loss: tensor(60.7705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2899 Loss: 1.0215705633163452 Val Loss: tensor(61.0075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2900 Loss: 1.0563050508499146 Val Loss: tensor(60.8284, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2901 Loss: 1.0764056742191315 Val Loss: tensor(61.0063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2902 Loss: 1.1029153764247894 Val Loss: tensor(60.8584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2903 Loss: 1.1082531213760376 Val Loss: tensor(60.9674, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2904 Loss: 1.118636131286621 Val Loss: tensor(60.8501, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2905 Loss: 1.107399582862854 Val Loss: tensor(60.8949, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2906 Loss: 1.101133644580841 Val Loss: tensor(60.8149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2907 Loss: 1.0760542154312134 Val Loss: tensor(60.8069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2908 Loss: 1.0575222969055176 Val Loss: tensor(60.7711, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2909 Loss: 1.0230713486671448 Val Loss: tensor(60.7209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2910 Loss: 0.9978368580341339 Val Loss: tensor(60.7287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2911 Loss: 0.9596720337867737 Val Loss: tensor(60.6473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2912 Loss: 0.9335471093654633 Val Loss: tensor(60.6895, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2913 Loss: 0.8977919220924377 Val Loss: tensor(60.5906, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2914 Loss: 0.8755923807621002 Val Loss: tensor(60.6538, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2915 Loss: 0.8466727435588837 Val Loss: tensor(60.5500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2916 Loss: 0.8305034041404724 Val Loss: tensor(60.6226, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2917 Loss: 0.8097746819257736 Val Loss: tensor(60.5214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2918 Loss: 0.7991251349449158 Val Loss: tensor(60.5970, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2919 Loss: 0.7855624258518219 Val Loss: tensor(60.5002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2920 Loss: 0.7788027822971344 Val Loss: tensor(60.5774, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2921 Loss: 0.7704435586929321 Val Loss: tensor(60.4836, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2922 Loss: 0.7660952508449554 Val Loss: tensor(60.5628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2923 Loss: 0.7611247152090073 Val Loss: tensor(60.4699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2924 Loss: 0.7582591474056244 Val Loss: tensor(60.5522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2925 Loss: 0.7553740739822388 Val Loss: tensor(60.4577, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2926 Loss: 0.7535548657178879 Val Loss: tensor(60.5443, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2927 Loss: 0.7518985718488693 Val Loss: tensor(60.4459, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2928 Loss: 0.7509486079216003 Val Loss: tensor(60.5382, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2929 Loss: 0.7499864548444748 Val Loss: tensor(60.4337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2930 Loss: 0.7498228996992111 Val Loss: tensor(60.5336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2931 Loss: 0.7492459267377853 Val Loss: tensor(60.4212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2932 Loss: 0.7498783320188522 Val Loss: tensor(60.5303, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2933 Loss: 0.749467208981514 Val Loss: tensor(60.4084, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2934 Loss: 0.7509462833404541 Val Loss: tensor(60.5281, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2935 Loss: 0.750584289431572 Val Loss: tensor(60.3955, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2936 Loss: 0.7530128210783005 Val Loss: tensor(60.5273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2937 Loss: 0.7526682317256927 Val Loss: tensor(60.3824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2938 Loss: 0.7561753541231155 Val Loss: tensor(60.5278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2939 Loss: 0.7558747828006744 Val Loss: tensor(60.3690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2940 Loss: 0.7606824487447739 Val Loss: tensor(60.5297, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2941 Loss: 0.7605380713939667 Val Loss: tensor(60.3556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2942 Loss: 0.7669467031955719 Val Loss: tensor(60.5337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2943 Loss: 0.7672004997730255 Val Loss: tensor(60.3425, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2944 Loss: 0.7756179124116898 Val Loss: tensor(60.5404, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2945 Loss: 0.7766675502061844 Val Loss: tensor(60.3308, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2946 Loss: 0.7876098155975342 Val Loss: tensor(60.5509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2947 Loss: 0.790053591132164 Val Loss: tensor(60.3217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2948 Loss: 0.8041044473648071 Val Loss: tensor(60.5667, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2949 Loss: 0.8087586164474487 Val Loss: tensor(60.3175, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2950 Loss: 0.826359748840332 Val Loss: tensor(60.5889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2951 Loss: 0.8340958654880524 Val Loss: tensor(60.3207, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2952 Loss: 0.8551172614097595 Val Loss: tensor(60.6177, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2953 Loss: 0.8663181662559509 Val Loss: tensor(60.3335, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2954 Loss: 0.889271080493927 Val Loss: tensor(60.6496, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2955 Loss: 0.9027255773544312 Val Loss: tensor(60.3556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2956 Loss: 0.9240801930427551 Val Loss: tensor(60.6757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2957 Loss: 0.9357800185680389 Val Loss: tensor(60.3814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2958 Loss: 0.9509636461734772 Val Loss: tensor(60.6827, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2959 Loss: 0.9547630846500397 Val Loss: tensor(60.4012, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2960 Loss: 0.962125688791275 Val Loss: tensor(60.6622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2961 Loss: 0.9543579816818237 Val Loss: tensor(60.4092, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2962 Loss: 0.9583548307418823 Val Loss: tensor(60.6209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2963 Loss: 0.9420225322246552 Val Loss: tensor(60.4114, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2964 Loss: 0.9497871994972229 Val Loss: tensor(60.5764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2965 Loss: 0.9324281215667725 Val Loss: tensor(60.4189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2966 Loss: 0.9474176168441772 Val Loss: tensor(60.5423, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2967 Loss: 0.9361302256584167 Val Loss: tensor(60.4393, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2968 Loss: 0.9582589268684387 Val Loss: tensor(60.5255, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2969 Loss: 0.9585161507129669 Val Loss: tensor(60.4768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2970 Loss: 0.9888765811920166 Val Loss: tensor(60.5325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2971 Loss: 1.005069762468338 Val Loss: tensor(60.5382, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2972 Loss: 1.0474763214588165 Val Loss: tensor(60.5708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2973 Loss: 1.0808192491531372 Val Loss: tensor(60.6277, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2974 Loss: 1.1369693279266357 Val Loss: tensor(60.6377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2975 Loss: 1.180326908826828 Val Loss: tensor(60.7330, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2976 Loss: 1.2417742609977722 Val Loss: tensor(60.7061, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2977 Loss: 1.2752304673194885 Val Loss: tensor(60.8137, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2978 Loss: 1.3194145560264587 Val Loss: tensor(60.7258, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2979 Loss: 1.3170101940631866 Val Loss: tensor(60.8163, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2980 Loss: 1.3174406588077545 Val Loss: tensor(60.6608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2981 Loss: 1.2690479457378387 Val Loss: tensor(60.7238, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2982 Loss: 1.218481034040451 Val Loss: tensor(60.5282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2983 Loss: 1.1429645121097565 Val Loss: tensor(60.5805, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2984 Loss: 1.0634705722332 Val Loss: tensor(60.3820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2985 Loss: 0.9912291169166565 Val Loss: tensor(60.4476, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2986 Loss: 0.9152456521987915 Val Loss: tensor(60.2636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2987 Loss: 0.8645373284816742 Val Loss: tensor(60.3522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2988 Loss: 0.8108506500720978 Val Loss: tensor(60.1837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2989 Loss: 0.7829594314098358 Val Loss: tensor(60.2902, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2990 Loss: 0.7524091601371765 Val Loss: tensor(60.1353, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2991 Loss: 0.7397363930940628 Val Loss: tensor(60.2489, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2992 Loss: 0.7248067855834961 Val Loss: tensor(60.1066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2993 Loss: 0.7194922715425491 Val Loss: tensor(60.2201, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2994 Loss: 0.7129110991954803 Val Loss: tensor(60.0880, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2995 Loss: 0.7102845311164856 Val Loss: tensor(60.2002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2996 Loss: 0.7076005339622498 Val Loss: tensor(60.0747, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2997 Loss: 0.7058335244655609 Val Loss: tensor(60.1875, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2998 Loss: 0.704827293753624 Val Loss: tensor(60.0642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2999 Loss: 0.7034721672534943 Val Loss: tensor(60.1803, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3000 Loss: 0.7031737565994263 Val Loss: tensor(60.0552, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3001 Loss: 0.702244907617569 Val Loss: tensor(60.1764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3002 Loss: 0.7022485733032227 Val Loss: tensor(60.0468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3003 Loss: 0.7018174082040787 Val Loss: tensor(60.1742, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3004 Loss: 0.7019460499286652 Val Loss: tensor(60.0387, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3005 Loss: 0.7021179348230362 Val Loss: tensor(60.1730, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3006 Loss: 0.7022817730903625 Val Loss: tensor(60.0307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3007 Loss: 0.7031763345003128 Val Loss: tensor(60.1728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3008 Loss: 0.7033394277095795 Val Loss: tensor(60.0228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3009 Loss: 0.705088347196579 Val Loss: tensor(60.1738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3010 Loss: 0.705293208360672 Val Loss: tensor(60.0154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3011 Loss: 0.7080813199281693 Val Loss: tensor(60.1762, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3012 Loss: 0.7084130495786667 Val Loss: tensor(60.0085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3013 Loss: 0.7124902456998825 Val Loss: tensor(60.1798, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3014 Loss: 0.713112011551857 Val Loss: tensor(60.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3015 Loss: 0.7188443243503571 Val Loss: tensor(60.1853, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3016 Loss: 0.7200246155261993 Val Loss: tensor(59.9970, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3017 Loss: 0.7279148697853088 Val Loss: tensor(60.1931, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3018 Loss: 0.7300810068845749 Val Loss: tensor(59.9940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3019 Loss: 0.7408102750778198 Val Loss: tensor(60.2043, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3020 Loss: 0.7446254789829254 Val Loss: tensor(59.9944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3021 Loss: 0.7591069042682648 Val Loss: tensor(60.2206, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3022 Loss: 0.7655346095561981 Val Loss: tensor(59.9997, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3023 Loss: 0.7849609851837158 Val Loss: tensor(60.2437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3024 Loss: 0.7953368723392487 Val Loss: tensor(60.0119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3025 Loss: 0.821090430021286 Val Loss: tensor(60.2757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3026 Loss: 0.8371191173791885 Val Loss: tensor(60.0334, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3027 Loss: 0.87051722407341 Val Loss: tensor(60.3189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3028 Loss: 0.8940165638923645 Val Loss: tensor(60.0666, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3029 Loss: 0.9356195032596588 Val Loss: tensor(60.3742, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3030 Loss: 0.9677259027957916 Val Loss: tensor(60.1124, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3031 Loss: 1.0160300135612488 Val Loss: tensor(60.4394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3032 Loss: 1.055520236492157 Val Loss: tensor(60.1682, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3033 Loss: 1.105074018239975 Val Loss: tensor(60.5067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3034 Loss: 1.1460947096347809 Val Loss: tensor(60.2260, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3035 Loss: 1.1863184869289398 Val Loss: tensor(60.5623, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3036 Loss: 1.2173168659210205 Val Loss: tensor(60.2723, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3037 Loss: 1.234947293996811 Val Loss: tensor(60.5901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3038 Loss: 1.2429850101470947 Val Loss: tensor(60.2926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3039 Loss: 1.2303659617900848 Val Loss: tensor(60.5806, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3040 Loss: 1.2108013033866882 Val Loss: tensor(60.2775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3041 Loss: 1.1741873621940613 Val Loss: tensor(60.5368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3042 Loss: 1.1358766555786133 Val Loss: tensor(60.2267, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3043 Loss: 1.091909110546112 Val Loss: tensor(60.4722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3044 Loss: 1.0497148633003235 Val Loss: tensor(60.1508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3045 Loss: 1.0123960375785828 Val Loss: tensor(60.4046, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3046 Loss: 0.9767318367958069 Val Loss: tensor(60.0675, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3047 Loss: 0.950859934091568 Val Loss: tensor(60.3493, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3048 Loss: 0.9251920580863953 Val Loss: tensor(59.9939, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3049 Loss: 0.9092128276824951 Val Loss: tensor(60.3131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3050 Loss: 0.892500638961792 Val Loss: tensor(59.9377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3051 Loss: 0.8831011056900024 Val Loss: tensor(60.2933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3052 Loss: 0.8726241588592529 Val Loss: tensor(59.8970, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3053 Loss: 0.8672321140766144 Val Loss: tensor(60.2831, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3054 Loss: 0.8604524284601212 Val Loss: tensor(59.8652, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3055 Loss: 0.8576180636882782 Val Loss: tensor(60.2767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3056 Loss: 0.8528677523136139 Val Loss: tensor(59.8379, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3057 Loss: 0.8518754243850708 Val Loss: tensor(60.2717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3058 Loss: 0.8482058793306351 Val Loss: tensor(59.8131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3059 Loss: 0.8487538248300552 Val Loss: tensor(60.2672, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3060 Loss: 0.8455666154623032 Val Loss: tensor(59.7908, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3061 Loss: 0.8475453555583954 Val Loss: tensor(60.2630, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3062 Loss: 0.8444568812847137 Val Loss: tensor(59.7709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3063 Loss: 0.8477665185928345 Val Loss: tensor(60.2590, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3064 Loss: 0.8446000516414642 Val Loss: tensor(59.7530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3065 Loss: 0.8490435630083084 Val Loss: tensor(60.2552, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3066 Loss: 0.8457494825124741 Val Loss: tensor(59.7368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3067 Loss: 0.8510212749242783 Val Loss: tensor(60.2510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3068 Loss: 0.8476148545742035 Val Loss: tensor(59.7221, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3069 Loss: 0.8533406555652618 Val Loss: tensor(60.2458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3070 Loss: 0.8498457670211792 Val Loss: tensor(59.7089, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3071 Loss: 0.855669692158699 Val Loss: tensor(60.2388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3072 Loss: 0.8520753234624863 Val Loss: tensor(59.6972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3073 Loss: 0.8576859086751938 Val Loss: tensor(60.2289, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3074 Loss: 0.8539585620164871 Val Loss: tensor(59.6872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3075 Loss: 0.8590440601110458 Val Loss: tensor(60.2157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3076 Loss: 0.8551853597164154 Val Loss: tensor(59.6792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3077 Loss: 0.859489917755127 Val Loss: tensor(60.1984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3078 Loss: 0.8555550873279572 Val Loss: tensor(59.6735, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3079 Loss: 0.8589227497577667 Val Loss: tensor(60.1770, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3080 Loss: 0.8550127744674683 Val Loss: tensor(59.6703, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3081 Loss: 0.8574334383010864 Val Loss: tensor(60.1513, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3082 Loss: 0.8537618219852448 Val Loss: tensor(59.6698, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3083 Loss: 0.8554047495126724 Val Loss: tensor(60.1222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3084 Loss: 0.8522430211305618 Val Loss: tensor(59.6725, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3085 Loss: 0.8534284681081772 Val Loss: tensor(60.0905, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3086 Loss: 0.8511382043361664 Val Loss: tensor(59.6786, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3087 Loss: 0.8523541539907455 Val Loss: tensor(60.0575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3088 Loss: 0.8513406217098236 Val Loss: tensor(59.6887, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3089 Loss: 0.8530962765216827 Val Loss: tensor(60.0246, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3090 Loss: 0.8537900447845459 Val Loss: tensor(59.7032, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3091 Loss: 0.8565531969070435 Val Loss: tensor(59.9929, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3092 Loss: 0.8593240678310394 Val Loss: tensor(59.7223, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3093 Loss: 0.8634098470211029 Val Loss: tensor(59.9633, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3094 Loss: 0.8684704005718231 Val Loss: tensor(59.7458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3095 Loss: 0.873948335647583 Val Loss: tensor(59.9361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3096 Loss: 0.8812884092330933 Val Loss: tensor(59.7731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3097 Loss: 0.8878598213195801 Val Loss: tensor(59.9110, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3098 Loss: 0.8971039950847626 Val Loss: tensor(59.8025, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3099 Loss: 0.9041105508804321 Val Loss: tensor(59.8870, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3100 Loss: 0.9144963026046753 Val Loss: tensor(59.8322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3101 Loss: 0.9208316206932068 Val Loss: tensor(59.8629, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3102 Loss: 0.9312889277935028 Val Loss: tensor(59.8596, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3103 Loss: 0.9355923235416412 Val Loss: tensor(59.8375, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3104 Loss: 0.9448675513267517 Val Loss: tensor(59.8825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3105 Loss: 0.9457425475120544 Val Loss: tensor(59.8103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3106 Loss: 0.9526481032371521 Val Loss: tensor(59.8992, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3107 Loss: 0.9490169286727905 Val Loss: tensor(59.7812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3108 Loss: 0.952721506357193 Val Loss: tensor(59.9085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3109 Loss: 0.9441789984703064 Val Loss: tensor(59.7511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3110 Loss: 0.944455087184906 Val Loss: tensor(59.9105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3111 Loss: 0.9314085841178894 Val Loss: tensor(59.7212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3112 Loss: 0.9286206364631653 Val Loss: tensor(59.9054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3113 Loss: 0.9122110605239868 Val Loss: tensor(59.6926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3114 Loss: 0.9072502851486206 Val Loss: tensor(59.8945, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3115 Loss: 0.8890421688556671 Val Loss: tensor(59.6660, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3116 Loss: 0.8830606043338776 Val Loss: tensor(59.8791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3117 Loss: 0.8645976483821869 Val Loss: tensor(59.6417, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3118 Loss: 0.8587279319763184 Val Loss: tensor(59.8609, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3119 Loss: 0.8412617743015289 Val Loss: tensor(59.6199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3120 Loss: 0.836384117603302 Val Loss: tensor(59.8417, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3121 Loss: 0.820673793554306 Val Loss: tensor(59.6003, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3122 Loss: 0.81732177734375 Val Loss: tensor(59.8231, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3123 Loss: 0.8037086427211761 Val Loss: tensor(59.5831, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3124 Loss: 0.8020681142807007 Val Loss: tensor(59.8064, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3125 Loss: 0.790569007396698 Val Loss: tensor(59.5678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3126 Loss: 0.7906284034252167 Val Loss: tensor(59.7923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3127 Loss: 0.7810685336589813 Val Loss: tensor(59.5545, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3128 Loss: 0.7826938927173615 Val Loss: tensor(59.7811, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3129 Loss: 0.7748172283172607 Val Loss: tensor(59.5428, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3130 Loss: 0.7778507173061371 Val Loss: tensor(59.7728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3131 Loss: 0.7714036256074905 Val Loss: tensor(59.5325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3132 Loss: 0.7757176607847214 Val Loss: tensor(59.7674, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3133 Loss: 0.7704867422580719 Val Loss: tensor(59.5237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3134 Loss: 0.7760061919689178 Val Loss: tensor(59.7647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3135 Loss: 0.7718272507190704 Val Loss: tensor(59.5162, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3136 Loss: 0.7784911841154099 Val Loss: tensor(59.7644, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3137 Loss: 0.7752307504415512 Val Loss: tensor(59.5102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3138 Loss: 0.7830266207456589 Val Loss: tensor(59.7666, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3139 Loss: 0.7806092649698257 Val Loss: tensor(59.5057, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3140 Loss: 0.7895112484693527 Val Loss: tensor(59.7709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3141 Loss: 0.7878942936658859 Val Loss: tensor(59.5029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3142 Loss: 0.7978650629520416 Val Loss: tensor(59.7772, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3143 Loss: 0.7970147877931595 Val Loss: tensor(59.5016, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3144 Loss: 0.807954728603363 Val Loss: tensor(59.7850, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3145 Loss: 0.8078382015228271 Val Loss: tensor(59.5021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3146 Loss: 0.8196646273136139 Val Loss: tensor(59.7940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3147 Loss: 0.820211261510849 Val Loss: tensor(59.5042, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3148 Loss: 0.8326850235462189 Val Loss: tensor(59.8039, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3149 Loss: 0.8337961435317993 Val Loss: tensor(59.5080, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3150 Loss: 0.8466503322124481 Val Loss: tensor(59.8139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3151 Loss: 0.8481495380401611 Val Loss: tensor(59.5131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3152 Loss: 0.8609907329082489 Val Loss: tensor(59.8234, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3153 Loss: 0.8626434504985809 Val Loss: tensor(59.5194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3154 Loss: 0.8750075697898865 Val Loss: tensor(59.8316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3155 Loss: 0.8765270709991455 Val Loss: tensor(59.5262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3156 Loss: 0.8879463076591492 Val Loss: tensor(59.8377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3157 Loss: 0.8890136778354645 Val Loss: tensor(59.5331, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3158 Loss: 0.8990212976932526 Val Loss: tensor(59.8410, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3159 Loss: 0.8993315994739532 Val Loss: tensor(59.5395, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3160 Loss: 0.9075875580310822 Val Loss: tensor(59.8411, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3161 Loss: 0.9069094955921173 Val Loss: tensor(59.5446, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3162 Loss: 0.9132210612297058 Val Loss: tensor(59.8377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3163 Loss: 0.911434143781662 Val Loss: tensor(59.5480, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3164 Loss: 0.9157693386077881 Val Loss: tensor(59.8310, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3165 Loss: 0.9128295481204987 Val Loss: tensor(59.5494, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3166 Loss: 0.9153354167938232 Val Loss: tensor(59.8211, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3167 Loss: 0.911287397146225 Val Loss: tensor(59.5483, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3168 Loss: 0.9122094213962555 Val Loss: tensor(59.8086, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3169 Loss: 0.9071289896965027 Val Loss: tensor(59.5447, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3170 Loss: 0.9067733287811279 Val Loss: tensor(59.7938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3171 Loss: 0.900764137506485 Val Loss: tensor(59.5386, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3172 Loss: 0.8994227945804596 Val Loss: tensor(59.7773, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3173 Loss: 0.8925809264183044 Val Loss: tensor(59.5302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3174 Loss: 0.8905399441719055 Val Loss: tensor(59.7595, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3175 Loss: 0.883015900850296 Val Loss: tensor(59.5197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3176 Loss: 0.8805306851863861 Val Loss: tensor(59.7409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3177 Loss: 0.8724322617053986 Val Loss: tensor(59.5075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3178 Loss: 0.8697564899921417 Val Loss: tensor(59.7219, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3179 Loss: 0.8612827658653259 Val Loss: tensor(59.4939, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3180 Loss: 0.8586390912532806 Val Loss: tensor(59.7031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3181 Loss: 0.8499936163425446 Val Loss: tensor(59.4795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3182 Loss: 0.8475654125213623 Val Loss: tensor(59.6849, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3183 Loss: 0.8389589786529541 Val Loss: tensor(59.4646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3184 Loss: 0.8369423449039459 Val Loss: tensor(59.6677, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3185 Loss: 0.8285540044307709 Val Loss: tensor(59.4497, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3186 Loss: 0.8270825743675232 Val Loss: tensor(59.6516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3187 Loss: 0.8190759122371674 Val Loss: tensor(59.4350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3188 Loss: 0.8182826042175293 Val Loss: tensor(59.6372, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3189 Loss: 0.8107557147741318 Val Loss: tensor(59.4208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3190 Loss: 0.8107150048017502 Val Loss: tensor(59.6243, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3191 Loss: 0.8037226647138596 Val Loss: tensor(59.4070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3192 Loss: 0.8045010566711426 Val Loss: tensor(59.6133, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3193 Loss: 0.7980571091175079 Val Loss: tensor(59.3938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3194 Loss: 0.799669086933136 Val Loss: tensor(59.6040, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3195 Loss: 0.793778270483017 Val Loss: tensor(59.3813, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3196 Loss: 0.7962264567613602 Val Loss: tensor(59.5964, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3197 Loss: 0.7908433079719543 Val Loss: tensor(59.3694, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3198 Loss: 0.7941329628229141 Val Loss: tensor(59.5905, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3199 Loss: 0.7892286479473114 Val Loss: tensor(59.3580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3200 Loss: 0.7933445572853088 Val Loss: tensor(59.5863, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3201 Loss: 0.7888716757297516 Val Loss: tensor(59.3471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3202 Loss: 0.7937892228364944 Val Loss: tensor(59.5839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3203 Loss: 0.7897203117609024 Val Loss: tensor(59.3368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3204 Loss: 0.7954317927360535 Val Loss: tensor(59.5830, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3205 Loss: 0.7917527109384537 Val Loss: tensor(59.3271, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3206 Loss: 0.7982049584388733 Val Loss: tensor(59.5840, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3207 Loss: 0.7949168384075165 Val Loss: tensor(59.3180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3208 Loss: 0.8020530045032501 Val Loss: tensor(59.5868, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3209 Loss: 0.7992052733898163 Val Loss: tensor(59.3099, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3210 Loss: 0.8069271147251129 Val Loss: tensor(59.5914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3211 Loss: 0.8045767992734909 Val Loss: tensor(59.3031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3212 Loss: 0.8126813173294067 Val Loss: tensor(59.5976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3213 Loss: 0.810914009809494 Val Loss: tensor(59.2980, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3214 Loss: 0.819047600030899 Val Loss: tensor(59.6052, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3215 Loss: 0.8178476095199585 Val Loss: tensor(59.2950, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3216 Loss: 0.8253819346427917 Val Loss: tensor(59.6129, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3217 Loss: 0.8245488405227661 Val Loss: tensor(59.2937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3218 Loss: 0.8305199146270752 Val Loss: tensor(59.6185, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3219 Loss: 0.8294837474822998 Val Loss: tensor(59.2932, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3220 Loss: 0.8325927555561066 Val Loss: tensor(59.6187, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3221 Loss: 0.830316036939621 Val Loss: tensor(59.2910, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3222 Loss: 0.8293557465076447 Val Loss: tensor(59.6092, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3223 Loss: 0.8245401382446289 Val Loss: tensor(59.2837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3224 Loss: 0.8190684020519257 Val Loss: tensor(59.5875, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3225 Loss: 0.8108707964420319 Val Loss: tensor(59.2690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3226 Loss: 0.8018303513526917 Val Loss: tensor(59.5538, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3227 Loss: 0.7906211614608765 Val Loss: tensor(59.2467, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3228 Loss: 0.7801625430583954 Val Loss: tensor(59.5122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3229 Loss: 0.7676103115081787 Val Loss: tensor(59.2198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3230 Loss: 0.7581394463777542 Val Loss: tensor(59.4684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3231 Loss: 0.7465222328901291 Val Loss: tensor(59.1926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3232 Loss: 0.7397448122501373 Val Loss: tensor(59.4276, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3233 Loss: 0.7310504466295242 Val Loss: tensor(59.1692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3234 Loss: 0.7277448326349258 Val Loss: tensor(59.3939, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3235 Loss: 0.7233448475599289 Val Loss: tensor(59.1521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3236 Loss: 0.7237653285264969 Val Loss: tensor(59.3692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3237 Loss: 0.7246447950601578 Val Loss: tensor(59.1433, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3238 Loss: 0.7290341258049011 Val Loss: tensor(59.3553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3239 Loss: 0.7361870110034943 Val Loss: tensor(59.1443, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3240 Loss: 0.7450166940689087 Val Loss: tensor(59.3530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3241 Loss: 0.7596929222345352 Val Loss: tensor(59.1570, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3242 Loss: 0.7736816555261612 Val Loss: tensor(59.3628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3243 Loss: 0.797332763671875 Val Loss: tensor(59.1828, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3244 Loss: 0.8171233534812927 Val Loss: tensor(59.3843, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3245 Loss: 0.8508022576570511 Val Loss: tensor(59.2220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3246 Loss: 0.8763389587402344 Val Loss: tensor(59.4139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3247 Loss: 0.9196221977472305 Val Loss: tensor(59.2715, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3248 Loss: 0.9491968154907227 Val Loss: tensor(59.4445, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3249 Loss: 0.9984771311283112 Val Loss: tensor(59.3238, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3250 Loss: 1.0275316536426544 Val Loss: tensor(59.4641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3251 Loss: 1.0746788680553436 Val Loss: tensor(59.3676, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3252 Loss: 1.0953024625778198 Val Loss: tensor(59.4596, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3253 Loss: 1.12814199924469 Val Loss: tensor(59.3920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3254 Loss: 1.1304039061069489 Val Loss: tensor(59.4245, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3255 Loss: 1.1369460821151733 Val Loss: tensor(59.3914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3256 Loss: 1.1138668358325958 Val Loss: tensor(59.3636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3257 Loss: 1.0904071629047394 Val Loss: tensor(59.3673, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3258 Loss: 1.0453000962734222 Val Loss: tensor(59.2894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3259 Loss: 1.0025731325149536 Val Loss: tensor(59.3256, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3260 Loss: 0.9501981735229492 Val Loss: tensor(59.2131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3261 Loss: 0.907122790813446 Val Loss: tensor(59.2763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3262 Loss: 0.8633971959352493 Val Loss: tensor(59.1441, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3263 Loss: 0.8330198526382446 Val Loss: tensor(59.2318, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3264 Loss: 0.8053548038005829 Val Loss: tensor(59.0911, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3265 Loss: 0.7900514006614685 Val Loss: tensor(59.2027, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3266 Loss: 0.777723640203476 Val Loss: tensor(59.0588, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3267 Loss: 0.7742047905921936 Val Loss: tensor(59.1932, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3268 Loss: 0.7733045965433121 Val Loss: tensor(59.0471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3269 Loss: 0.7779419869184494 Val Loss: tensor(59.2022, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3270 Loss: 0.7845546454191208 Val Loss: tensor(59.0522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3271 Loss: 0.79519322514534 Val Loss: tensor(59.2259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3272 Loss: 0.8063089400529861 Val Loss: tensor(59.0683, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3273 Loss: 0.8218442499637604 Val Loss: tensor(59.2603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3274 Loss: 0.8351808190345764 Val Loss: tensor(59.0896, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3275 Loss: 0.854628399014473 Val Loss: tensor(59.3013, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3276 Loss: 0.8680638670921326 Val Loss: tensor(59.1112, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3277 Loss: 0.8899809271097183 Val Loss: tensor(59.3439, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3278 Loss: 0.9011176824569702 Val Loss: tensor(59.1287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3279 Loss: 0.9233641177415848 Val Loss: tensor(59.3823, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3280 Loss: 0.9295611381530762 Val Loss: tensor(59.1379, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3281 Loss: 0.9494273066520691 Val Loss: tensor(59.4105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3282 Loss: 0.9484109282493591 Val Loss: tensor(59.1355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3283 Loss: 0.96323561668396 Val Loss: tensor(59.4241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3284 Loss: 0.9537138640880585 Val Loss: tensor(59.1204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3285 Loss: 0.9617758095264435 Val Loss: tensor(59.4213, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3286 Loss: 0.9438989758491516 Val Loss: tensor(59.0944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3287 Loss: 0.944936215877533 Val Loss: tensor(59.4035, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3288 Loss: 0.9203913062810898 Val Loss: tensor(59.0606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3289 Loss: 0.9156770557165146 Val Loss: tensor(59.3744, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3290 Loss: 0.8873265981674194 Val Loss: tensor(59.0228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3291 Loss: 0.8792417943477631 Val Loss: tensor(59.3388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3292 Loss: 0.85047447681427 Val Loss: tensor(58.9847, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3293 Loss: 0.8416551798582077 Val Loss: tensor(59.3021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3294 Loss: 0.8154856860637665 Val Loss: tensor(58.9494, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3295 Loss: 0.8080873787403107 Val Loss: tensor(59.2682, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3296 Loss: 0.7865510582923889 Val Loss: tensor(58.9187, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3297 Loss: 0.7818157225847244 Val Loss: tensor(59.2399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3298 Loss: 0.7657355815172195 Val Loss: tensor(58.8941, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3299 Loss: 0.7640763372182846 Val Loss: tensor(59.2182, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3300 Loss: 0.7533162385225296 Val Loss: tensor(58.8764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3301 Loss: 0.7546966671943665 Val Loss: tensor(59.2034, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3302 Loss: 0.7486591339111328 Val Loss: tensor(58.8659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3303 Loss: 0.7529052495956421 Val Loss: tensor(59.1952, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3304 Loss: 0.7508996725082397 Val Loss: tensor(58.8628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3305 Loss: 0.7578951418399811 Val Loss: tensor(59.1931, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3306 Loss: 0.7594451010227203 Val Loss: tensor(58.8677, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3307 Loss: 0.7691893428564072 Val Loss: tensor(59.1964, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3308 Loss: 0.7741012275218964 Val Loss: tensor(58.8809, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3309 Loss: 0.7865316569805145 Val Loss: tensor(59.2046, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3310 Loss: 0.7947207093238831 Val Loss: tensor(58.9027, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3311 Loss: 0.8096265941858292 Val Loss: tensor(59.2159, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3312 Loss: 0.8208200335502625 Val Loss: tensor(58.9322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3313 Loss: 0.8375579118728638 Val Loss: tensor(59.2273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3314 Loss: 0.8509080708026886 Val Loss: tensor(58.9668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3315 Loss: 0.8683318197727203 Val Loss: tensor(59.2349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3316 Loss: 0.882244348526001 Val Loss: tensor(59.0021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3317 Loss: 0.8988263309001923 Val Loss: tensor(59.2342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3318 Loss: 0.9111905097961426 Val Loss: tensor(59.0331, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3319 Loss: 0.9252802431583405 Val Loss: tensor(59.2224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3320 Loss: 0.934045672416687 Val Loss: tensor(59.0562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3321 Loss: 0.9440070986747742 Val Loss: tensor(59.1991, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3322 Loss: 0.9476067423820496 Val Loss: tensor(59.0699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3323 Loss: 0.9518977105617523 Val Loss: tensor(59.1662, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3324 Loss: 0.9495002031326294 Val Loss: tensor(59.0745, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3325 Loss: 0.946945071220398 Val Loss: tensor(59.1264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3326 Loss: 0.938529908657074 Val Loss: tensor(59.0705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3327 Loss: 0.9289775043725967 Val Loss: tensor(59.0832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3328 Loss: 0.9156439453363419 Val Loss: tensor(59.0589, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3329 Loss: 0.9004562944173813 Val Loss: tensor(59.0400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3330 Loss: 0.8843591809272766 Val Loss: tensor(59.0414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3331 Loss: 0.8661713153123856 Val Loss: tensor(58.9995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3332 Loss: 0.8498933017253876 Val Loss: tensor(59.0205, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3333 Loss: 0.8316541761159897 Val Loss: tensor(58.9639, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3334 Loss: 0.8173085004091263 Val Loss: tensor(58.9987, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3335 Loss: 0.8012198656797409 Val Loss: tensor(58.9344, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3336 Loss: 0.7899213433265686 Val Loss: tensor(58.9790, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3337 Loss: 0.7770640701055527 Val Loss: tensor(58.9112, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3338 Loss: 0.7690063267946243 Val Loss: tensor(58.9627, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3339 Loss: 0.7594611048698425 Val Loss: tensor(58.8941, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3340 Loss: 0.754305362701416 Val Loss: tensor(58.9504, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3341 Loss: 0.7475993633270264 Val Loss: tensor(58.8820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3342 Loss: 0.7448585629463196 Val Loss: tensor(58.9421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3343 Loss: 0.7403744608163834 Val Loss: tensor(58.8740, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3344 Loss: 0.7396142184734344 Val Loss: tensor(58.9374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3345 Loss: 0.7367867231369019 Val Loss: tensor(58.8689, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3346 Loss: 0.7377099692821503 Val Loss: tensor(58.9358, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3347 Loss: 0.736078605055809 Val Loss: tensor(58.8657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3348 Loss: 0.7384899854660034 Val Loss: tensor(58.9370, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3349 Loss: 0.7377539575099945 Val Loss: tensor(58.8637, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3350 Loss: 0.7415682226419449 Val Loss: tensor(58.9406, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3351 Loss: 0.7415045499801636 Val Loss: tensor(58.8625, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3352 Loss: 0.7466469407081604 Val Loss: tensor(58.9464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3353 Loss: 0.7471048980951309 Val Loss: tensor(58.8616, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3354 Loss: 0.7535795420408249 Val Loss: tensor(58.9542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3355 Loss: 0.7544776499271393 Val Loss: tensor(58.8607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3356 Loss: 0.762255534529686 Val Loss: tensor(58.9640, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3357 Loss: 0.7635367810726166 Val Loss: tensor(58.8595, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3358 Loss: 0.7726085335016251 Val Loss: tensor(58.9757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3359 Loss: 0.7742634564638138 Val Loss: tensor(58.8579, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3360 Loss: 0.7846222221851349 Val Loss: tensor(58.9893, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3361 Loss: 0.7866434901952744 Val Loss: tensor(58.8556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3362 Loss: 0.7982068806886673 Val Loss: tensor(59.0049, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3363 Loss: 0.8006241023540497 Val Loss: tensor(58.8529, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3364 Loss: 0.8132555484771729 Val Loss: tensor(59.0226, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3365 Loss: 0.8160571902990341 Val Loss: tensor(58.8497, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3366 Loss: 0.8294666558504105 Val Loss: tensor(59.0423, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3367 Loss: 0.832600012421608 Val Loss: tensor(58.8465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3368 Loss: 0.8462391346693039 Val Loss: tensor(59.0637, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3369 Loss: 0.8494795113801956 Val Loss: tensor(58.8434, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3370 Loss: 0.862498939037323 Val Loss: tensor(59.0857, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3371 Loss: 0.8653417527675629 Val Loss: tensor(58.8410, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3372 Loss: 0.8763728588819504 Val Loss: tensor(59.1059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3373 Loss: 0.8778220415115356 Val Loss: tensor(58.8387, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3374 Loss: 0.8850858956575394 Val Loss: tensor(59.1204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3375 Loss: 0.883631706237793 Val Loss: tensor(58.8350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3376 Loss: 0.885346919298172 Val Loss: tensor(59.1243, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3377 Loss: 0.8792731463909149 Val Loss: tensor(58.8273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3378 Loss: 0.8746687471866608 Val Loss: tensor(59.1135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3379 Loss: 0.8629785776138306 Val Loss: tensor(58.8130, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3380 Loss: 0.8531562983989716 Val Loss: tensor(59.0877, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3381 Loss: 0.8365868330001831 Val Loss: tensor(58.7915, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3382 Loss: 0.8244132995605469 Val Loss: tensor(59.0513, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3383 Loss: 0.8053959906101227 Val Loss: tensor(58.7648, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3384 Loss: 0.7940434515476227 Val Loss: tensor(59.0110, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3385 Loss: 0.7754360884428024 Val Loss: tensor(58.7362, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3386 Loss: 0.7669266015291214 Val Loss: tensor(58.9728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3387 Loss: 0.7507527023553848 Val Loss: tensor(58.7085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3388 Loss: 0.7455656230449677 Val Loss: tensor(58.9400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3389 Loss: 0.7325640320777893 Val Loss: tensor(58.6834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3390 Loss: 0.730294406414032 Val Loss: tensor(58.9142, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3391 Loss: 0.7203101813793182 Val Loss: tensor(58.6618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3392 Loss: 0.7202847748994827 Val Loss: tensor(58.8954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3393 Loss: 0.7128217220306396 Val Loss: tensor(58.6437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3394 Loss: 0.7144932001829147 Val Loss: tensor(58.8829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3395 Loss: 0.7090092748403549 Val Loss: tensor(58.6292, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3396 Loss: 0.7120288163423538 Val Loss: tensor(58.8757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3397 Loss: 0.7081095725297928 Val Loss: tensor(58.6179, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3398 Loss: 0.7123041450977325 Val Loss: tensor(58.8731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3399 Loss: 0.7096497565507889 Val Loss: tensor(58.6097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3400 Loss: 0.715032771229744 Val Loss: tensor(58.8742, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3401 Loss: 0.7134920358657837 Val Loss: tensor(58.6042, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3402 Loss: 0.7201562076807022 Val Loss: tensor(58.8788, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3403 Loss: 0.719718798995018 Val Loss: tensor(58.6016, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3404 Loss: 0.7279129773378372 Val Loss: tensor(58.8867, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3405 Loss: 0.7286509275436401 Val Loss: tensor(58.6018, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3406 Loss: 0.738685205578804 Val Loss: tensor(58.8981, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3407 Loss: 0.7408032864332199 Val Loss: tensor(58.6051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3408 Loss: 0.7530657947063446 Val Loss: tensor(58.9138, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3409 Loss: 0.7568107396364212 Val Loss: tensor(58.6119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3410 Loss: 0.7717500776052475 Val Loss: tensor(58.9344, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3411 Loss: 0.7773858904838562 Val Loss: tensor(58.6226, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3412 Loss: 0.7954050004482269 Val Loss: tensor(58.9606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3413 Loss: 0.8031560033559799 Val Loss: tensor(58.6377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3414 Loss: 0.8244545608758926 Val Loss: tensor(58.9926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3415 Loss: 0.8343300968408585 Val Loss: tensor(58.6570, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3416 Loss: 0.8586643636226654 Val Loss: tensor(59.0297, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3417 Loss: 0.8702407628297806 Val Loss: tensor(58.6798, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3418 Loss: 0.8966284543275833 Val Loss: tensor(59.0695, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3419 Loss: 0.908741295337677 Val Loss: tensor(58.7040, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3420 Loss: 0.9351809322834015 Val Loss: tensor(59.1075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3421 Loss: 0.9456616044044495 Val Loss: tensor(58.7263, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3422 Loss: 0.9690828621387482 Val Loss: tensor(59.1370, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3423 Loss: 0.9747775197029114 Val Loss: tensor(58.7422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3424 Loss: 0.99152672290802 Val Loss: tensor(59.1507, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3425 Loss: 0.9889316260814667 Val Loss: tensor(58.7474, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3426 Loss: 0.9959296584129333 Val Loss: tensor(59.1427, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3427 Loss: 0.9824628829956055 Val Loss: tensor(58.7388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3428 Loss: 0.9787926971912384 Val Loss: tensor(59.1119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3429 Loss: 0.9543865919113159 Val Loss: tensor(58.7164, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3430 Loss: 0.942130446434021 Val Loss: tensor(59.0634, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3431 Loss: 0.9098941087722778 Val Loss: tensor(58.6825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3432 Loss: 0.8936240077018738 Val Loss: tensor(59.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3433 Loss: 0.8588379323482513 Val Loss: tensor(58.6420, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3434 Loss: 0.8436350226402283 Val Loss: tensor(58.9532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3435 Loss: 0.811770886182785 Val Loss: tensor(58.6007, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3436 Loss: 0.8012827187776566 Val Loss: tensor(58.9105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3437 Loss: 0.7762644290924072 Val Loss: tensor(58.5641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3438 Loss: 0.772182896733284 Val Loss: tensor(58.8832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3439 Loss: 0.7559735774993896 Val Loss: tensor(58.5369, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3440 Loss: 0.758525013923645 Val Loss: tensor(58.8734, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3441 Loss: 0.7517857998609543 Val Loss: tensor(58.5224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3442 Loss: 0.7604899257421494 Val Loss: tensor(58.8806, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3443 Loss: 0.7634067684412003 Val Loss: tensor(58.5225, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3444 Loss: 0.7772460877895355 Val Loss: tensor(58.9023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3445 Loss: 0.7896297127008438 Val Loss: tensor(58.5370, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3446 Loss: 0.8064605295658112 Val Loss: tensor(58.9315, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3447 Loss: 0.8268909603357315 Val Loss: tensor(58.5615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3448 Loss: 0.8426167964935303 Val Loss: tensor(58.9550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3449 Loss: 0.86688432097435 Val Loss: tensor(58.5860, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3450 Loss: 0.8756421506404877 Val Loss: tensor(58.9543, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3451 Loss: 0.8965546935796738 Val Loss: tensor(58.5966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3452 Loss: 0.8936314135789871 Val Loss: tensor(58.9151, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3453 Loss: 0.9043929427862167 Val Loss: tensor(58.5842, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3454 Loss: 0.8900880962610245 Val Loss: tensor(58.8391, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3455 Loss: 0.8894967138767242 Val Loss: tensor(58.5549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3456 Loss: 0.8686704039573669 Val Loss: tensor(58.7449, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3457 Loss: 0.8609606176614761 Val Loss: tensor(58.5251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3458 Loss: 0.8385898470878601 Val Loss: tensor(58.6535, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3459 Loss: 0.8285767436027527 Val Loss: tensor(58.5065, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3460 Loss: 0.8071916103363037 Val Loss: tensor(58.5777, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3461 Loss: 0.7973499596118927 Val Loss: tensor(58.4989, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3462 Loss: 0.7778681367635727 Val Loss: tensor(58.5205, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3463 Loss: 0.7692398130893707 Val Loss: tensor(58.4960, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3464 Loss: 0.7521673142910004 Val Loss: tensor(58.4798, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3465 Loss: 0.7453337609767914 Val Loss: tensor(58.4924, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3466 Loss: 0.7309644371271133 Val Loss: tensor(58.4518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3467 Loss: 0.7262217402458191 Val Loss: tensor(58.4862, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3468 Loss: 0.7146778851747513 Val Loss: tensor(58.4323, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3469 Loss: 0.7120413035154343 Val Loss: tensor(58.4790, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3470 Loss: 0.7033337354660034 Val Loss: tensor(58.4184, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3471 Loss: 0.7026269286870956 Val Loss: tensor(58.4731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3472 Loss: 0.6967005133628845 Val Loss: tensor(58.4092, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3473 Loss: 0.6976931989192963 Val Loss: tensor(58.4704, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3474 Loss: 0.6944478899240494 Val Loss: tensor(58.4039, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3475 Loss: 0.6970259249210358 Val Loss: tensor(58.4716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3476 Loss: 0.6964221298694611 Val Loss: tensor(58.4023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3477 Loss: 0.7006655931472778 Val Loss: tensor(58.4775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3478 Loss: 0.7028155326843262 Val Loss: tensor(58.4049, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3479 Loss: 0.7090895622968674 Val Loss: tensor(58.4892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3480 Loss: 0.7143296301364899 Val Loss: tensor(58.4122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3481 Loss: 0.7232388705015182 Val Loss: tensor(58.5084, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3482 Loss: 0.7320915907621384 Val Loss: tensor(58.4254, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3483 Loss: 0.7444479167461395 Val Loss: tensor(58.5372, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3484 Loss: 0.7575304210186005 Val Loss: tensor(58.4458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3485 Loss: 0.7741131335496902 Val Loss: tensor(58.5771, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3486 Loss: 0.7919119745492935 Val Loss: tensor(58.4738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3487 Loss: 0.8131389617919922 Val Loss: tensor(58.6291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3488 Loss: 0.8357146233320236 Val Loss: tensor(58.5084, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3489 Loss: 0.861143171787262 Val Loss: tensor(58.6917, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3490 Loss: 0.8876816630363464 Val Loss: tensor(58.5460, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3491 Loss: 0.915410041809082 Val Loss: tensor(58.7605, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3492 Loss: 0.9438218176364899 Val Loss: tensor(58.5805, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3493 Loss: 0.9702487587928772 Val Loss: tensor(58.8277, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3494 Loss: 0.9970709979534149 Val Loss: tensor(58.6047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3495 Loss: 1.0173951387405396 Val Loss: tensor(58.8834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3496 Loss: 1.0382303595542908 Val Loss: tensor(58.6127, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3497 Loss: 1.0477950274944305 Val Loss: tensor(58.9191, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3498 Loss: 1.0582276284694672 Val Loss: tensor(58.6041, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3499 Loss: 1.0540010631084442 Val Loss: tensor(58.9299, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3500 Loss: 1.0510745346546173 Val Loss: tensor(58.5833, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3501 Loss: 1.0321076214313507 Val Loss: tensor(58.9131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3502 Loss: 1.0151063799858093 Val Loss: tensor(58.5549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3503 Loss: 0.9823107421398163 Val Loss: tensor(58.8651, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3504 Loss: 0.953073650598526 Val Loss: tensor(58.5167, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3505 Loss: 0.910484790802002 Val Loss: tensor(58.7848, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3506 Loss: 0.8739757835865021 Val Loss: tensor(58.4632, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3507 Loss: 0.8307106792926788 Val Loss: tensor(58.6851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3508 Loss: 0.7953638434410095 Val Loss: tensor(58.3997, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3509 Loss: 0.7615968585014343 Val Loss: tensor(58.5892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3510 Loss: 0.7352646738290787 Val Loss: tensor(58.3414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3511 Loss: 0.7149214595556259 Val Loss: tensor(58.5130, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3512 Loss: 0.7001482546329498 Val Loss: tensor(58.3002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3513 Loss: 0.6905389726161957 Val Loss: tensor(58.4572, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3514 Loss: 0.685014471411705 Val Loss: tensor(58.2778, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3515 Loss: 0.6814829409122467 Val Loss: tensor(58.4166, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3516 Loss: 0.6815375834703445 Val Loss: tensor(58.2687, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3517 Loss: 0.6805513352155685 Val Loss: tensor(58.3861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3518 Loss: 0.6833927184343338 Val Loss: tensor(58.2659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3519 Loss: 0.6829090267419815 Val Loss: tensor(58.3621, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3520 Loss: 0.6869821697473526 Val Loss: tensor(58.2646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3521 Loss: 0.6859262883663177 Val Loss: tensor(58.3414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3522 Loss: 0.6904396414756775 Val Loss: tensor(58.2638, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3523 Loss: 0.6883162707090378 Val Loss: tensor(58.3218, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3524 Loss: 0.6929230391979218 Val Loss: tensor(58.2640, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3525 Loss: 0.6895827203989029 Val Loss: tensor(58.3025, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3526 Loss: 0.6941613256931305 Val Loss: tensor(58.2661, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3527 Loss: 0.6896975934505463 Val Loss: tensor(58.2841, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3528 Loss: 0.6942109763622284 Val Loss: tensor(58.2704, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3529 Loss: 0.6888976842164993 Val Loss: tensor(58.2673, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3530 Loss: 0.6933387070894241 Val Loss: tensor(58.2763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3531 Loss: 0.6875177323818207 Val Loss: tensor(58.2526, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3532 Loss: 0.6918758749961853 Val Loss: tensor(58.2832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3533 Loss: 0.6859039515256882 Val Loss: tensor(58.2409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3534 Loss: 0.6901034265756607 Val Loss: tensor(58.2904, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3535 Loss: 0.6843357086181641 Val Loss: tensor(58.2325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3536 Loss: 0.6881750673055649 Val Loss: tensor(58.2965, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3537 Loss: 0.682919830083847 Val Loss: tensor(58.2280, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3538 Loss: 0.6861138790845871 Val Loss: tensor(58.2995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3539 Loss: 0.6816065609455109 Val Loss: tensor(58.2264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3540 Loss: 0.6840176582336426 Val Loss: tensor(58.2976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3541 Loss: 0.6805344372987747 Val Loss: tensor(58.2271, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3542 Loss: 0.6823903471231461 Val Loss: tensor(58.2901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3543 Loss: 0.6805804669857025 Val Loss: tensor(58.2298, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3544 Loss: 0.682680532336235 Val Loss: tensor(58.2779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3545 Loss: 0.6837514340877533 Val Loss: tensor(58.2355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3546 Loss: 0.6874399036169052 Val Loss: tensor(58.2646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3547 Loss: 0.6932136565446854 Val Loss: tensor(58.2473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3548 Loss: 0.7001522034406662 Val Loss: tensor(58.2547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3549 Loss: 0.7127767354249954 Val Loss: tensor(58.2690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3550 Loss: 0.7247852832078934 Val Loss: tensor(58.2527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3551 Loss: 0.7462504804134369 Val Loss: tensor(58.3048, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3552 Loss: 0.7651481032371521 Val Loss: tensor(58.2628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3553 Loss: 0.7968400865793228 Val Loss: tensor(58.3581, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3554 Loss: 0.8243139833211899 Val Loss: tensor(58.2875, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3555 Loss: 0.8662551045417786 Val Loss: tensor(58.4307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3556 Loss: 0.9032861441373825 Val Loss: tensor(58.3269, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3557 Loss: 0.9529321789741516 Val Loss: tensor(58.5210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3558 Loss: 0.9985474646091461 Val Loss: tensor(58.3776, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3559 Loss: 1.0492900907993317 Val Loss: tensor(58.6209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3560 Loss: 1.0984211266040802 Val Loss: tensor(58.4313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3561 Loss: 1.1384893655776978 Val Loss: tensor(58.7136, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3562 Loss: 1.1799391508102417 Val Loss: tensor(58.4752, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3563 Loss: 1.1940824091434479 Val Loss: tensor(58.7735, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3564 Loss: 1.2120091021060944 Val Loss: tensor(58.4938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3565 Loss: 1.189218521118164 Val Loss: tensor(58.7751, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3566 Loss: 1.172706514596939 Val Loss: tensor(58.4725, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3567 Loss: 1.1179895401000977 Val Loss: tensor(58.7118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3568 Loss: 1.073362648487091 Val Loss: tensor(58.4077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3569 Loss: 1.0083579421043396 Val Loss: tensor(58.6085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3570 Loss: 0.9564279019832611 Val Loss: tensor(58.3153, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3571 Loss: 0.9024405777454376 Val Loss: tensor(58.5033, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3572 Loss: 0.8606646955013275 Val Loss: tensor(58.2217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3573 Loss: 0.8250651657581329 Val Loss: tensor(58.4205, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3574 Loss: 0.7985041439533234 Val Loss: tensor(58.1474, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3575 Loss: 0.7777892649173737 Val Loss: tensor(58.3656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3576 Loss: 0.7633893191814423 Val Loss: tensor(58.0997, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3577 Loss: 0.751567617058754 Val Loss: tensor(58.3336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3578 Loss: 0.7443802505731583 Val Loss: tensor(58.0740, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3579 Loss: 0.7372128069400787 Val Loss: tensor(58.3165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3580 Loss: 0.733415961265564 Val Loss: tensor(58.0603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3581 Loss: 0.728536531329155 Val Loss: tensor(58.3067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3582 Loss: 0.7258538156747818 Val Loss: tensor(58.0496, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3583 Loss: 0.7221070677042007 Val Loss: tensor(58.2988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3584 Loss: 0.719438374042511 Val Loss: tensor(58.0388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3585 Loss: 0.7164039015769958 Val Loss: tensor(58.2893, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3586 Loss: 0.713335782289505 Val Loss: tensor(58.0285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3587 Loss: 0.7110871374607086 Val Loss: tensor(58.2779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3588 Loss: 0.7076303213834763 Val Loss: tensor(58.0205, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3589 Loss: 0.7063399702310562 Val Loss: tensor(58.2654, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3590 Loss: 0.7027277499437332 Val Loss: tensor(58.0146, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3591 Loss: 0.7025195807218552 Val Loss: tensor(58.2531, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3592 Loss: 0.6990875452756882 Val Loss: tensor(58.0100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3593 Loss: 0.700016662478447 Val Loss: tensor(58.2419, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3594 Loss: 0.6970511674880981 Val Loss: tensor(58.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3595 Loss: 0.6992125064134598 Val Loss: tensor(58.2323, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3596 Loss: 0.6968729794025421 Val Loss: tensor(58.0030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3597 Loss: 0.7003957033157349 Val Loss: tensor(58.2253, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3598 Loss: 0.6987865716218948 Val Loss: tensor(58.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3599 Loss: 0.7037932723760605 Val Loss: tensor(58.2215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3600 Loss: 0.7029505372047424 Val Loss: tensor(57.9995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3601 Loss: 0.7095173895359039 Val Loss: tensor(58.2211, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3602 Loss: 0.7094771713018417 Val Loss: tensor(57.9995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3603 Loss: 0.7176095396280289 Val Loss: tensor(58.2244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3604 Loss: 0.718328982591629 Val Loss: tensor(58.0006, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3605 Loss: 0.7279670387506485 Val Loss: tensor(58.2313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3606 Loss: 0.7293487340211868 Val Loss: tensor(58.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3607 Loss: 0.7403006255626678 Val Loss: tensor(58.2410, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3608 Loss: 0.7421375662088394 Val Loss: tensor(58.0055, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3609 Loss: 0.7540967613458633 Val Loss: tensor(58.2530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3610 Loss: 0.7560515701770782 Val Loss: tensor(58.0087, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3611 Loss: 0.7685177475214005 Val Loss: tensor(58.2660, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3612 Loss: 0.7701271027326584 Val Loss: tensor(58.0117, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3613 Loss: 0.7824501842260361 Val Loss: tensor(58.2787, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3614 Loss: 0.7831050008535385 Val Loss: tensor(58.0137, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3615 Loss: 0.7945247739553452 Val Loss: tensor(58.2894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3616 Loss: 0.7935727834701538 Val Loss: tensor(58.0139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3617 Loss: 0.8032442778348923 Val Loss: tensor(58.2964, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3618 Loss: 0.8000241369009018 Val Loss: tensor(58.0114, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3619 Loss: 0.807203084230423 Val Loss: tensor(58.2980, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3620 Loss: 0.801232784986496 Val Loss: tensor(58.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3621 Loss: 0.8053854703903198 Val Loss: tensor(58.2932, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3622 Loss: 0.7964425981044769 Val Loss: tensor(57.9967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3623 Loss: 0.7973844856023788 Val Loss: tensor(58.2812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3624 Loss: 0.7856455594301224 Val Loss: tensor(57.9837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3625 Loss: 0.7836699336767197 Val Loss: tensor(58.2622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3626 Loss: 0.769727349281311 Val Loss: tensor(57.9674, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3627 Loss: 0.7655065953731537 Val Loss: tensor(58.2372, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3628 Loss: 0.7503384798765182 Val Loss: tensor(57.9484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3629 Loss: 0.7448196858167648 Val Loss: tensor(58.2079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3630 Loss: 0.7295443266630173 Val Loss: tensor(57.9279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3631 Loss: 0.7237260937690735 Val Loss: tensor(58.1762, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3632 Loss: 0.7094468474388123 Val Loss: tensor(57.9077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3633 Loss: 0.7041585743427277 Val Loss: tensor(58.1439, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3634 Loss: 0.6917799860239029 Val Loss: tensor(57.8891, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3635 Loss: 0.6875369548797607 Val Loss: tensor(58.1122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3636 Loss: 0.6776628196239471 Val Loss: tensor(57.8736, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3637 Loss: 0.6747502982616425 Val Loss: tensor(58.0817, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3638 Loss: 0.6677230000495911 Val Loss: tensor(57.8617, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3639 Loss: 0.6661331504583359 Val Loss: tensor(58.0528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3640 Loss: 0.6620570719242096 Val Loss: tensor(57.8536, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3641 Loss: 0.6616077125072479 Val Loss: tensor(58.0251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3642 Loss: 0.6603754907846451 Val Loss: tensor(57.8484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3643 Loss: 0.6606665253639221 Val Loss: tensor(57.9982, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3644 Loss: 0.6619955003261566 Val Loss: tensor(57.8452, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3645 Loss: 0.6625377982854843 Val Loss: tensor(57.9714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3646 Loss: 0.6660087704658508 Val Loss: tensor(57.8430, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3647 Loss: 0.6662963181734085 Val Loss: tensor(57.9446, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3648 Loss: 0.6714683622121811 Val Loss: tensor(57.8408, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3649 Loss: 0.6711660027503967 Val Loss: tensor(57.9175, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3650 Loss: 0.6776349246501923 Val Loss: tensor(57.8383, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3651 Loss: 0.6766450107097626 Val Loss: tensor(57.8908, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3652 Loss: 0.6841367334127426 Val Loss: tensor(57.8361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3653 Loss: 0.682602271437645 Val Loss: tensor(57.8651, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3654 Loss: 0.6909695416688919 Val Loss: tensor(57.8350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3655 Loss: 0.689181923866272 Val Loss: tensor(57.8416, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3656 Loss: 0.6983587294816971 Val Loss: tensor(57.8359, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3657 Loss: 0.696605384349823 Val Loss: tensor(57.8211, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3658 Loss: 0.7065104246139526 Val Loss: tensor(57.8396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3659 Loss: 0.7049456685781479 Val Loss: tensor(57.8045, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3660 Loss: 0.7154043614864349 Val Loss: tensor(57.8464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3661 Loss: 0.714023157954216 Val Loss: tensor(57.7923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3662 Loss: 0.7246733754873276 Val Loss: tensor(57.8556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3663 Loss: 0.7232591807842255 Val Loss: tensor(57.7851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3664 Loss: 0.7334682792425156 Val Loss: tensor(57.8661, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3665 Loss: 0.7316067665815353 Val Loss: tensor(57.7829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3666 Loss: 0.7404851466417313 Val Loss: tensor(57.8750, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3667 Loss: 0.7374837696552277 Val Loss: tensor(57.7847, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3668 Loss: 0.7438948899507523 Val Loss: tensor(57.8787, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3669 Loss: 0.7388637214899063 Val Loss: tensor(57.7888, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3670 Loss: 0.7417870163917542 Val Loss: tensor(57.8728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3671 Loss: 0.7338630110025406 Val Loss: tensor(57.7922, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3672 Loss: 0.7329777628183365 Val Loss: tensor(57.8545, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3673 Loss: 0.7220221906900406 Val Loss: tensor(57.7924, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3674 Loss: 0.7184077352285385 Val Loss: tensor(57.8240, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3675 Loss: 0.7058436870574951 Val Loss: tensor(57.7896, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3676 Loss: 0.702107384800911 Val Loss: tensor(57.7872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3677 Loss: 0.6912694871425629 Val Loss: tensor(57.7880, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3678 Loss: 0.69102843105793 Val Loss: tensor(57.7529, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3679 Loss: 0.686871737241745 Val Loss: tensor(57.7953, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3680 Loss: 0.693995013833046 Val Loss: tensor(57.7320, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3681 Loss: 0.7023386657238007 Val Loss: tensor(57.8221, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3682 Loss: 0.7207857668399811 Val Loss: tensor(57.7347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3683 Loss: 0.7477610111236572 Val Loss: tensor(57.8800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3684 Loss: 0.7817359864711761 Val Loss: tensor(57.7708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3685 Loss: 0.8330710381269455 Val Loss: tensor(57.9799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3686 Loss: 0.8869629055261612 Val Loss: tensor(57.8474, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3687 Loss: 0.9659760296344757 Val Loss: tensor(58.1299, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3688 Loss: 1.0428698062896729 Val Loss: tensor(57.9664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3689 Loss: 1.1461818218231201 Val Loss: tensor(58.3268, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3690 Loss: 1.2437905371189117 Val Loss: tensor(58.1167, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3691 Loss: 1.3548251688480377 Val Loss: tensor(58.5446, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3692 Loss: 1.4565745294094086 Val Loss: tensor(58.2660, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3693 Loss: 1.54048353433609 Val Loss: tensor(58.7229, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3694 Loss: 1.6056967079639435 Val Loss: tensor(58.3653, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3695 Loss: 1.617608368396759 Val Loss: tensor(58.7812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3696 Loss: 1.5951389372348785 Val Loss: tensor(58.3700, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3697 Loss: 1.519344002008438 Val Loss: tensor(58.6785, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3698 Loss: 1.406419277191162 Val Loss: tensor(58.2667, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3699 Loss: 1.2907739579677582 Val Loss: tensor(58.4748, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3700 Loss: 1.1536494493484497 Val Loss: tensor(58.0933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3701 Loss: 1.0638065338134766 Val Loss: tensor(58.2809, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3702 Loss: 0.9644459635019302 Val Loss: tensor(57.9189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3703 Loss: 0.9241620600223541 Val Loss: tensor(58.1538, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3704 Loss: 0.8768370002508163 Val Loss: tensor(57.8008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3705 Loss: 0.8739612549543381 Val Loss: tensor(58.0943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3706 Loss: 0.8619291484355927 Val Loss: tensor(57.7584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3707 Loss: 0.8742591142654419 Val Loss: tensor(58.0783, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3708 Loss: 0.8737764954566956 Val Loss: tensor(57.7667, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3709 Loss: 0.8817739188671112 Val Loss: tensor(58.0723, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3710 Loss: 0.8757630884647369 Val Loss: tensor(57.7804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3711 Loss: 0.8710067123174667 Val Loss: tensor(58.0527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3712 Loss: 0.8545563519001007 Val Loss: tensor(57.7729, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3713 Loss: 0.8419310003519058 Val Loss: tensor(58.0140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3714 Loss: 0.8200210481882095 Val Loss: tensor(57.7522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3715 Loss: 0.809110164642334 Val Loss: tensor(57.9660, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3716 Loss: 0.7889254242181778 Val Loss: tensor(57.7375, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3717 Loss: 0.7849329859018326 Val Loss: tensor(57.9241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3718 Loss: 0.7698090374469757 Val Loss: tensor(57.7349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3719 Loss: 0.7726719230413437 Val Loss: tensor(57.8998, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3720 Loss: 0.7622193843126297 Val Loss: tensor(57.7378, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3721 Loss: 0.7701292484998703 Val Loss: tensor(57.8936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3722 Loss: 0.7629730999469757 Val Loss: tensor(57.7407, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3723 Loss: 0.7740636318922043 Val Loss: tensor(57.8990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3724 Loss: 0.7691098302602768 Val Loss: tensor(57.7438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3725 Loss: 0.781474232673645 Val Loss: tensor(57.9088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3726 Loss: 0.7775321006774902 Val Loss: tensor(57.7466, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3727 Loss: 0.7891485542058945 Val Loss: tensor(57.9177, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3728 Loss: 0.7848366796970367 Val Loss: tensor(57.7464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3729 Loss: 0.7939093261957169 Val Loss: tensor(57.9219, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3730 Loss: 0.7881234586238861 Val Loss: tensor(57.7401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3731 Loss: 0.7934982478618622 Val Loss: tensor(57.9191, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3732 Loss: 0.7857970744371414 Val Loss: tensor(57.7266, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3733 Loss: 0.7872172296047211 Val Loss: tensor(57.9096, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3734 Loss: 0.7779316902160645 Val Loss: tensor(57.7070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3735 Loss: 0.7759678363800049 Val Loss: tensor(57.8953, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3736 Loss: 0.765883594751358 Val Loss: tensor(57.6843, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3737 Loss: 0.7616006433963776 Val Loss: tensor(57.8790, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3738 Loss: 0.7516032308340073 Val Loss: tensor(57.6610, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3739 Loss: 0.7462698519229889 Val Loss: tensor(57.8634, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3740 Loss: 0.7371277362108231 Val Loss: tensor(57.6392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3741 Loss: 0.7319263517856598 Val Loss: tensor(57.8505, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3742 Loss: 0.7241591215133667 Val Loss: tensor(57.6200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3743 Loss: 0.7200687527656555 Val Loss: tensor(57.8418, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3744 Loss: 0.7139554768800735 Val Loss: tensor(57.6040, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3745 Loss: 0.7116396874189377 Val Loss: tensor(57.8382, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3746 Loss: 0.7072311639785767 Val Loss: tensor(57.5918, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3747 Loss: 0.7071043699979782 Val Loss: tensor(57.8402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3748 Loss: 0.7043202966451645 Val Loss: tensor(57.5836, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3749 Loss: 0.7065694332122803 Val Loss: tensor(57.8479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3750 Loss: 0.7052177488803864 Val Loss: tensor(57.5791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3751 Loss: 0.7098854780197144 Val Loss: tensor(57.8608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3752 Loss: 0.7097153514623642 Val Loss: tensor(57.5784, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3753 Loss: 0.7167680710554123 Val Loss: tensor(57.8783, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3754 Loss: 0.717503696680069 Val Loss: tensor(57.5810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3755 Loss: 0.7268315106630325 Val Loss: tensor(57.8995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3756 Loss: 0.7281899750232697 Val Loss: tensor(57.5866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3757 Loss: 0.7395748794078827 Val Loss: tensor(57.9234, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3758 Loss: 0.7412337958812714 Val Loss: tensor(57.5946, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3759 Loss: 0.7543595880270004 Val Loss: tensor(57.9488, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3760 Loss: 0.756011351943016 Val Loss: tensor(57.6048, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3761 Loss: 0.7704075872898102 Val Loss: tensor(57.9741, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3762 Loss: 0.7716840654611588 Val Loss: tensor(57.6166, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3763 Loss: 0.7867380976676941 Val Loss: tensor(57.9978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3764 Loss: 0.7872177362442017 Val Loss: tensor(57.6293, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3765 Loss: 0.8021836578845978 Val Loss: tensor(58.0182, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3766 Loss: 0.8014719635248184 Val Loss: tensor(57.6422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3767 Loss: 0.8155566453933716 Val Loss: tensor(58.0336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3768 Loss: 0.8132192939519882 Val Loss: tensor(57.6542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3769 Loss: 0.8257035166025162 Val Loss: tensor(58.0428, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3770 Loss: 0.8214401304721832 Val Loss: tensor(57.6646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3771 Loss: 0.8317305445671082 Val Loss: tensor(58.0450, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3772 Loss: 0.8253835141658783 Val Loss: tensor(57.6724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3773 Loss: 0.833141878247261 Val Loss: tensor(58.0399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3774 Loss: 0.8247545510530472 Val Loss: tensor(57.6773, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3775 Loss: 0.8298985809087753 Val Loss: tensor(58.0282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3776 Loss: 0.8197155892848969 Val Loss: tensor(57.6789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3777 Loss: 0.8224220424890518 Val Loss: tensor(58.0109, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3778 Loss: 0.8108187019824982 Val Loss: tensor(57.6772, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3779 Loss: 0.8114194124937057 Val Loss: tensor(57.9893, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3780 Loss: 0.7989179044961929 Val Loss: tensor(57.6728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3781 Loss: 0.7978944331407547 Val Loss: tensor(57.9650, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3782 Loss: 0.7850061357021332 Val Loss: tensor(57.6658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3783 Loss: 0.7828799486160278 Val Loss: tensor(57.9395, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3784 Loss: 0.770114541053772 Val Loss: tensor(57.6570, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3785 Loss: 0.7673886567354202 Val Loss: tensor(57.9141, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3786 Loss: 0.7551470249891281 Val Loss: tensor(57.6471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3787 Loss: 0.7522702068090439 Val Loss: tensor(57.8899, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3788 Loss: 0.740856721997261 Val Loss: tensor(57.6366, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3789 Loss: 0.7381710708141327 Val Loss: tensor(57.8674, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3790 Loss: 0.727774977684021 Val Loss: tensor(57.6261, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3791 Loss: 0.7255167812108994 Val Loss: tensor(57.8472, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3792 Loss: 0.7162256836891174 Val Loss: tensor(57.6157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3793 Loss: 0.7145501673221588 Val Loss: tensor(57.8293, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3794 Loss: 0.7063692808151245 Val Loss: tensor(57.6060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3795 Loss: 0.7053728252649307 Val Loss: tensor(57.8140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3796 Loss: 0.6982371509075165 Val Loss: tensor(57.5969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3797 Loss: 0.6979358196258545 Val Loss: tensor(57.8010, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3798 Loss: 0.6917701959609985 Val Loss: tensor(57.5886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3799 Loss: 0.6921735554933548 Val Loss: tensor(57.7903, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3800 Loss: 0.686882495880127 Val Loss: tensor(57.5810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3801 Loss: 0.6879851073026657 Val Loss: tensor(57.7817, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3802 Loss: 0.6834989339113235 Val Loss: tensor(57.5742, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3803 Loss: 0.6852912306785583 Val Loss: tensor(57.7753, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3804 Loss: 0.6815441995859146 Val Loss: tensor(57.5680, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3805 Loss: 0.6840381324291229 Val Loss: tensor(57.7708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3806 Loss: 0.6809790730476379 Val Loss: tensor(57.5627, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3807 Loss: 0.6842025816440582 Val Loss: tensor(57.7684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3808 Loss: 0.6818284690380096 Val Loss: tensor(57.5582, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3809 Loss: 0.6858183443546295 Val Loss: tensor(57.7680, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3810 Loss: 0.6841285079717636 Val Loss: tensor(57.5546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3811 Loss: 0.6889347285032272 Val Loss: tensor(57.7699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3812 Loss: 0.6879909783601761 Val Loss: tensor(57.5521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3813 Loss: 0.6937152296304703 Val Loss: tensor(57.7740, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3814 Loss: 0.6935763657093048 Val Loss: tensor(57.5508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3815 Loss: 0.7003260850906372 Val Loss: tensor(57.7807, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3816 Loss: 0.7010896354913712 Val Loss: tensor(57.5510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3817 Loss: 0.7089892029762268 Val Loss: tensor(57.7901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3818 Loss: 0.710797443985939 Val Loss: tensor(57.5531, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3819 Loss: 0.7199284583330154 Val Loss: tensor(57.8023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3820 Loss: 0.7229068726301193 Val Loss: tensor(57.5573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3821 Loss: 0.7333400547504425 Val Loss: tensor(57.8175, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3822 Loss: 0.7376169115304947 Val Loss: tensor(57.5641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3823 Loss: 0.7493285238742828 Val Loss: tensor(57.8354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3824 Loss: 0.7549923062324524 Val Loss: tensor(57.5737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3825 Loss: 0.7677757441997528 Val Loss: tensor(57.8557, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3826 Loss: 0.774783730506897 Val Loss: tensor(57.5862, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3827 Loss: 0.7882070094347 Val Loss: tensor(57.8770, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3828 Loss: 0.7963111400604248 Val Loss: tensor(57.6014, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3829 Loss: 0.8095752447843552 Val Loss: tensor(57.8978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3830 Loss: 0.8182328790426254 Val Loss: tensor(57.6184, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3831 Loss: 0.8301550149917603 Val Loss: tensor(57.9154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3832 Loss: 0.8384498506784439 Val Loss: tensor(57.6358, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3833 Loss: 0.8475201725959778 Val Loss: tensor(57.9267, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3834 Loss: 0.8541523516178131 Val Loss: tensor(57.6514, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3835 Loss: 0.8588011264801025 Val Loss: tensor(57.9282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3836 Loss: 0.8622554540634155 Val Loss: tensor(57.6627, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3837 Loss: 0.8611864745616913 Val Loss: tensor(57.9176, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3838 Loss: 0.8601056337356567 Val Loss: tensor(57.6671, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3839 Loss: 0.8529210090637207 Val Loss: tensor(57.8937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3840 Loss: 0.846525639295578 Val Loss: tensor(57.6631, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3841 Loss: 0.8339493274688721 Val Loss: tensor(57.8582, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3842 Loss: 0.8224438577890396 Val Loss: tensor(57.6502, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3843 Loss: 0.8062300980091095 Val Loss: tensor(57.8141, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3844 Loss: 0.7908265292644501 Val Loss: tensor(57.6298, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3845 Loss: 0.7732548415660858 Val Loss: tensor(57.7658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3846 Loss: 0.755775973200798 Val Loss: tensor(57.6043, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3847 Loss: 0.7389398068189621 Val Loss: tensor(57.7173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3848 Loss: 0.7212314307689667 Val Loss: tensor(57.5768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3849 Loss: 0.70660300552845 Val Loss: tensor(57.6717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3850 Loss: 0.6901363730430603 Val Loss: tensor(57.5500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3851 Loss: 0.6784800440073013 Val Loss: tensor(57.6309, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3852 Loss: 0.664152204990387 Val Loss: tensor(57.5263, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3853 Loss: 0.6556908488273621 Val Loss: tensor(57.5958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3854 Loss: 0.6439870595932007 Val Loss: tensor(57.5075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3855 Loss: 0.6387183666229248 Val Loss: tensor(57.5669, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3856 Loss: 0.6299206018447876 Val Loss: tensor(57.4944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3857 Loss: 0.6278126835823059 Val Loss: tensor(57.5442, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3858 Loss: 0.6222354024648666 Val Loss: tensor(57.4881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3859 Loss: 0.6233403980731964 Val Loss: tensor(57.5275, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3860 Loss: 0.6214894652366638 Val Loss: tensor(57.4892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3861 Loss: 0.6260860711336136 Val Loss: tensor(57.5173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3862 Loss: 0.6287746280431747 Val Loss: tensor(57.4990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3863 Loss: 0.6373981237411499 Val Loss: tensor(57.5141, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3864 Loss: 0.6458171010017395 Val Loss: tensor(57.5188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3865 Loss: 0.6593737006187439 Val Loss: tensor(57.5193, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3866 Loss: 0.6749922186136246 Val Loss: tensor(57.5504, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3867 Loss: 0.6948171406984329 Val Loss: tensor(57.5349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3868 Loss: 0.7191749215126038 Val Loss: tensor(57.5960, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3869 Loss: 0.7469508349895477 Val Loss: tensor(57.5627, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3870 Loss: 0.7811616361141205 Val Loss: tensor(57.6568, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3871 Loss: 0.8187555819749832 Val Loss: tensor(57.6043, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3872 Loss: 0.8625793755054474 Val Loss: tensor(57.7323, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3873 Loss: 0.9114292860031128 Val Loss: tensor(57.6594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3874 Loss: 0.9620825350284576 Val Loss: tensor(57.8186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3875 Loss: 1.0221199095249176 Val Loss: tensor(57.7256, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3876 Loss: 1.0730043053627014 Val Loss: tensor(57.9078, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3877 Loss: 1.14075568318367 Val Loss: tensor(57.7978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3878 Loss: 1.1812516748905182 Val Loss: tensor(57.9888, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3879 Loss: 1.2466053068637848 Val Loss: tensor(57.8674, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3880 Loss: 1.2629777789115906 Val Loss: tensor(58.0464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3881 Loss: 1.3068072199821472 Val Loss: tensor(57.9170, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3882 Loss: 1.2858311533927917 Val Loss: tensor(58.0582, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3883 Loss: 1.286345899105072 Val Loss: tensor(57.9202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3884 Loss: 1.2259686291217804 Val Loss: tensor(58.0056, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3885 Loss: 1.1792429089546204 Val Loss: tensor(57.8650, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3886 Loss: 1.1004250943660736 Val Loss: tensor(57.9014, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3887 Loss: 1.0314912796020508 Val Loss: tensor(57.7800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3888 Loss: 0.9652687609195709 Val Loss: tensor(57.7900, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3889 Loss: 0.9039851576089859 Val Loss: tensor(57.7077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3890 Loss: 0.8632806837558746 Val Loss: tensor(57.7028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3891 Loss: 0.8193279206752777 Val Loss: tensor(57.6553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3892 Loss: 0.7962025851011276 Val Loss: tensor(57.6392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3893 Loss: 0.7648568749427795 Val Loss: tensor(57.6047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3894 Loss: 0.7482892423868179 Val Loss: tensor(57.5884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3895 Loss: 0.725218802690506 Val Loss: tensor(57.5518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3896 Loss: 0.711898609995842 Val Loss: tensor(57.5476, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3897 Loss: 0.6956804096698761 Val Loss: tensor(57.5060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3898 Loss: 0.6864614933729172 Val Loss: tensor(57.5175, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3899 Loss: 0.6752541214227676 Val Loss: tensor(57.4718, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3900 Loss: 0.6699942052364349 Val Loss: tensor(57.4962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3901 Loss: 0.6616179645061493 Val Loss: tensor(57.4465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3902 Loss: 0.6589908301830292 Val Loss: tensor(57.4805, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3903 Loss: 0.6521255075931549 Val Loss: tensor(57.4263, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3904 Loss: 0.6510340720415115 Val Loss: tensor(57.4678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3905 Loss: 0.6451360583305359 Val Loss: tensor(57.4100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3906 Loss: 0.6450629681348801 Val Loss: tensor(57.4567, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3907 Loss: 0.6399425566196442 Val Loss: tensor(57.3967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3908 Loss: 0.6406936943531036 Val Loss: tensor(57.4469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3909 Loss: 0.6362079828977585 Val Loss: tensor(57.3855, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3910 Loss: 0.6376648098230362 Val Loss: tensor(57.4384, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3911 Loss: 0.6336602419614792 Val Loss: tensor(57.3758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3912 Loss: 0.6357344835996628 Val Loss: tensor(57.4314, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3913 Loss: 0.6320545077323914 Val Loss: tensor(57.3666, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3914 Loss: 0.6346444636583328 Val Loss: tensor(57.4255, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3915 Loss: 0.6311828345060349 Val Loss: tensor(57.3573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3916 Loss: 0.6342078149318695 Val Loss: tensor(57.4204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3917 Loss: 0.6308848708868027 Val Loss: tensor(57.3481, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3918 Loss: 0.6342651546001434 Val Loss: tensor(57.4157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3919 Loss: 0.6310199350118637 Val Loss: tensor(57.3390, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3920 Loss: 0.6347172111272812 Val Loss: tensor(57.4110, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3921 Loss: 0.6315203011035919 Val Loss: tensor(57.3304, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3922 Loss: 0.6354832053184509 Val Loss: tensor(57.4064, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3923 Loss: 0.6323161572217941 Val Loss: tensor(57.3223, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3924 Loss: 0.6364913880825043 Val Loss: tensor(57.4019, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3925 Loss: 0.6333720088005066 Val Loss: tensor(57.3149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3926 Loss: 0.6377432197332382 Val Loss: tensor(57.3975, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3927 Loss: 0.6346866190433502 Val Loss: tensor(57.3080, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3928 Loss: 0.6392226964235306 Val Loss: tensor(57.3932, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3929 Loss: 0.6362626403570175 Val Loss: tensor(57.3016, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3930 Loss: 0.6409614831209183 Val Loss: tensor(57.3890, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3931 Loss: 0.6381073445081711 Val Loss: tensor(57.2958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3932 Loss: 0.6429715305566788 Val Loss: tensor(57.3848, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3933 Loss: 0.6402623653411865 Val Loss: tensor(57.2904, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3934 Loss: 0.6452720016241074 Val Loss: tensor(57.3807, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3935 Loss: 0.6427445113658905 Val Loss: tensor(57.2856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3936 Loss: 0.6479005515575409 Val Loss: tensor(57.3767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3937 Loss: 0.6455577164888382 Val Loss: tensor(57.2814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3938 Loss: 0.6508685052394867 Val Loss: tensor(57.3728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3939 Loss: 0.6487451940774918 Val Loss: tensor(57.2775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3940 Loss: 0.654176875948906 Val Loss: tensor(57.3690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3941 Loss: 0.6522972136735916 Val Loss: tensor(57.2741, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3942 Loss: 0.6578532755374908 Val Loss: tensor(57.3654, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3943 Loss: 0.6562163084745407 Val Loss: tensor(57.2711, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3944 Loss: 0.6618798673152924 Val Loss: tensor(57.3619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3945 Loss: 0.6604621112346649 Val Loss: tensor(57.2684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3946 Loss: 0.666229322552681 Val Loss: tensor(57.3584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3947 Loss: 0.6650414913892746 Val Loss: tensor(57.2659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3948 Loss: 0.6708840131759644 Val Loss: tensor(57.3550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3949 Loss: 0.6699060350656509 Val Loss: tensor(57.2636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3950 Loss: 0.675817459821701 Val Loss: tensor(57.3517, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3951 Loss: 0.6750304251909256 Val Loss: tensor(57.2615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3952 Loss: 0.6810125261545181 Val Loss: tensor(57.3484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3953 Loss: 0.6804080307483673 Val Loss: tensor(57.2594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3954 Loss: 0.6864511668682098 Val Loss: tensor(57.3454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3955 Loss: 0.6860337555408478 Val Loss: tensor(57.2573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3956 Loss: 0.6921380758285522 Val Loss: tensor(57.3426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3957 Loss: 0.6919381767511368 Val Loss: tensor(57.2553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3958 Loss: 0.6981177479028702 Val Loss: tensor(57.3401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3959 Loss: 0.6981714814901352 Val Loss: tensor(57.2535, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3960 Loss: 0.704469695687294 Val Loss: tensor(57.3382, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3961 Loss: 0.704892098903656 Val Loss: tensor(57.2519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3962 Loss: 0.7113698273897171 Val Loss: tensor(57.3373, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3963 Loss: 0.7123122662305832 Val Loss: tensor(57.2509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3964 Loss: 0.7190822064876556 Val Loss: tensor(57.3377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3965 Loss: 0.7207746654748917 Val Loss: tensor(57.2508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3966 Loss: 0.7280011475086212 Val Loss: tensor(57.3401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3967 Loss: 0.7308028787374496 Val Loss: tensor(57.2523, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3968 Loss: 0.7387353330850601 Val Loss: tensor(57.3453, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3969 Loss: 0.7431416362524033 Val Loss: tensor(57.2561, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3970 Loss: 0.7521197199821472 Val Loss: tensor(57.3545, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3971 Loss: 0.7588099837303162 Val Loss: tensor(57.2638, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3972 Loss: 0.7693465352058411 Val Loss: tensor(57.3692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3973 Loss: 0.7792740017175674 Val Loss: tensor(57.2768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3974 Loss: 0.7920254319906235 Val Loss: tensor(57.3915, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3975 Loss: 0.8064213842153549 Val Loss: tensor(57.2977, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3976 Loss: 0.82232965528965 Val Loss: tensor(57.4244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3977 Loss: 0.8427994847297668 Val Loss: tensor(57.3298, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3978 Loss: 0.8629961907863617 Val Loss: tensor(57.4714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3979 Loss: 0.8914735615253448 Val Loss: tensor(57.3771, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3980 Loss: 0.9172980785369873 Val Loss: tensor(57.5374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3981 Loss: 0.9557294547557831 Val Loss: tensor(57.4441, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3982 Loss: 0.9883778989315033 Val Loss: tensor(57.6264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3983 Loss: 1.0378694832324982 Val Loss: tensor(57.5342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3984 Loss: 1.0775453746318817 Val Loss: tensor(57.7402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3985 Loss: 1.136289805173874 Val Loss: tensor(57.6452, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3986 Loss: 1.1804518699645996 Val Loss: tensor(57.8717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3987 Loss: 1.2402089834213257 Val Loss: tensor(57.7620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3988 Loss: 1.2806443274021149 Val Loss: tensor(57.9962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3989 Loss: 1.3233900666236877 Val Loss: tensor(57.8491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3990 Loss: 1.34438356757164 Val Loss: tensor(58.0671, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3991 Loss: 1.345876395702362 Val Loss: tensor(57.8610, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3992 Loss: 1.329991951584816 Val Loss: tensor(58.0375, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3993 Loss: 1.2772356122732162 Val Loss: tensor(57.7787, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3994 Loss: 1.2217352986335754 Val Loss: tensor(57.9058, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3995 Loss: 1.1308943033218384 Val Loss: tensor(57.6391, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3996 Loss: 1.0559020191431046 Val Loss: tensor(57.7309, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3997 Loss: 0.9615826904773712 Val Loss: tensor(57.5013, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3998 Loss: 0.8932256549596786 Val Loss: tensor(57.5760, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3999 Loss: 0.8203170895576477 Val Loss: tensor(57.3977, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4000 Loss: 0.7718255370855331 Val Loss: tensor(57.4631, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4001 Loss: 0.7253264337778091 Val Loss: tensor(57.3287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4002 Loss: 0.6961072832345963 Val Loss: tensor(57.3835, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4003 Loss: 0.6697944104671478 Val Loss: tensor(57.2839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4004 Loss: 0.6534590125083923 Val Loss: tensor(57.3248, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4005 Loss: 0.6394215524196625 Val Loss: tensor(57.2535, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4006 Loss: 0.6300290524959564 Val Loss: tensor(57.2804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4007 Loss: 0.6224670857191086 Val Loss: tensor(57.2307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4008 Loss: 0.6166158467531204 Val Loss: tensor(57.2481, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4009 Loss: 0.6120773702859879 Val Loss: tensor(57.2126, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4010 Loss: 0.6083298027515411 Val Loss: tensor(57.2254, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4011 Loss: 0.6050477474927902 Val Loss: tensor(57.1988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4012 Loss: 0.6028472036123276 Val Loss: tensor(57.2099, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4013 Loss: 0.6000667214393616 Val Loss: tensor(57.1887, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4014 Loss: 0.5990206301212311 Val Loss: tensor(57.1984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4015 Loss: 0.5965221226215363 Val Loss: tensor(57.1812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4016 Loss: 0.5962590426206589 Val Loss: tensor(57.1888, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4017 Loss: 0.5940330028533936 Val Loss: tensor(57.1749, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4018 Loss: 0.5942401885986328 Val Loss: tensor(57.1800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4019 Loss: 0.592390775680542 Val Loss: tensor(57.1688, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4020 Loss: 0.592913493514061 Val Loss: tensor(57.1720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4021 Loss: 0.5915520489215851 Val Loss: tensor(57.1629, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4022 Loss: 0.5923121869564056 Val Loss: tensor(57.1650, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4023 Loss: 0.5915959030389786 Val Loss: tensor(57.1575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4024 Loss: 0.5925951898097992 Val Loss: tensor(57.1589, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4025 Loss: 0.592657208442688 Val Loss: tensor(57.1530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4026 Loss: 0.5939084142446518 Val Loss: tensor(57.1537, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4027 Loss: 0.5948555320501328 Val Loss: tensor(57.1498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4028 Loss: 0.59635229408741 Val Loss: tensor(57.1491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4029 Loss: 0.5982907265424728 Val Loss: tensor(57.1482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4030 Loss: 0.6000282764434814 Val Loss: tensor(57.1448, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4031 Loss: 0.6030377447605133 Val Loss: tensor(57.1484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4032 Loss: 0.6050058454275131 Val Loss: tensor(57.1402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4033 Loss: 0.6091304868459702 Val Loss: tensor(57.1508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4034 Loss: 0.6113400161266327 Val Loss: tensor(57.1349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4035 Loss: 0.6166521906852722 Val Loss: tensor(57.1557, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4036 Loss: 0.619145467877388 Val Loss: tensor(57.1287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4037 Loss: 0.625706136226654 Val Loss: tensor(57.1630, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4038 Loss: 0.6285795420408249 Val Loss: tensor(57.1218, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4039 Loss: 0.6365144699811935 Val Loss: tensor(57.1728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4040 Loss: 0.639920100569725 Val Loss: tensor(57.1147, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4041 Loss: 0.6493639796972275 Val Loss: tensor(57.1852, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4042 Loss: 0.6535146981477737 Val Loss: tensor(57.1084, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4043 Loss: 0.6646665036678314 Val Loss: tensor(57.2003, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4044 Loss: 0.6698339134454727 Val Loss: tensor(57.1044, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4045 Loss: 0.6828023046255112 Val Loss: tensor(57.2180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4046 Loss: 0.6892478764057159 Val Loss: tensor(57.1039, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4047 Loss: 0.7040408700704575 Val Loss: tensor(57.2383, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4048 Loss: 0.7119033336639404 Val Loss: tensor(57.1079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4049 Loss: 0.7282244712114334 Val Loss: tensor(57.2606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4050 Loss: 0.7374342530965805 Val Loss: tensor(57.1170, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4051 Loss: 0.7545757740736008 Val Loss: tensor(57.2840, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4052 Loss: 0.7646138221025467 Val Loss: tensor(57.1304, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4053 Loss: 0.7813640385866165 Val Loss: tensor(57.3065, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4054 Loss: 0.7911898642778397 Val Loss: tensor(57.1466, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4055 Loss: 0.8059612661600113 Val Loss: tensor(57.3257, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4056 Loss: 0.81407630443573 Val Loss: tensor(57.1628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4057 Loss: 0.8251840621232986 Val Loss: tensor(57.3391, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4058 Loss: 0.8299471437931061 Val Loss: tensor(57.1764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4059 Loss: 0.836236834526062 Val Loss: tensor(57.3450, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4060 Loss: 0.836577445268631 Val Loss: tensor(57.1853, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4061 Loss: 0.8379632234573364 Val Loss: tensor(57.3429, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4062 Loss: 0.8339129090309143 Val Loss: tensor(57.1891, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4063 Loss: 0.8316141366958618 Val Loss: tensor(57.3348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4064 Loss: 0.8245307803153992 Val Loss: tensor(57.1890, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4065 Loss: 0.8206877410411835 Val Loss: tensor(57.3237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4066 Loss: 0.8126834034919739 Val Loss: tensor(57.1882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4067 Loss: 0.8096030354499817 Val Loss: tensor(57.3131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4068 Loss: 0.8028432577848434 Val Loss: tensor(57.1898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4069 Loss: 0.8023991584777832 Val Loss: tensor(57.3063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4070 Loss: 0.7983337193727493 Val Loss: tensor(57.1969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4071 Loss: 0.80159892141819 Val Loss: tensor(57.3054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4072 Loss: 0.8006971329450607 Val Loss: tensor(57.2104, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4073 Loss: 0.807906910777092 Val Loss: tensor(57.3107, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4074 Loss: 0.809538945555687 Val Loss: tensor(57.2288, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4075 Loss: 0.8200013786554337 Val Loss: tensor(57.3213, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4076 Loss: 0.8224870264530182 Val Loss: tensor(57.2473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4077 Loss: 0.8344445526599884 Val Loss: tensor(57.3337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4078 Loss: 0.8352715075016022 Val Loss: tensor(57.2585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4079 Loss: 0.8460119962692261 Val Loss: tensor(57.3428, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4080 Loss: 0.8424844592809677 Val Loss: tensor(57.2550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4081 Loss: 0.8491472899913788 Val Loss: tensor(57.3430, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4082 Loss: 0.8394284546375275 Val Loss: tensor(57.2334, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4083 Loss: 0.8402581363916397 Val Loss: tensor(57.3308, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4084 Loss: 0.8242349922657013 Val Loss: tensor(57.1964, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4085 Loss: 0.8195033669471741 Val Loss: tensor(57.3067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4086 Loss: 0.7987620681524277 Val Loss: tensor(57.1512, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4087 Loss: 0.7903141677379608 Val Loss: tensor(57.2735, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4088 Loss: 0.7671713531017303 Val Loss: tensor(57.1057, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4089 Loss: 0.7571227550506592 Val Loss: tensor(57.2349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4090 Loss: 0.7337384223937988 Val Loss: tensor(57.0645, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4091 Loss: 0.7234863638877869 Val Loss: tensor(57.1933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4092 Loss: 0.701337531208992 Val Loss: tensor(57.0291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4093 Loss: 0.6915931701660156 Val Loss: tensor(57.1507, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4094 Loss: 0.6716051399707794 Val Loss: tensor(56.9990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4095 Loss: 0.6628105193376541 Val Loss: tensor(57.1098, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4096 Loss: 0.6456073373556137 Val Loss: tensor(56.9735, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4097 Loss: 0.6382901519536972 Val Loss: tensor(57.0740, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4098 Loss: 0.6242991238832474 Val Loss: tensor(56.9522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4099 Loss: 0.6189526915550232 Val Loss: tensor(57.0463, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4100 Loss: 0.6083125323057175 Val Loss: tensor(56.9346, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4101 Loss: 0.6051348894834518 Val Loss: tensor(57.0280, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4102 Loss: 0.5975309461355209 Val Loss: tensor(56.9199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4103 Loss: 0.5962397158145905 Val Loss: tensor(57.0180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4104 Loss: 0.5910496562719345 Val Loss: tensor(56.9075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4105 Loss: 0.5910793393850327 Val Loss: tensor(57.0131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4106 Loss: 0.5875426530838013 Val Loss: tensor(56.8979, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4107 Loss: 0.5883113145828247 Val Loss: tensor(57.0088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4108 Loss: 0.5857177823781967 Val Loss: tensor(56.8923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4109 Loss: 0.5868319272994995 Val Loss: tensor(57.0010, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4110 Loss: 0.5845825970172882 Val Loss: tensor(56.8917, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4111 Loss: 0.5858249664306641 Val Loss: tensor(56.9873, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4112 Loss: 0.5834712386131287 Val Loss: tensor(56.8957, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4113 Loss: 0.5846970826387405 Val Loss: tensor(56.9676, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4114 Loss: 0.5820882767438889 Val Loss: tensor(56.9021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4115 Loss: 0.5833355337381363 Val Loss: tensor(56.9455, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4116 Loss: 0.5806967616081238 Val Loss: tensor(56.9072, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4117 Loss: 0.5821020603179932 Val Loss: tensor(56.9258, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4118 Loss: 0.5798185467720032 Val Loss: tensor(56.9076, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4119 Loss: 0.5814455598592758 Val Loss: tensor(56.9127, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4120 Loss: 0.5797083526849747 Val Loss: tensor(56.9011, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4121 Loss: 0.5814253091812134 Val Loss: tensor(56.9081, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4122 Loss: 0.5801142603158951 Val Loss: tensor(56.8879, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4123 Loss: 0.5818023979663849 Val Loss: tensor(56.9103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4124 Loss: 0.580653727054596 Val Loss: tensor(56.8705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4125 Loss: 0.5823995620012283 Val Loss: tensor(56.9152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4126 Loss: 0.5812622010707855 Val Loss: tensor(56.8529, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4127 Loss: 0.5833563655614853 Val Loss: tensor(56.9189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4128 Loss: 0.5822508633136749 Val Loss: tensor(56.8389, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4129 Loss: 0.5849197953939438 Val Loss: tensor(56.9200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4130 Loss: 0.5840055644512177 Val Loss: tensor(56.8299, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4131 Loss: 0.5873528569936752 Val Loss: tensor(56.9198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4132 Loss: 0.5868817716836929 Val Loss: tensor(56.8245, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4133 Loss: 0.5909201353788376 Val Loss: tensor(56.9217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4134 Loss: 0.5911664366722107 Val Loss: tensor(56.8195, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4135 Loss: 0.5958759039640427 Val Loss: tensor(56.9288, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4136 Loss: 0.5970879048109055 Val Loss: tensor(56.8122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4137 Loss: 0.602570965886116 Val Loss: tensor(56.9429, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4138 Loss: 0.6051120609045029 Val Loss: tensor(56.8034, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4139 Loss: 0.6117132902145386 Val Loss: tensor(56.9638, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4140 Loss: 0.6162264496088028 Val Loss: tensor(56.7970, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4141 Loss: 0.6243717223405838 Val Loss: tensor(56.9900, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4142 Loss: 0.631606251001358 Val Loss: tensor(56.7982, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4143 Loss: 0.641141951084137 Val Loss: tensor(57.0186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4144 Loss: 0.6513848453760147 Val Loss: tensor(56.8104, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4145 Loss: 0.6608323603868484 Val Loss: tensor(57.0430, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4146 Loss: 0.6728814840316772 Val Loss: tensor(56.8325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4147 Loss: 0.6791811734437943 Val Loss: tensor(57.0522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4148 Loss: 0.6894914358854294 Val Loss: tensor(56.8573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4149 Loss: 0.689242497086525 Val Loss: tensor(57.0339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4150 Loss: 0.6930087953805923 Val Loss: tensor(56.8733, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4151 Loss: 0.685371994972229 Val Loss: tensor(56.9845, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4152 Loss: 0.6801903247833252 Val Loss: tensor(56.8729, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4153 Loss: 0.6684765219688416 Val Loss: tensor(56.9158, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4154 Loss: 0.6568751037120819 Val Loss: tensor(56.8575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4155 Loss: 0.6459968686103821 Val Loss: tensor(56.8470, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4156 Loss: 0.6330000907182693 Val Loss: tensor(56.8351, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4157 Loss: 0.6260419636964798 Val Loss: tensor(56.7900, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4158 Loss: 0.6154113411903381 Val Loss: tensor(56.8156, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4159 Loss: 0.6130377799272537 Val Loss: tensor(56.7451, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4160 Loss: 0.606086328625679 Val Loss: tensor(56.8064, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4161 Loss: 0.6074298918247223 Val Loss: tensor(56.7072, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4162 Loss: 0.6036406755447388 Val Loss: tensor(56.8089, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4163 Loss: 0.6072062999010086 Val Loss: tensor(56.6748, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4164 Loss: 0.6055669635534286 Val Loss: tensor(56.8173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4165 Loss: 0.6103443503379822 Val Loss: tensor(56.6533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4166 Loss: 0.6103929728269577 Val Loss: tensor(56.8225, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4167 Loss: 0.6160382330417633 Val Loss: tensor(56.6488, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4168 Loss: 0.6173655092716217 Val Loss: tensor(56.8186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4169 Loss: 0.6236133426427841 Val Loss: tensor(56.6606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4170 Loss: 0.62542924284935 Val Loss: tensor(56.8075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4171 Loss: 0.6322228610515594 Val Loss: tensor(56.6798, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4172 Loss: 0.6337921619415283 Val Loss: tensor(56.7964, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4173 Loss: 0.6415475010871887 Val Loss: tensor(56.6966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4174 Loss: 0.6427873522043228 Val Loss: tensor(56.7910, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4175 Loss: 0.6525604575872421 Val Loss: tensor(56.7071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4176 Loss: 0.6544963121414185 Val Loss: tensor(56.7925, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4177 Loss: 0.6682668924331665 Val Loss: tensor(56.7164, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4178 Loss: 0.6731939166784286 Val Loss: tensor(56.7989, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4179 Loss: 0.6936341375112534 Val Loss: tensor(56.7338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4180 Loss: 0.7047735303640366 Val Loss: tensor(56.8090, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4181 Loss: 0.7345192432403564 Val Loss: tensor(56.7676, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4182 Loss: 0.7556178420782089 Val Loss: tensor(56.8252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4183 Loss: 0.7966576367616653 Val Loss: tensor(56.8231, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4184 Loss: 0.8315240442752838 Val Loss: tensor(56.8534, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4185 Loss: 0.8841748386621475 Val Loss: tensor(56.9024, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4186 Loss: 0.9351207613945007 Val Loss: tensor(56.8985, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4187 Loss: 0.9947773814201355 Val Loss: tensor(56.9996, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4188 Loss: 1.0585909187793732 Val Loss: tensor(56.9564, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4189 Loss: 1.1109461784362793 Val Loss: tensor(57.0953, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4190 Loss: 1.1733642518520355 Val Loss: tensor(57.0082, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4191 Loss: 1.193841964006424 Val Loss: tensor(57.1630, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4192 Loss: 1.2304927110671997 Val Loss: tensor(57.0318, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4193 Loss: 1.1985207200050354 Val Loss: tensor(57.1924, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4194 Loss: 1.1965503990650177 Val Loss: tensor(57.0286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4195 Loss: 1.1239674091339111 Val Loss: tensor(57.2029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4196 Loss: 1.1076070666313171 Val Loss: tensor(57.0251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4197 Loss: 1.0405384600162506 Val Loss: tensor(57.2326, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4198 Loss: 1.047285944223404 Val Loss: tensor(57.0360, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4199 Loss: 1.0205904245376587 Val Loss: tensor(57.3175, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4200 Loss: 1.0674959868192673 Val Loss: tensor(57.0818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4201 Loss: 1.0908843874931335 Val Loss: tensor(57.4729, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4202 Loss: 1.1897265762090683 Val Loss: tensor(57.2236, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4203 Loss: 1.2688654363155365 Val Loss: tensor(57.7045, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4204 Loss: 1.4292695820331573 Val Loss: tensor(57.5010, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4205 Loss: 1.5601650476455688 Val Loss: tensor(57.9992, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4206 Loss: 1.753705233335495 Val Loss: tensor(57.8355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4207 Loss: 1.8724073469638824 Val Loss: tensor(58.2300, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4208 Loss: 1.9734018743038177 Val Loss: tensor(57.9644, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4209 Loss: 1.9254036247730255 Val Loss: tensor(58.1399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4210 Loss: 1.7999013364315033 Val Loss: tensor(57.7111, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4211 Loss: 1.558941125869751 Val Loss: tensor(57.7181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4212 Loss: 1.3192759454250336 Val Loss: tensor(57.3678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4213 Loss: 1.0867391228675842 Val Loss: tensor(57.3360, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4214 Loss: 0.9178462326526642 Val Loss: tensor(57.1873, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4215 Loss: 0.7914991080760956 Val Loss: tensor(57.1106, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4216 Loss: 0.7181878983974457 Val Loss: tensor(57.1224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4217 Loss: 0.669600784778595 Val Loss: tensor(56.9774, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4218 Loss: 0.6480405926704407 Val Loss: tensor(57.0763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4219 Loss: 0.6343687921762466 Val Loss: tensor(56.8960, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4220 Loss: 0.629081591963768 Val Loss: tensor(57.0207, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4221 Loss: 0.6235924959182739 Val Loss: tensor(56.8468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4222 Loss: 0.6193497329950333 Val Loss: tensor(56.9676, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4223 Loss: 0.6137546747922897 Val Loss: tensor(56.8253, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4224 Loss: 0.6092360317707062 Val Loss: tensor(56.9198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4225 Loss: 0.60285784304142 Val Loss: tensor(56.8208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4226 Loss: 0.5989295989274979 Val Loss: tensor(56.8819, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4227 Loss: 0.5925524532794952 Val Loss: tensor(56.8240, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4228 Loss: 0.5893511772155762 Val Loss: tensor(56.8541, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4229 Loss: 0.5843067467212677 Val Loss: tensor(56.8304, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4230 Loss: 0.5818506181240082 Val Loss: tensor(56.8338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4231 Loss: 0.5790284425020218 Val Loss: tensor(56.8347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4232 Loss: 0.5772293955087662 Val Loss: tensor(56.8196, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4233 Loss: 0.5763522833585739 Val Loss: tensor(56.8327, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4234 Loss: 0.5749461799860001 Val Loss: tensor(56.8097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4235 Loss: 0.5750219076871872 Val Loss: tensor(56.8238, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4236 Loss: 0.5736003667116165 Val Loss: tensor(56.8040, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4237 Loss: 0.5737975537776947 Val Loss: tensor(56.8095, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4238 Loss: 0.572220966219902 Val Loss: tensor(56.8022, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4239 Loss: 0.5721743404865265 Val Loss: tensor(56.7935, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4240 Loss: 0.5707953870296478 Val Loss: tensor(56.8019, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4241 Loss: 0.570496067404747 Val Loss: tensor(56.7793, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4242 Loss: 0.5698227286338806 Val Loss: tensor(56.7997, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4243 Loss: 0.5693750828504562 Val Loss: tensor(56.7691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4244 Loss: 0.5695469677448273 Val Loss: tensor(56.7934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4245 Loss: 0.5688841342926025 Val Loss: tensor(56.7637, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4246 Loss: 0.569543719291687 Val Loss: tensor(56.7823, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4247 Loss: 0.5684893131256104 Val Loss: tensor(56.7619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4248 Loss: 0.5691483914852142 Val Loss: tensor(56.7684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4249 Loss: 0.5677998960018158 Val Loss: tensor(56.7607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4250 Loss: 0.5682063102722168 Val Loss: tensor(56.7549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4251 Loss: 0.5670198351144791 Val Loss: tensor(56.7569, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4252 Loss: 0.5671978294849396 Val Loss: tensor(56.7451, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4253 Loss: 0.5665660202503204 Val Loss: tensor(56.7488, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4254 Loss: 0.5665155202150345 Val Loss: tensor(56.7402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4255 Loss: 0.5664205998182297 Val Loss: tensor(56.7369, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4256 Loss: 0.5660673677921295 Val Loss: tensor(56.7382, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4257 Loss: 0.5661973059177399 Val Loss: tensor(56.7239, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4258 Loss: 0.5656604468822479 Val Loss: tensor(56.7350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4259 Loss: 0.5657489448785782 Val Loss: tensor(56.7140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4260 Loss: 0.5653214454650879 Val Loss: tensor(56.7266, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4261 Loss: 0.5652377009391785 Val Loss: tensor(56.7103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4262 Loss: 0.5651061832904816 Val Loss: tensor(56.7119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4263 Loss: 0.5647627860307693 Val Loss: tensor(56.7119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4264 Loss: 0.5648315399885178 Val Loss: tensor(56.6936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4265 Loss: 0.5643519759178162 Val Loss: tensor(56.7142, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4266 Loss: 0.5644579082727432 Val Loss: tensor(56.6779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4267 Loss: 0.5641572177410126 Val Loss: tensor(56.7108, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4268 Loss: 0.5641505718231201 Val Loss: tensor(56.6706, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4269 Loss: 0.5642282366752625 Val Loss: tensor(56.6975, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4270 Loss: 0.5638942271471024 Val Loss: tensor(56.6732, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4271 Loss: 0.5641820132732391 Val Loss: tensor(56.6761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4272 Loss: 0.563501626253128 Val Loss: tensor(56.6804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4273 Loss: 0.563790500164032 Val Loss: tensor(56.6543, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4274 Loss: 0.5632121562957764 Val Loss: tensor(56.6832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4275 Loss: 0.5633799731731415 Val Loss: tensor(56.6410, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4276 Loss: 0.5632738918066025 Val Loss: tensor(56.6744, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4277 Loss: 0.5630697160959244 Val Loss: tensor(56.6404, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4278 Loss: 0.5633210837841034 Val Loss: tensor(56.6543, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4279 Loss: 0.5626126825809479 Val Loss: tensor(56.6473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4280 Loss: 0.5629678070545197 Val Loss: tensor(56.6316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4281 Loss: 0.562237560749054 Val Loss: tensor(56.6512, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4282 Loss: 0.5625661462545395 Val Loss: tensor(56.6181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4283 Loss: 0.5622857958078384 Val Loss: tensor(56.6421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4284 Loss: 0.562255010008812 Val Loss: tensor(56.6190, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4285 Loss: 0.562281146645546 Val Loss: tensor(56.6195, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4286 Loss: 0.56171615421772 Val Loss: tensor(56.6285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4287 Loss: 0.5618405491113663 Val Loss: tensor(56.5944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4288 Loss: 0.561308279633522 Val Loss: tensor(56.6335, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4289 Loss: 0.5614691972732544 Val Loss: tensor(56.5808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4290 Loss: 0.5614497810602188 Val Loss: tensor(56.6235, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4291 Loss: 0.5612630844116211 Val Loss: tensor(56.5834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4292 Loss: 0.5615687221288681 Val Loss: tensor(56.5998, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4293 Loss: 0.5608972162008286 Val Loss: tensor(56.5936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4294 Loss: 0.5613259077072144 Val Loss: tensor(56.5758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4295 Loss: 0.5606931000947952 Val Loss: tensor(56.5966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4296 Loss: 0.5611001402139664 Val Loss: tensor(56.5652, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4297 Loss: 0.5607626438140869 Val Loss: tensor(56.5834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4298 Loss: 0.5608218163251877 Val Loss: tensor(56.5697, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4299 Loss: 0.560662180185318 Val Loss: tensor(56.5593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4300 Loss: 0.5605297535657883 Val Loss: tensor(56.5786, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4301 Loss: 0.5604615062475204 Val Loss: tensor(56.5379, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4302 Loss: 0.5606259107589722 Val Loss: tensor(56.5793, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4303 Loss: 0.5604837387800217 Val Loss: tensor(56.5279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4304 Loss: 0.561020016670227 Val Loss: tensor(56.5696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4305 Loss: 0.5606889426708221 Val Loss: tensor(56.5257, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4306 Loss: 0.5615322142839432 Val Loss: tensor(56.5578, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4307 Loss: 0.5612050294876099 Val Loss: tensor(56.5210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4308 Loss: 0.5622768998146057 Val Loss: tensor(56.5523, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4309 Loss: 0.5620864182710648 Val Loss: tensor(56.5094, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4310 Loss: 0.5634725838899612 Val Loss: tensor(56.5533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4311 Loss: 0.5635665357112885 Val Loss: tensor(56.4952, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4312 Loss: 0.5656311213970184 Val Loss: tensor(56.5552, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4313 Loss: 0.5661420822143555 Val Loss: tensor(56.4832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4314 Loss: 0.569165825843811 Val Loss: tensor(56.5586, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4315 Loss: 0.570363238453865 Val Loss: tensor(56.4705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4316 Loss: 0.5746545493602753 Val Loss: tensor(56.5699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4317 Loss: 0.5771164894104004 Val Loss: tensor(56.4523, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4318 Loss: 0.5832648873329163 Val Loss: tensor(56.5934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4319 Loss: 0.5879421830177307 Val Loss: tensor(56.4310, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4320 Loss: 0.5972143858671188 Val Loss: tensor(56.6271, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4321 Loss: 0.6056754291057587 Val Loss: tensor(56.4158, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4322 Loss: 0.6197793334722519 Val Loss: tensor(56.6709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4323 Loss: 0.6344046592712402 Val Loss: tensor(56.4122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4324 Loss: 0.6550041735172272 Val Loss: tensor(56.7345, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4325 Loss: 0.6795054376125336 Val Loss: tensor(56.4214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4326 Loss: 0.7077885568141937 Val Loss: tensor(56.8278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4327 Loss: 0.7470059394836426 Val Loss: tensor(56.4536, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4328 Loss: 0.7815822213888168 Val Loss: tensor(56.9390, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4329 Loss: 0.8374412357807159 Val Loss: tensor(56.5290, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4330 Loss: 0.8682336062192917 Val Loss: tensor(57.0228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4331 Loss: 0.9289369285106659 Val Loss: tensor(56.6448, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4332 Loss: 0.9354461133480072 Val Loss: tensor(57.0170, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4333 Loss: 0.9730261862277985 Val Loss: tensor(56.7504, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4334 Loss: 0.9466053247451782 Val Loss: tensor(56.9181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4335 Loss: 0.9508928656578064 Val Loss: tensor(56.8052, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4336 Loss: 0.9158633053302765 Val Loss: tensor(56.8117, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4337 Loss: 0.9181108474731445 Val Loss: tensor(56.8368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4338 Loss: 0.9081496298313141 Val Loss: tensor(56.7874, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4339 Loss: 0.9384317994117737 Val Loss: tensor(56.8772, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4340 Loss: 0.969986230134964 Val Loss: tensor(56.8848, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4341 Loss: 1.0265654921531677 Val Loss: tensor(56.9267, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4342 Loss: 1.0964913368225098 Val Loss: tensor(57.0754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4343 Loss: 1.1642641127109528 Val Loss: tensor(56.9877, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4344 Loss: 1.2548702359199524 Val Loss: tensor(57.2849, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4345 Loss: 1.3412900567054749 Val Loss: tensor(57.1088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4346 Loss: 1.445929080247879 Val Loss: tensor(57.5016, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4347 Loss: 1.5783970654010773 Val Loss: tensor(57.3541, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4348 Loss: 1.7047253251075745 Val Loss: tensor(57.7986, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4349 Loss: 1.8717292249202728 Val Loss: tensor(57.6851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4350 Loss: 1.986431509256363 Val Loss: tensor(58.1428, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4351 Loss: 2.1091017723083496 Val Loss: tensor(57.9151, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4352 Loss: 2.1287776827812195 Val Loss: tensor(58.2821, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4353 Loss: 2.1097943782806396 Val Loss: tensor(57.8306, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4354 Loss: 1.9599018096923828 Val Loss: tensor(57.9522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4355 Loss: 1.7562699615955353 Val Loss: tensor(57.5183, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4356 Loss: 1.4763773083686829 Val Loss: tensor(57.3335, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4357 Loss: 1.216183990240097 Val Loss: tensor(57.2625, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4358 Loss: 0.987145334482193 Val Loss: tensor(56.9135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4359 Loss: 0.8399599492549896 Val Loss: tensor(57.0615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4360 Loss: 0.7510461062192917 Val Loss: tensor(56.7797, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4361 Loss: 0.7105687111616135 Val Loss: tensor(56.8858, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4362 Loss: 0.6974302977323532 Val Loss: tensor(56.7394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4363 Loss: 0.6839714646339417 Val Loss: tensor(56.7868, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4364 Loss: 0.6814509928226471 Val Loss: tensor(56.6951, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4365 Loss: 0.6646597683429718 Val Loss: tensor(56.7458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4366 Loss: 0.6546325087547302 Val Loss: tensor(56.6617, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4367 Loss: 0.6354532688856125 Val Loss: tensor(56.7044, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4368 Loss: 0.6212035268545151 Val Loss: tensor(56.6520, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4369 Loss: 0.6057940721511841 Val Loss: tensor(56.6517, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4370 Loss: 0.5942641645669937 Val Loss: tensor(56.6555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4371 Loss: 0.5857516825199127 Val Loss: tensor(56.6108, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4372 Loss: 0.5802026987075806 Val Loss: tensor(56.6584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4373 Loss: 0.5763553529977798 Val Loss: tensor(56.5933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4374 Loss: 0.5755733698606491 Val Loss: tensor(56.6548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4375 Loss: 0.5729930549860001 Val Loss: tensor(56.5914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4376 Loss: 0.5738606601953506 Val Loss: tensor(56.6429, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4377 Loss: 0.5710146725177765 Val Loss: tensor(56.5990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4378 Loss: 0.571252316236496 Val Loss: tensor(56.6233, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4379 Loss: 0.5684103667736053 Val Loss: tensor(56.6086, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4380 Loss: 0.5679284483194351 Val Loss: tensor(56.6019, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4381 Loss: 0.5660776644945145 Val Loss: tensor(56.6162, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4382 Loss: 0.5656446665525436 Val Loss: tensor(56.5847, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4383 Loss: 0.5654724985361099 Val Loss: tensor(56.6193, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4384 Loss: 0.5651439279317856 Val Loss: tensor(56.5762, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4385 Loss: 0.5662996023893356 Val Loss: tensor(56.6143, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4386 Loss: 0.5653963088989258 Val Loss: tensor(56.5775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4387 Loss: 0.5667164921760559 Val Loss: tensor(56.5986, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4388 Loss: 0.5647449195384979 Val Loss: tensor(56.5851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4389 Loss: 0.5653723180294037 Val Loss: tensor(56.5757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4390 Loss: 0.5630199313163757 Val Loss: tensor(56.5935, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4391 Loss: 0.5631299763917923 Val Loss: tensor(56.5535, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4392 Loss: 0.5618469268083572 Val Loss: tensor(56.5976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4393 Loss: 0.5619666874408722 Val Loss: tensor(56.5395, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4394 Loss: 0.5624788999557495 Val Loss: tensor(56.5932, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4395 Loss: 0.5623137950897217 Val Loss: tensor(56.5368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4396 Loss: 0.5638648420572281 Val Loss: tensor(56.5793, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4397 Loss: 0.562572106719017 Val Loss: tensor(56.5421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4398 Loss: 0.5638692826032639 Val Loss: tensor(56.5593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4399 Loss: 0.5615259855985641 Val Loss: tensor(56.5483, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4400 Loss: 0.5622310489416122 Val Loss: tensor(56.5406, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4401 Loss: 0.5602298378944397 Val Loss: tensor(56.5481, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4402 Loss: 0.5607674568891525 Val Loss: tensor(56.5311, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4403 Loss: 0.5601734817028046 Val Loss: tensor(56.5376, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4404 Loss: 0.5604406297206879 Val Loss: tensor(56.5330, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4405 Loss: 0.5608704686164856 Val Loss: tensor(56.5179, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4406 Loss: 0.5603350549936295 Val Loss: tensor(56.5406, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4407 Loss: 0.5608833581209183 Val Loss: tensor(56.4968, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4408 Loss: 0.5599598288536072 Val Loss: tensor(56.5443, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4409 Loss: 0.56045301258564 Val Loss: tensor(56.4846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4410 Loss: 0.560369148850441 Val Loss: tensor(56.5365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4411 Loss: 0.5609373152256012 Val Loss: tensor(56.4869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4412 Loss: 0.5621933043003082 Val Loss: tensor(56.5171, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4413 Loss: 0.5625142753124237 Val Loss: tensor(56.4988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4414 Loss: 0.5646687895059586 Val Loss: tensor(56.4956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4415 Loss: 0.5647965967655182 Val Loss: tensor(56.5070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4416 Loss: 0.5674949437379837 Val Loss: tensor(56.4876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4417 Loss: 0.5682767182588577 Val Loss: tensor(56.4971, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4418 Loss: 0.5712437033653259 Val Loss: tensor(56.5035, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4419 Loss: 0.5732961595058441 Val Loss: tensor(56.4652, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4420 Loss: 0.5763342678546906 Val Loss: tensor(56.5399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4421 Loss: 0.5798609107732773 Val Loss: tensor(56.4238, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4422 Loss: 0.5844762325286865 Val Loss: tensor(56.5799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4423 Loss: 0.5905439406633377 Val Loss: tensor(56.3976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4424 Loss: 0.5996905863285065 Val Loss: tensor(56.6062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4425 Loss: 0.6096141338348389 Val Loss: tensor(56.4068, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4426 Loss: 0.6252583861351013 Val Loss: tensor(56.6152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4427 Loss: 0.6400886625051498 Val Loss: tensor(56.4532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4428 Loss: 0.6634879410266876 Val Loss: tensor(56.6268, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4429 Loss: 0.6864011734724045 Val Loss: tensor(56.5194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4430 Loss: 0.719890683889389 Val Loss: tensor(56.6749, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4431 Loss: 0.756925955414772 Val Loss: tensor(56.5860, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4432 Loss: 0.8023158609867096 Val Loss: tensor(56.7833, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4433 Loss: 0.8593569397926331 Val Loss: tensor(56.6530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4434 Loss: 0.9174824655056 Val Loss: tensor(56.9473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4435 Loss: 0.996396854519844 Val Loss: tensor(56.7439, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4436 Loss: 1.0670825093984604 Val Loss: tensor(57.1302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4437 Loss: 1.1577149033546448 Val Loss: tensor(56.8743, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4438 Loss: 1.2289728075265884 Val Loss: tensor(57.2714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4439 Loss: 1.2996681481599808 Val Loss: tensor(57.0098, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4440 Loss: 1.3389717936515808 Val Loss: tensor(57.3080, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4441 Loss: 1.3459001928567886 Val Loss: tensor(57.0768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4442 Loss: 1.3244123458862305 Val Loss: tensor(57.2206, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4443 Loss: 1.252707988023758 Val Loss: tensor(57.0322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4444 Loss: 1.1829432100057602 Val Loss: tensor(57.0549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4445 Loss: 1.0709593296051025 Val Loss: tensor(56.9003, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4446 Loss: 0.9914455711841583 Val Loss: tensor(56.8675, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4447 Loss: 0.8890316337347031 Val Loss: tensor(56.7450, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4448 Loss: 0.8245322555303574 Val Loss: tensor(56.6914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4449 Loss: 0.7544956207275391 Val Loss: tensor(56.6234, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4450 Loss: 0.7125073075294495 Val Loss: tensor(56.5558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4451 Loss: 0.6755339950323105 Val Loss: tensor(56.5511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4452 Loss: 0.6531963646411896 Val Loss: tensor(56.4820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4453 Loss: 0.6378829181194305 Val Loss: tensor(56.5068, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4454 Loss: 0.6260550618171692 Val Loss: tensor(56.4632, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4455 Loss: 0.6187416315078735 Val Loss: tensor(56.4648, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4456 Loss: 0.610557809472084 Val Loss: tensor(56.4745, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4457 Loss: 0.6059091836214066 Val Loss: tensor(56.4202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4458 Loss: 0.6019934415817261 Val Loss: tensor(56.4957, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4459 Loss: 0.6015419065952301 Val Loss: tensor(56.3871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4460 Loss: 0.6051184982061386 Val Loss: tensor(56.5162, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4461 Loss: 0.6077918112277985 Val Loss: tensor(56.3801, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4462 Loss: 0.6159021407365799 Val Loss: tensor(56.5242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4463 Loss: 0.6160031706094742 Val Loss: tensor(56.3992, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4464 Loss: 0.6207465976476669 Val Loss: tensor(56.5097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4465 Loss: 0.6147281378507614 Val Loss: tensor(56.4287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4466 Loss: 0.6131044775247574 Val Loss: tensor(56.4724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4467 Loss: 0.6042234748601913 Val Loss: tensor(56.4547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4468 Loss: 0.601780354976654 Val Loss: tensor(56.4273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4469 Loss: 0.5976482927799225 Val Loss: tensor(56.4736, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4470 Loss: 0.599587619304657 Val Loss: tensor(56.3972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4471 Loss: 0.6030591428279877 Val Loss: tensor(56.4810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4472 Loss: 0.6051974892616272 Val Loss: tensor(56.3905, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4473 Loss: 0.6101178824901581 Val Loss: tensor(56.4719, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4474 Loss: 0.6054348051548004 Val Loss: tensor(56.3955, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4475 Loss: 0.6053484529256821 Val Loss: tensor(56.4524, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4476 Loss: 0.5951350778341293 Val Loss: tensor(56.3946, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4477 Loss: 0.5915696620941162 Val Loss: tensor(56.4393, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4478 Loss: 0.5831594616174698 Val Loss: tensor(56.3779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4479 Loss: 0.5813390463590622 Val Loss: tensor(56.4432, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4480 Loss: 0.5778165757656097 Val Loss: tensor(56.3471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4481 Loss: 0.5775018781423569 Val Loss: tensor(56.4558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4482 Loss: 0.576030433177948 Val Loss: tensor(56.3157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4483 Loss: 0.5741455256938934 Val Loss: tensor(56.4571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4484 Loss: 0.5728307217359543 Val Loss: tensor(56.3062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4485 Loss: 0.5705289095640182 Val Loss: tensor(56.4326, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4486 Loss: 0.5700808614492416 Val Loss: tensor(56.3337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4487 Loss: 0.5700353980064392 Val Loss: tensor(56.3808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4488 Loss: 0.570699080824852 Val Loss: tensor(56.3914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4489 Loss: 0.5731336772441864 Val Loss: tensor(56.3199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4490 Loss: 0.5749928802251816 Val Loss: tensor(56.4496, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4491 Loss: 0.5802729278802872 Val Loss: tensor(56.2859, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4492 Loss: 0.5846214294433594 Val Loss: tensor(56.4719, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4493 Loss: 0.591876283288002 Val Loss: tensor(56.3084, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4494 Loss: 0.5978636890649796 Val Loss: tensor(56.4352, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4495 Loss: 0.6020059883594513 Val Loss: tensor(56.3838, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4496 Loss: 0.6060081571340561 Val Loss: tensor(56.3501, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4497 Loss: 0.6061741411685944 Val Loss: tensor(56.4706, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4498 Loss: 0.6086613237857819 Val Loss: tensor(56.2661, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4499 Loss: 0.6124269217252731 Val Loss: tensor(56.5202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4500 Loss: 0.6177674382925034 Val Loss: tensor(56.2368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4501 Loss: 0.6298536211252213 Val Loss: tensor(56.5126, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4502 Loss: 0.6377523839473724 Val Loss: tensor(56.2818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4503 Loss: 0.6545264720916748 Val Loss: tensor(56.4701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4504 Loss: 0.6629251539707184 Val Loss: tensor(56.3701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4505 Loss: 0.6810806542634964 Val Loss: tensor(56.4435, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4506 Loss: 0.6930819153785706 Val Loss: tensor(56.4369, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4507 Loss: 0.711531475186348 Val Loss: tensor(56.4695, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4508 Loss: 0.7294716089963913 Val Loss: tensor(56.4391, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4509 Loss: 0.7444404661655426 Val Loss: tensor(56.5477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4510 Loss: 0.7688018232584 Val Loss: tensor(56.3999, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4511 Loss: 0.7838900536298752 Val Loss: tensor(56.6485, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4512 Loss: 0.8177119940519333 Val Loss: tensor(56.3963, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4513 Loss: 0.8457800894975662 Val Loss: tensor(56.7312, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4514 Loss: 0.8901257067918777 Val Loss: tensor(56.4825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4515 Loss: 0.9335775524377823 Val Loss: tensor(56.7739, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4516 Loss: 0.9801138788461685 Val Loss: tensor(56.6350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4517 Loss: 1.0250339359045029 Val Loss: tensor(56.7849, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4518 Loss: 1.0575286149978638 Val Loss: tensor(56.7701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4519 Loss: 1.0846614241600037 Val Loss: tensor(56.7860, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4520 Loss: 1.0855163633823395 Val Loss: tensor(56.8039, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4521 Loss: 1.079045832157135 Val Loss: tensor(56.7787, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4522 Loss: 1.0422699004411697 Val Loss: tensor(56.7236, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4523 Loss: 1.0038528591394424 Val Loss: tensor(56.7349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4524 Loss: 0.9448279291391373 Val Loss: tensor(56.5995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4525 Loss: 0.8954306244850159 Val Loss: tensor(56.6316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4526 Loss: 0.8350839912891388 Val Loss: tensor(56.5045, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4527 Loss: 0.7917065471410751 Val Loss: tensor(56.4917, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4528 Loss: 0.7428399622440338 Val Loss: tensor(56.4479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4529 Loss: 0.7107782661914825 Val Loss: tensor(56.3723, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4530 Loss: 0.6779982596635818 Val Loss: tensor(56.4000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4531 Loss: 0.6561429798603058 Val Loss: tensor(56.3126, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4532 Loss: 0.6368671208620071 Val Loss: tensor(56.3426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4533 Loss: 0.6217440217733383 Val Loss: tensor(56.3050, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4534 Loss: 0.6105623841285706 Val Loss: tensor(56.2846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4535 Loss: 0.5997090935707092 Val Loss: tensor(56.3158, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4536 Loss: 0.5924900770187378 Val Loss: tensor(56.2397, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4537 Loss: 0.5855477005243301 Val Loss: tensor(56.3239, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4538 Loss: 0.5805829465389252 Val Loss: tensor(56.2118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4539 Loss: 0.5779392421245575 Val Loss: tensor(56.3256, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4540 Loss: 0.5746534913778305 Val Loss: tensor(56.2007, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4541 Loss: 0.5755744278430939 Val Loss: tensor(56.3245, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4542 Loss: 0.5731246918439865 Val Loss: tensor(56.2047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4543 Loss: 0.5756434202194214 Val Loss: tensor(56.3221, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4544 Loss: 0.573244646191597 Val Loss: tensor(56.2185, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4545 Loss: 0.5754317790269852 Val Loss: tensor(56.3158, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4546 Loss: 0.5728593617677689 Val Loss: tensor(56.2329, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4547 Loss: 0.5735379606485367 Val Loss: tensor(56.3021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4548 Loss: 0.5709795653820038 Val Loss: tensor(56.2390, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4549 Loss: 0.5695669800043106 Val Loss: tensor(56.2817, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4550 Loss: 0.5674896240234375 Val Loss: tensor(56.2376, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4551 Loss: 0.5641717165708542 Val Loss: tensor(56.2584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4552 Loss: 0.5633722990751266 Val Loss: tensor(56.2393, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4553 Loss: 0.5599870681762695 Val Loss: tensor(56.2359, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4554 Loss: 0.5612661987543106 Val Loss: tensor(56.2550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4555 Loss: 0.5599039644002914 Val Loss: tensor(56.2144, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4556 Loss: 0.5628166943788528 Val Loss: tensor(56.2820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4557 Loss: 0.5632764548063278 Val Loss: tensor(56.1933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4558 Loss: 0.5656979382038116 Val Loss: tensor(56.3038, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4559 Loss: 0.5661212652921677 Val Loss: tensor(56.1784, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4560 Loss: 0.566223993897438 Val Loss: tensor(56.3010, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4561 Loss: 0.5655389726161957 Val Loss: tensor(56.1810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4562 Loss: 0.5635736286640167 Val Loss: tensor(56.2656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4563 Loss: 0.561904713511467 Val Loss: tensor(56.2061, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4564 Loss: 0.5592719167470932 Val Loss: tensor(56.2078, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4565 Loss: 0.5577080398797989 Val Loss: tensor(56.2428, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4566 Loss: 0.5559565126895905 Val Loss: tensor(56.1538, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4567 Loss: 0.5562310814857483 Val Loss: tensor(56.2693, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4568 Loss: 0.5562668591737747 Val Loss: tensor(56.1307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4569 Loss: 0.5587167888879776 Val Loss: tensor(56.2671, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4570 Loss: 0.5593776404857635 Val Loss: tensor(56.1466, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4571 Loss: 0.5620457530021667 Val Loss: tensor(56.2350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4572 Loss: 0.5616247206926346 Val Loss: tensor(56.1826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4573 Loss: 0.5632347613573074 Val Loss: tensor(56.1933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4574 Loss: 0.5623505413532257 Val Loss: tensor(56.2050, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4575 Loss: 0.5635684281587601 Val Loss: tensor(56.1724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4576 Loss: 0.5635927319526672 Val Loss: tensor(56.1933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4577 Loss: 0.5646397024393082 Val Loss: tensor(56.1876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4578 Loss: 0.5657378435134888 Val Loss: tensor(56.1557, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4579 Loss: 0.5670598894357681 Val Loss: tensor(56.2250, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4580 Loss: 0.5696412473917007 Val Loss: tensor(56.1202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4581 Loss: 0.5732371509075165 Val Loss: tensor(56.2528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4582 Loss: 0.5780070573091507 Val Loss: tensor(56.1123, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4583 Loss: 0.5852076560258865 Val Loss: tensor(56.2544, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4584 Loss: 0.5924245715141296 Val Loss: tensor(56.1367, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4585 Loss: 0.6034101843833923 Val Loss: tensor(56.2448, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4586 Loss: 0.6143471747636795 Val Loss: tensor(56.1740, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4587 Loss: 0.6291211545467377 Val Loss: tensor(56.2516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4588 Loss: 0.6457111537456512 Val Loss: tensor(56.2010, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4589 Loss: 0.6645870506763458 Val Loss: tensor(56.2895, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4590 Loss: 0.6892476230859756 Val Loss: tensor(56.2215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4591 Loss: 0.7142338007688522 Val Loss: tensor(56.3515, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4592 Loss: 0.7496212124824524 Val Loss: tensor(56.2600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4593 Loss: 0.7830271422863007 Val Loss: tensor(56.4216, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4594 Loss: 0.8297700881958008 Val Loss: tensor(56.3332, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4595 Loss: 0.8714475929737091 Val Loss: tensor(56.4909, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4596 Loss: 0.9270190894603729 Val Loss: tensor(56.4325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4597 Loss: 0.9717694222927094 Val Loss: tensor(56.5555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4598 Loss: 1.0269306302070618 Val Loss: tensor(56.5273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4599 Loss: 1.0627114176750183 Val Loss: tensor(56.6013, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4600 Loss: 1.0988928973674774 Val Loss: tensor(56.5829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4601 Loss: 1.1106087267398834 Val Loss: tensor(56.6058, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4602 Loss: 1.1103330254554749 Val Loss: tensor(56.5821, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4603 Loss: 1.0902985036373138 Val Loss: tensor(56.5605, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4604 Loss: 1.0567918717861176 Val Loss: tensor(56.5356, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4605 Loss: 1.01643306016922 Val Loss: tensor(56.4868, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4606 Loss: 0.9731167256832123 Val Loss: tensor(56.4686, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4607 Loss: 0.9344576895236969 Val Loss: tensor(56.4176, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4608 Loss: 0.9001193344593048 Val Loss: tensor(56.3988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4609 Loss: 0.873482808470726 Val Loss: tensor(56.3610, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4610 Loss: 0.8512121587991714 Val Loss: tensor(56.3395, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4611 Loss: 0.8330499827861786 Val Loss: tensor(56.3031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4612 Loss: 0.8195066601037979 Val Loss: tensor(56.3061, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4613 Loss: 0.8046220391988754 Val Loss: tensor(56.2457, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4614 Loss: 0.7968519330024719 Val Loss: tensor(56.3039, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4615 Loss: 0.7828201651573181 Val Loss: tensor(56.2101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4616 Loss: 0.7762088179588318 Val Loss: tensor(56.3110, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4617 Loss: 0.7599940598011017 Val Loss: tensor(56.2030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4618 Loss: 0.7479337155818939 Val Loss: tensor(56.2871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4619 Loss: 0.7259611636400223 Val Loss: tensor(56.2076, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4620 Loss: 0.7072674930095673 Val Loss: tensor(56.2155, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4621 Loss: 0.683325931429863 Val Loss: tensor(56.2101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4622 Loss: 0.6648858189582825 Val Loss: tensor(56.1237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4623 Loss: 0.6497953236103058 Val Loss: tensor(56.2119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4624 Loss: 0.6377125829458237 Val Loss: tensor(56.0527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4625 Loss: 0.6356472373008728 Val Loss: tensor(56.2178, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4626 Loss: 0.6290706247091293 Val Loss: tensor(56.0260, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4627 Loss: 0.6329465955495834 Val Loss: tensor(56.2272, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4628 Loss: 0.6271167993545532 Val Loss: tensor(56.0368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4629 Loss: 0.6278849989175797 Val Loss: tensor(56.2221, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4630 Loss: 0.6189105808734894 Val Loss: tensor(56.0581, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4631 Loss: 0.6149842143058777 Val Loss: tensor(56.1830, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4632 Loss: 0.6049565821886063 Val Loss: tensor(56.0707, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4633 Loss: 0.6008606553077698 Val Loss: tensor(56.1220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4634 Loss: 0.596063569188118 Val Loss: tensor(56.0804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4635 Loss: 0.59218829870224 Val Loss: tensor(56.0664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4636 Loss: 0.5934712737798691 Val Loss: tensor(56.1009, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4637 Loss: 0.588732585310936 Val Loss: tensor(56.0348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4638 Loss: 0.5929553806781769 Val Loss: tensor(56.1383, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4639 Loss: 0.5898294746875763 Val Loss: tensor(56.0241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4640 Loss: 0.5947836488485336 Val Loss: tensor(56.1758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4641 Loss: 0.5929751992225647 Val Loss: tensor(56.0185, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4642 Loss: 0.5956454575061798 Val Loss: tensor(56.1828, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4643 Loss: 0.5917616039514542 Val Loss: tensor(56.0157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4644 Loss: 0.5888103097677231 Val Loss: tensor(56.1405, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4645 Loss: 0.5829884558916092 Val Loss: tensor(56.0329, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4646 Loss: 0.5764103084802628 Val Loss: tensor(56.0595, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4647 Loss: 0.5720845758914948 Val Loss: tensor(56.0827, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4648 Loss: 0.5672486275434494 Val Loss: tensor(55.9717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4649 Loss: 0.5673659592866898 Val Loss: tensor(56.1510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4650 Loss: 0.5669702738523483 Val Loss: tensor(55.9135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4651 Loss: 0.5723982900381088 Val Loss: tensor(56.2007, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4652 Loss: 0.5757748782634735 Val Loss: tensor(55.9132, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4653 Loss: 0.5834476351737976 Val Loss: tensor(56.1938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4654 Loss: 0.5861764848232269 Val Loss: tensor(55.9716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4655 Loss: 0.5897960066795349 Val Loss: tensor(56.1214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4656 Loss: 0.5871629565954208 Val Loss: tensor(56.0508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4657 Loss: 0.5852263122797012 Val Loss: tensor(56.0250, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4658 Loss: 0.5810663998126984 Val Loss: tensor(56.0988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4659 Loss: 0.5794274359941483 Val Loss: tensor(55.9693, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4660 Loss: 0.5813943147659302 Val Loss: tensor(56.0933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4661 Loss: 0.5827154666185379 Val Loss: tensor(55.9891, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4662 Loss: 0.589860811829567 Val Loss: tensor(56.0559, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4663 Loss: 0.5916173309087753 Val Loss: tensor(56.0577, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4664 Loss: 0.5979149043560028 Val Loss: tensor(56.0216, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4665 Loss: 0.5987802296876907 Val Loss: tensor(56.0987, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4666 Loss: 0.6010316908359528 Val Loss: tensor(56.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4667 Loss: 0.6005038917064667 Val Loss: tensor(56.0697, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4668 Loss: 0.5988043248653412 Val Loss: tensor(56.0154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4669 Loss: 0.5992823541164398 Val Loss: tensor(56.0198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4670 Loss: 0.5981335788965225 Val Loss: tensor(56.0227, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4671 Loss: 0.6030172407627106 Val Loss: tensor(56.0257, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4672 Loss: 0.6067291647195816 Val Loss: tensor(55.9908, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4673 Loss: 0.613023042678833 Val Loss: tensor(56.1007, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4674 Loss: 0.6194489002227783 Val Loss: tensor(55.9142, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4675 Loss: 0.6243000030517578 Val Loss: tensor(56.1927, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4676 Loss: 0.6362260729074478 Val Loss: tensor(55.8587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4677 Loss: 0.6482439339160919 Val Loss: tensor(56.2401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4678 Loss: 0.672352060675621 Val Loss: tensor(55.8989, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4679 Loss: 0.6958956569433212 Val Loss: tensor(56.2273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4680 Loss: 0.728112131357193 Val Loss: tensor(56.0297, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4681 Loss: 0.7556915432214737 Val Loss: tensor(56.2024, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4682 Loss: 0.7910238802433014 Val Loss: tensor(56.1602, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4683 Loss: 0.8153551518917084 Val Loss: tensor(56.2247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4684 Loss: 0.850996732711792 Val Loss: tensor(56.2078, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4685 Loss: 0.8654668480157852 Val Loss: tensor(56.3042, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4686 Loss: 0.8983891755342484 Val Loss: tensor(56.1897, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4687 Loss: 0.9057629704475403 Val Loss: tensor(56.3936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4688 Loss: 0.9368939101696014 Val Loss: tensor(56.1923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4689 Loss: 0.9465471804141998 Val Loss: tensor(56.4252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4690 Loss: 0.9715831726789474 Val Loss: tensor(56.2669, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4691 Loss: 0.9795921444892883 Val Loss: tensor(56.3798, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4692 Loss: 0.9892044365406036 Val Loss: tensor(56.3706, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4693 Loss: 0.9841275215148926 Val Loss: tensor(56.3006, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4694 Loss: 0.9725852906703949 Val Loss: tensor(56.4145, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4695 Loss: 0.9515630453824997 Val Loss: tensor(56.2362, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4696 Loss: 0.9210226535797119 Val Loss: tensor(56.3577, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4697 Loss: 0.8896554410457611 Val Loss: tensor(56.1914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4698 Loss: 0.8513791114091873 Val Loss: tensor(56.2467, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4699 Loss: 0.8180013746023178 Val Loss: tensor(56.1372, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4700 Loss: 0.78246009349823 Val Loss: tensor(56.1548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4701 Loss: 0.7514469027519226 Val Loss: tensor(56.0622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4702 Loss: 0.7223195880651474 Val Loss: tensor(56.1070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4703 Loss: 0.6947846561670303 Val Loss: tensor(55.9944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4704 Loss: 0.673205628991127 Val Loss: tensor(56.0723, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4705 Loss: 0.6500697284936905 Val Loss: tensor(55.9622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4706 Loss: 0.6357045620679855 Val Loss: tensor(56.0201, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4707 Loss: 0.6177528649568558 Val Loss: tensor(55.9630, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4708 Loss: 0.6085593849420547 Val Loss: tensor(55.9577, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4709 Loss: 0.5965982973575592 Val Loss: tensor(55.9789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4710 Loss: 0.589765727519989 Val Loss: tensor(55.9058, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4711 Loss: 0.5835489481687546 Val Loss: tensor(55.9994, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4712 Loss: 0.5778237283229828 Val Loss: tensor(55.8738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4713 Loss: 0.5758813768625259 Val Loss: tensor(56.0187, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4714 Loss: 0.5709730833768845 Val Loss: tensor(55.8595, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4715 Loss: 0.5710915625095367 Val Loss: tensor(56.0274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4716 Loss: 0.5661442577838898 Val Loss: tensor(55.8583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4717 Loss: 0.5660216808319092 Val Loss: tensor(56.0153, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4718 Loss: 0.5605686604976654 Val Loss: tensor(55.8703, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4719 Loss: 0.5592221170663834 Val Loss: tensor(55.9802, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4720 Loss: 0.5543878972530365 Val Loss: tensor(55.8975, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4721 Loss: 0.5526454001665115 Val Loss: tensor(55.9325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4722 Loss: 0.5501922070980072 Val Loss: tensor(55.9348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4723 Loss: 0.549449548125267 Val Loss: tensor(55.8911, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4724 Loss: 0.5504782646894455 Val Loss: tensor(55.9691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4725 Loss: 0.5514644533395767 Val Loss: tensor(55.8733, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4726 Loss: 0.5555736869573593 Val Loss: tensor(55.9870, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4727 Loss: 0.5574624091386795 Val Loss: tensor(55.8837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4728 Loss: 0.5619096010923386 Val Loss: tensor(55.9824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4729 Loss: 0.5622958391904831 Val Loss: tensor(55.9075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4730 Loss: 0.5636772215366364 Val Loss: tensor(55.9593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4731 Loss: 0.5610967129468918 Val Loss: tensor(55.9200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4732 Loss: 0.558675229549408 Val Loss: tensor(55.9309, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4733 Loss: 0.5547801852226257 Val Loss: tensor(55.9122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4734 Loss: 0.550908699631691 Val Loss: tensor(55.9099, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4735 Loss: 0.5487560927867889 Val Loss: tensor(55.9003, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4736 Loss: 0.5463513731956482 Val Loss: tensor(55.8966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4737 Loss: 0.5469759553670883 Val Loss: tensor(55.9036, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4738 Loss: 0.5465973019599915 Val Loss: tensor(55.8799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4739 Loss: 0.5479880422353745 Val Loss: tensor(55.9195, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4740 Loss: 0.5479830354452133 Val Loss: tensor(55.8580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4741 Loss: 0.5484297573566437 Val Loss: tensor(55.9281, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4742 Loss: 0.548593059182167 Val Loss: tensor(55.8479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4743 Loss: 0.5492955148220062 Val Loss: tensor(55.9152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4744 Loss: 0.5504272878170013 Val Loss: tensor(55.8618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4745 Loss: 0.5523276925086975 Val Loss: tensor(55.8902, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4746 Loss: 0.5540849566459656 Val Loss: tensor(55.8826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4747 Loss: 0.5568356066942215 Val Loss: tensor(55.8800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4748 Loss: 0.5592999011278152 Val Loss: tensor(55.8805, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4749 Loss: 0.5622910857200623 Val Loss: tensor(55.9005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4750 Loss: 0.565569594502449 Val Loss: tensor(55.8526, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4751 Loss: 0.5689124464988708 Val Loss: tensor(55.9347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4752 Loss: 0.5741572827100754 Val Loss: tensor(55.8312, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4753 Loss: 0.5799520164728165 Val Loss: tensor(55.9508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4754 Loss: 0.5881770551204681 Val Loss: tensor(55.8439, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4755 Loss: 0.5972613841295242 Val Loss: tensor(55.9465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4756 Loss: 0.6088486760854721 Val Loss: tensor(55.8804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4757 Loss: 0.6213793307542801 Val Loss: tensor(55.9514, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4758 Loss: 0.6380303204059601 Val Loss: tensor(55.9091, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4759 Loss: 0.6544636338949203 Val Loss: tensor(55.9842, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4760 Loss: 0.6781265884637833 Val Loss: tensor(55.9325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4761 Loss: 0.7004565000534058 Val Loss: tensor(56.0329, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4762 Loss: 0.7327538877725601 Val Loss: tensor(55.9795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4763 Loss: 0.7612146884202957 Val Loss: tensor(56.0786, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4764 Loss: 0.7992219924926758 Val Loss: tensor(56.0494, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4765 Loss: 0.8307239413261414 Val Loss: tensor(56.1282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4766 Loss: 0.8711481839418411 Val Loss: tensor(56.1068, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4767 Loss: 0.9002626538276672 Val Loss: tensor(56.1830, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4768 Loss: 0.9361150115728378 Val Loss: tensor(56.1274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4769 Loss: 0.9545491933822632 Val Loss: tensor(56.2123, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4770 Loss: 0.9761853814125061 Val Loss: tensor(56.1267, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4771 Loss: 0.9780827313661575 Val Loss: tensor(56.1860, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4772 Loss: 0.9788064360618591 Val Loss: tensor(56.1222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4773 Loss: 0.9610190093517303 Val Loss: tensor(56.1203, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4774 Loss: 0.9408838152885437 Val Loss: tensor(56.0988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4775 Loss: 0.9065664112567902 Val Loss: tensor(56.0551, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4776 Loss: 0.8719698488712311 Val Loss: tensor(56.0316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4777 Loss: 0.8319617211818695 Val Loss: tensor(56.0096, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4778 Loss: 0.7945775985717773 Val Loss: tensor(55.9377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4779 Loss: 0.7618145942687988 Val Loss: tensor(55.9780, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4780 Loss: 0.7326217591762543 Val Loss: tensor(55.8670, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4781 Loss: 0.7124439626932144 Val Loss: tensor(55.9322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4782 Loss: 0.6960370391607285 Val Loss: tensor(55.8628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4783 Loss: 0.68824402987957 Val Loss: tensor(55.8676, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4784 Loss: 0.6846901178359985 Val Loss: tensor(55.9228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4785 Loss: 0.6869739443063736 Val Loss: tensor(55.8184, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4786 Loss: 0.6918754726648331 Val Loss: tensor(55.9961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4787 Loss: 0.6995445340871811 Val Loss: tensor(55.8159, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4788 Loss: 0.7054339051246643 Val Loss: tensor(56.0295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4789 Loss: 0.7114441245794296 Val Loss: tensor(55.8550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4790 Loss: 0.7105789631605148 Val Loss: tensor(56.0048, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4791 Loss: 0.706550270318985 Val Loss: tensor(55.8936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4792 Loss: 0.6954174488782883 Val Loss: tensor(55.9458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4793 Loss: 0.6806884557008743 Val Loss: tensor(55.8861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4794 Loss: 0.6648415923118591 Val Loss: tensor(55.8931, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4795 Loss: 0.648292139172554 Val Loss: tensor(55.8366, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4796 Loss: 0.634681448340416 Val Loss: tensor(55.8576, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4797 Loss: 0.623289629817009 Val Loss: tensor(55.8041, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4798 Loss: 0.6141067445278168 Val Loss: tensor(55.8141, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4799 Loss: 0.6084565818309784 Val Loss: tensor(55.8296, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4800 Loss: 0.6026983112096786 Val Loss: tensor(55.7438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4801 Loss: 0.5980499088764191 Val Loss: tensor(55.8834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4802 Loss: 0.5931113511323929 Val Loss: tensor(55.6800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4803 Loss: 0.5890671461820602 Val Loss: tensor(55.9023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4804 Loss: 0.5892694294452667 Val Loss: tensor(55.6849, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4805 Loss: 0.5919442772865295 Val Loss: tensor(55.8579, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4806 Loss: 0.5978285819292068 Val Loss: tensor(55.7600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4807 Loss: 0.6030279844999313 Val Loss: tensor(55.7986, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4808 Loss: 0.6080465614795685 Val Loss: tensor(55.8217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4809 Loss: 0.6105644404888153 Val Loss: tensor(55.7938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4810 Loss: 0.6131178736686707 Val Loss: tensor(55.7997, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4811 Loss: 0.6106298714876175 Val Loss: tensor(55.8478, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4812 Loss: 0.6103784739971161 Val Loss: tensor(55.7461, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4813 Loss: 0.6067122966051102 Val Loss: tensor(55.8899, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4814 Loss: 0.6094720661640167 Val Loss: tensor(55.7515, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4815 Loss: 0.6096885502338409 Val Loss: tensor(55.8489, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4816 Loss: 0.6108362674713135 Val Loss: tensor(55.8147, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4817 Loss: 0.6101459860801697 Val Loss: tensor(55.7854, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4818 Loss: 0.6089451909065247 Val Loss: tensor(55.8478, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4819 Loss: 0.6096645593643188 Val Loss: tensor(55.7922, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4820 Loss: 0.6089927107095718 Val Loss: tensor(55.7886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4821 Loss: 0.605921283364296 Val Loss: tensor(55.8528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4822 Loss: 0.6057469695806503 Val Loss: tensor(55.7292, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4823 Loss: 0.6073359102010727 Val Loss: tensor(55.8715, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4824 Loss: 0.6169396489858627 Val Loss: tensor(55.7682, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4825 Loss: 0.6247458159923553 Val Loss: tensor(55.8143, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4826 Loss: 0.6317453980445862 Val Loss: tensor(55.8568, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4827 Loss: 0.6383522897958755 Val Loss: tensor(55.7900, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4828 Loss: 0.6440876722335815 Val Loss: tensor(55.8548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4829 Loss: 0.6471845954656601 Val Loss: tensor(55.8426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4830 Loss: 0.647507056593895 Val Loss: tensor(55.7720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4831 Loss: 0.6465082913637161 Val Loss: tensor(55.9074, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4832 Loss: 0.6559724062681198 Val Loss: tensor(55.7651, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4833 Loss: 0.6661140620708466 Val Loss: tensor(55.8766, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4834 Loss: 0.6765478551387787 Val Loss: tensor(55.8540, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4835 Loss: 0.6832154840230942 Val Loss: tensor(55.8243, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4836 Loss: 0.6883323192596436 Val Loss: tensor(55.9014, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4837 Loss: 0.6948873698711395 Val Loss: tensor(55.8516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4838 Loss: 0.6951795816421509 Val Loss: tensor(55.8134, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4839 Loss: 0.6906351298093796 Val Loss: tensor(55.9209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4840 Loss: 0.6937924176454544 Val Loss: tensor(55.7607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4841 Loss: 0.7015700191259384 Val Loss: tensor(55.9179, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4842 Loss: 0.7155229449272156 Val Loss: tensor(55.8270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4843 Loss: 0.720073938369751 Val Loss: tensor(55.8431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4844 Loss: 0.719296783208847 Val Loss: tensor(55.9012, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4845 Loss: 0.7203051149845123 Val Loss: tensor(55.8440, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4846 Loss: 0.7142226994037628 Val Loss: tensor(55.8131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4847 Loss: 0.7026656717061996 Val Loss: tensor(55.9021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4848 Loss: 0.6903610080480576 Val Loss: tensor(55.6960, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4849 Loss: 0.6863984167575836 Val Loss: tensor(55.9329, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4850 Loss: 0.6994418054819107 Val Loss: tensor(55.7208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4851 Loss: 0.7064088433980942 Val Loss: tensor(55.8506, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4852 Loss: 0.7079336196184158 Val Loss: tensor(55.8285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4853 Loss: 0.704634815454483 Val Loss: tensor(55.8160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4854 Loss: 0.6962090283632278 Val Loss: tensor(55.8038, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4855 Loss: 0.6857498586177826 Val Loss: tensor(55.8576, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4856 Loss: 0.6639028638601303 Val Loss: tensor(55.6469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4857 Loss: 0.6477113515138626 Val Loss: tensor(55.9285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4858 Loss: 0.6504605710506439 Val Loss: tensor(55.6088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4859 Loss: 0.6619791239500046 Val Loss: tensor(55.8867, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4860 Loss: 0.676894336938858 Val Loss: tensor(55.7235, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4861 Loss: 0.6792404055595398 Val Loss: tensor(55.8047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4862 Loss: 0.6737591624259949 Val Loss: tensor(55.8050, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4863 Loss: 0.6701260954141617 Val Loss: tensor(55.8205, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4864 Loss: 0.654301717877388 Val Loss: tensor(55.6741, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4865 Loss: 0.6354768723249435 Val Loss: tensor(55.9026, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4866 Loss: 0.6218462139368057 Val Loss: tensor(55.5551, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4867 Loss: 0.6256903558969498 Val Loss: tensor(55.9383, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4868 Loss: 0.6490646749734879 Val Loss: tensor(55.6210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4869 Loss: 0.6629618108272552 Val Loss: tensor(55.8302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4870 Loss: 0.6638917177915573 Val Loss: tensor(55.7718, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4871 Loss: 0.6628033369779587 Val Loss: tensor(55.8008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4872 Loss: 0.6567103117704391 Val Loss: tensor(55.7382, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4873 Loss: 0.6469279080629349 Val Loss: tensor(55.8634, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4874 Loss: 0.6254485547542572 Val Loss: tensor(55.5598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4875 Loss: 0.6129643619060516 Val Loss: tensor(55.9633, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4876 Loss: 0.6274359226226807 Val Loss: tensor(55.5413, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4877 Loss: 0.6499732732772827 Val Loss: tensor(55.8977, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4878 Loss: 0.6675460487604141 Val Loss: tensor(55.6904, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4879 Loss: 0.6699425876140594 Val Loss: tensor(55.8087, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4880 Loss: 0.6655081808567047 Val Loss: tensor(55.7818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4881 Loss: 0.6644033938646317 Val Loss: tensor(55.8392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4882 Loss: 0.648282453417778 Val Loss: tensor(55.6119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4883 Loss: 0.6249315589666367 Val Loss: tensor(55.9460, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4884 Loss: 0.6158910989761353 Val Loss: tensor(55.4911, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4885 Loss: 0.6301851570606232 Val Loss: tensor(55.9789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4886 Loss: 0.66475909948349 Val Loss: tensor(55.5815, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4887 Loss: 0.6807560622692108 Val Loss: tensor(55.8480, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4888 Loss: 0.6779817491769791 Val Loss: tensor(55.7594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4889 Loss: 0.6765448451042175 Val Loss: tensor(55.8311, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4890 Loss: 0.6703014224767685 Val Loss: tensor(55.7017, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4891 Loss: 0.6527357399463654 Val Loss: tensor(55.8959, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4892 Loss: 0.6253392249345779 Val Loss: tensor(55.4922, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4893 Loss: 0.6109261363744736 Val Loss: tensor(56.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4894 Loss: 0.6325809210538864 Val Loss: tensor(55.4753, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4895 Loss: 0.6605442464351654 Val Loss: tensor(55.9220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4896 Loss: 0.6788584887981415 Val Loss: tensor(55.6477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4897 Loss: 0.6805481165647507 Val Loss: tensor(55.8202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4898 Loss: 0.6745457202196121 Val Loss: tensor(55.7694, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4899 Loss: 0.6687854677438736 Val Loss: tensor(55.8282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4900 Loss: 0.6484899967908859 Val Loss: tensor(55.5899, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4901 Loss: 0.6174883097410202 Val Loss: tensor(55.9337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4902 Loss: 0.6007776856422424 Val Loss: tensor(55.4290, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4903 Loss: 0.6079338937997818 Val Loss: tensor(55.9838, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4904 Loss: 0.6401857435703278 Val Loss: tensor(55.4942, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4905 Loss: 0.6598469913005829 Val Loss: tensor(55.8265, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4906 Loss: 0.657081812620163 Val Loss: tensor(55.7043, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4907 Loss: 0.6539813131093979 Val Loss: tensor(55.7548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4908 Loss: 0.6490090489387512 Val Loss: tensor(55.7281, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4909 Loss: 0.6382028758525848 Val Loss: tensor(55.7873, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4910 Loss: 0.6131477355957031 Val Loss: tensor(55.5057, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4911 Loss: 0.5882342606782913 Val Loss: tensor(55.9254, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4912 Loss: 0.5899986028671265 Val Loss: tensor(55.4105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4913 Loss: 0.6097199320793152 Val Loss: tensor(55.8898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4914 Loss: 0.632227897644043 Val Loss: tensor(55.5338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4915 Loss: 0.6392092555761337 Val Loss: tensor(55.7244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4916 Loss: 0.6293598115444183 Val Loss: tensor(55.7313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4917 Loss: 0.6296139061450958 Val Loss: tensor(55.6799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4918 Loss: 0.6257333755493164 Val Loss: tensor(55.6582, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4919 Loss: 0.6116618812084198 Val Loss: tensor(55.7657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4920 Loss: 0.5910335779190063 Val Loss: tensor(55.4458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4921 Loss: 0.5811890810728073 Val Loss: tensor(55.9006, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4922 Loss: 0.599554032087326 Val Loss: tensor(55.4160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4923 Loss: 0.6210302263498306 Val Loss: tensor(55.7904, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4924 Loss: 0.6252133250236511 Val Loss: tensor(55.5791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4925 Loss: 0.6226744204759598 Val Loss: tensor(55.6542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4926 Loss: 0.6173586249351501 Val Loss: tensor(55.7206, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4927 Loss: 0.6225439608097076 Val Loss: tensor(55.6525, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4928 Loss: 0.6146920919418335 Val Loss: tensor(55.5607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4929 Loss: 0.5939825028181076 Val Loss: tensor(55.7894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4930 Loss: 0.5834615528583527 Val Loss: tensor(55.3983, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4931 Loss: 0.5929474979639053 Val Loss: tensor(55.8693, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4932 Loss: 0.6187361925840378 Val Loss: tensor(55.4291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4933 Loss: 0.6311198621988297 Val Loss: tensor(55.7121, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4934 Loss: 0.6200030744075775 Val Loss: tensor(55.6158, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4935 Loss: 0.6181226223707199 Val Loss: tensor(55.6321, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4936 Loss: 0.6201696246862411 Val Loss: tensor(55.6598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4937 Loss: 0.6200920790433884 Val Loss: tensor(55.6782, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4938 Loss: 0.6041957885026932 Val Loss: tensor(55.4563, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4939 Loss: 0.5853430777788162 Val Loss: tensor(55.8431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4940 Loss: 0.5927296429872513 Val Loss: tensor(55.3625, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4941 Loss: 0.6159590482711792 Val Loss: tensor(55.8336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4942 Loss: 0.6348862648010254 Val Loss: tensor(55.4547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4943 Loss: 0.6394101679325104 Val Loss: tensor(55.6907, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4944 Loss: 0.6324953436851501 Val Loss: tensor(55.6436, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4945 Loss: 0.6374484896659851 Val Loss: tensor(55.6598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4946 Loss: 0.6397503763437271 Val Loss: tensor(55.5924, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4947 Loss: 0.6247042864561081 Val Loss: tensor(55.7446, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4948 Loss: 0.6117650121450424 Val Loss: tensor(55.4197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4949 Loss: 0.6086809635162354 Val Loss: tensor(55.8834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4950 Loss: 0.632904589176178 Val Loss: tensor(55.4070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4951 Loss: 0.658422440290451 Val Loss: tensor(55.7955, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4952 Loss: 0.6719751954078674 Val Loss: tensor(55.5811, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4953 Loss: 0.6826606392860413 Val Loss: tensor(55.6733, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4954 Loss: 0.6985022574663162 Val Loss: tensor(55.7580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4955 Loss: 0.714377373456955 Val Loss: tensor(55.6347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4956 Loss: 0.728603258728981 Val Loss: tensor(55.6902, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4957 Loss: 0.7254317402839661 Val Loss: tensor(55.7375, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4958 Loss: 0.7389228045940399 Val Loss: tensor(55.6112, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4959 Loss: 0.7633060961961746 Val Loss: tensor(55.8343, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4960 Loss: 0.7919674515724182 Val Loss: tensor(55.6789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4961 Loss: 0.8274296373128891 Val Loss: tensor(55.7276, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4962 Loss: 0.8508310317993164 Val Loss: tensor(55.8820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4963 Loss: 0.8982793092727661 Val Loss: tensor(55.6063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4964 Loss: 0.9348147213459015 Val Loss: tensor(56.0313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4965 Loss: 1.0098882466554642 Val Loss: tensor(55.6441, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4966 Loss: 1.0555691123008728 Val Loss: tensor(56.0098, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4967 Loss: 1.1172266602516174 Val Loss: tensor(55.8714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4968 Loss: 1.120086431503296 Val Loss: tensor(55.9181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4969 Loss: 1.1321999728679657 Val Loss: tensor(56.0169, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4970 Loss: 1.0756403505802155 Val Loss: tensor(55.8403, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4971 Loss: 1.0272239744663239 Val Loss: tensor(55.8943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4972 Loss: 0.9540266394615173 Val Loss: tensor(55.8340, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4973 Loss: 0.8935575187206268 Val Loss: tensor(55.6878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4974 Loss: 0.8493612557649612 Val Loss: tensor(55.8000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4975 Loss: 0.811917319893837 Val Loss: tensor(55.6146, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4976 Loss: 0.796324297785759 Val Loss: tensor(55.6764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4977 Loss: 0.7771411091089249 Val Loss: tensor(55.6974, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4978 Loss: 0.7719933986663818 Val Loss: tensor(55.5477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4979 Loss: 0.7633802443742752 Val Loss: tensor(55.7663, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4980 Loss: 0.755996435880661 Val Loss: tensor(55.5266, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4981 Loss: 0.7441777288913727 Val Loss: tensor(55.6944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4982 Loss: 0.727872908115387 Val Loss: tensor(55.5888, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4983 Loss: 0.7133881151676178 Val Loss: tensor(55.5589, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4984 Loss: 0.6934501081705093 Val Loss: tensor(55.6322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4985 Loss: 0.6801307648420334 Val Loss: tensor(55.4777, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4986 Loss: 0.6586919873952866 Val Loss: tensor(55.6077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4987 Loss: 0.6464008688926697 Val Loss: tensor(55.4821, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4988 Loss: 0.6277751624584198 Val Loss: tensor(55.5242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4989 Loss: 0.620127946138382 Val Loss: tensor(55.5288, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4990 Loss: 0.6097045689821243 Val Loss: tensor(55.4447, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4991 Loss: 0.6060746163129807 Val Loss: tensor(55.5560, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4992 Loss: 0.5998278558254242 Val Loss: tensor(55.4298, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4993 Loss: 0.597137376666069 Val Loss: tensor(55.5245, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4994 Loss: 0.5916805416345596 Val Loss: tensor(55.4751, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4995 Loss: 0.5901145786046982 Val Loss: tensor(55.4575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4996 Loss: 0.5826593041419983 Val Loss: tensor(55.5241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4997 Loss: 0.5790822952985764 Val Loss: tensor(55.4216, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4998 Loss: 0.5701757520437241 Val Loss: tensor(55.5201, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4999 Loss: 0.5683916807174683 Val Loss: tensor(55.4590, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5000 Loss: 0.5657215118408203 Val Loss: tensor(55.4574, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5001 Loss: 0.5683166831731796 Val Loss: tensor(55.5353, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5002 Loss: 0.5685435682535172 Val Loss: tensor(55.4026, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5003 Loss: 0.568840816617012 Val Loss: tensor(55.5661, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5004 Loss: 0.5678635984659195 Val Loss: tensor(55.4268, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5005 Loss: 0.5683836340904236 Val Loss: tensor(55.5008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5006 Loss: 0.5671750754117966 Val Loss: tensor(55.5169, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5007 Loss: 0.5668008327484131 Val Loss: tensor(55.4015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5008 Loss: 0.5610579997301102 Val Loss: tensor(55.5824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5009 Loss: 0.5601080060005188 Val Loss: tensor(55.3838, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5010 Loss: 0.5590750128030777 Val Loss: tensor(55.5381, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5011 Loss: 0.5640607625246048 Val Loss: tensor(55.4717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5012 Loss: 0.5684187114238739 Val Loss: tensor(55.4326, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5013 Loss: 0.5709473937749863 Val Loss: tensor(55.5722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5014 Loss: 0.5736456662416458 Val Loss: tensor(55.4059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5015 Loss: 0.57719986140728 Val Loss: tensor(55.5482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5016 Loss: 0.5797928869724274 Val Loss: tensor(55.4935, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5017 Loss: 0.5818435549736023 Val Loss: tensor(55.4140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5018 Loss: 0.5766906440258026 Val Loss: tensor(55.6102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5019 Loss: 0.5796445459127426 Val Loss: tensor(55.3555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5020 Loss: 0.5836105793714523 Val Loss: tensor(55.6063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5021 Loss: 0.5965774953365326 Val Loss: tensor(55.4450, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5022 Loss: 0.6050833910703659 Val Loss: tensor(55.4841, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5023 Loss: 0.6093377470970154 Val Loss: tensor(55.5861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5024 Loss: 0.6137870252132416 Val Loss: tensor(55.4404, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5025 Loss: 0.6223015934228897 Val Loss: tensor(55.5836, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5026 Loss: 0.6297545582056046 Val Loss: tensor(55.5355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5027 Loss: 0.6361018717288971 Val Loss: tensor(55.4308, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5028 Loss: 0.6344926804304123 Val Loss: tensor(55.6829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5029 Loss: 0.647754579782486 Val Loss: tensor(55.3718, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5030 Loss: 0.6629853844642639 Val Loss: tensor(55.6915, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5031 Loss: 0.6895366609096527 Val Loss: tensor(55.4830, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5032 Loss: 0.7035124003887177 Val Loss: tensor(55.5720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5033 Loss: 0.7134304940700531 Val Loss: tensor(55.6464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5034 Loss: 0.7189774513244629 Val Loss: tensor(55.5471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5035 Loss: 0.7313784509897232 Val Loss: tensor(55.6238, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5036 Loss: 0.7363085895776749 Val Loss: tensor(55.6512, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5037 Loss: 0.7411852926015854 Val Loss: tensor(55.4513, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5038 Loss: 0.7379576712846756 Val Loss: tensor(55.7828, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5039 Loss: 0.7552667260169983 Val Loss: tensor(55.4032, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5040 Loss: 0.7705009132623672 Val Loss: tensor(55.7442, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5041 Loss: 0.7980988770723343 Val Loss: tensor(55.5377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5042 Loss: 0.8040695637464523 Val Loss: tensor(55.5954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5043 Loss: 0.8031534105539322 Val Loss: tensor(55.6911, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5044 Loss: 0.7862056493759155 Val Loss: tensor(55.5388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5045 Loss: 0.7683394104242325 Val Loss: tensor(55.6084, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5046 Loss: 0.7452555298805237 Val Loss: tensor(55.5970, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5047 Loss: 0.7148834019899368 Val Loss: tensor(55.3689, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5048 Loss: 0.6866702437400818 Val Loss: tensor(55.6681, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5049 Loss: 0.6703389883041382 Val Loss: tensor(55.2744, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5050 Loss: 0.6665167361497879 Val Loss: tensor(55.5830, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5051 Loss: 0.677520215511322 Val Loss: tensor(55.4122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5052 Loss: 0.6849420815706253 Val Loss: tensor(55.4000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5053 Loss: 0.6821407526731491 Val Loss: tensor(55.6023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5054 Loss: 0.671767458319664 Val Loss: tensor(55.3316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5055 Loss: 0.6581108272075653 Val Loss: tensor(55.5653, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5056 Loss: 0.6481993198394775 Val Loss: tensor(55.4278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5057 Loss: 0.6311254948377609 Val Loss: tensor(55.3165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5058 Loss: 0.612803265452385 Val Loss: tensor(55.5705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5059 Loss: 0.5994735211133957 Val Loss: tensor(55.1938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5060 Loss: 0.5965075641870499 Val Loss: tensor(55.5585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5061 Loss: 0.6114325076341629 Val Loss: tensor(55.3291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5062 Loss: 0.628919929265976 Val Loss: tensor(55.3538, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5063 Loss: 0.6340036541223526 Val Loss: tensor(55.5678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5064 Loss: 0.632534995675087 Val Loss: tensor(55.2374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5065 Loss: 0.6286002844572067 Val Loss: tensor(55.6160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5066 Loss: 0.6305270195007324 Val Loss: tensor(55.3374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5067 Loss: 0.6297461241483688 Val Loss: tensor(55.3581, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5068 Loss: 0.6180883496999741 Val Loss: tensor(55.5492, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5069 Loss: 0.6093374937772751 Val Loss: tensor(55.1659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5070 Loss: 0.6026020646095276 Val Loss: tensor(55.6355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5071 Loss: 0.619206428527832 Val Loss: tensor(55.2579, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5072 Loss: 0.6405979692935944 Val Loss: tensor(55.4215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5073 Loss: 0.6502831876277924 Val Loss: tensor(55.5208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5074 Loss: 0.6525485068559647 Val Loss: tensor(55.2323, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5075 Loss: 0.6477536708116531 Val Loss: tensor(55.6681, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5076 Loss: 0.6498399972915649 Val Loss: tensor(55.2938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5077 Loss: 0.653661921620369 Val Loss: tensor(55.4291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5078 Loss: 0.6459332704544067 Val Loss: tensor(55.5202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5079 Loss: 0.6376328319311142 Val Loss: tensor(55.1487, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5080 Loss: 0.6212928146123886 Val Loss: tensor(55.6923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5081 Loss: 0.6325632482767105 Val Loss: tensor(55.1775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5082 Loss: 0.6548237204551697 Val Loss: tensor(55.5273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5083 Loss: 0.6779664009809494 Val Loss: tensor(55.4387, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5084 Loss: 0.6905724853277206 Val Loss: tensor(55.2642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5085 Loss: 0.6831081509590149 Val Loss: tensor(55.6812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5086 Loss: 0.6778925359249115 Val Loss: tensor(55.2586, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5087 Loss: 0.6794419139623642 Val Loss: tensor(55.5400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5088 Loss: 0.677877813577652 Val Loss: tensor(55.4673, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5089 Loss: 0.6723655164241791 Val Loss: tensor(55.1800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5090 Loss: 0.6491786986589432 Val Loss: tensor(55.6951, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5091 Loss: 0.6464251726865768 Val Loss: tensor(55.0885, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5092 Loss: 0.6525408625602722 Val Loss: tensor(55.6481, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5093 Loss: 0.6823871433734894 Val Loss: tensor(55.3111, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5094 Loss: 0.7121464759111404 Val Loss: tensor(55.3624, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5095 Loss: 0.7242289483547211 Val Loss: tensor(55.6190, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5096 Loss: 0.7266484051942825 Val Loss: tensor(55.2377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5097 Loss: 0.7225935459136963 Val Loss: tensor(55.6631, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5098 Loss: 0.7108206003904343 Val Loss: tensor(55.3840, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5099 Loss: 0.6998919546604156 Val Loss: tensor(55.3368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5100 Loss: 0.6848111003637314 Val Loss: tensor(55.6169, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5101 Loss: 0.6776375621557236 Val Loss: tensor(55.0777, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5102 Loss: 0.660563200712204 Val Loss: tensor(55.6958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5103 Loss: 0.6627134531736374 Val Loss: tensor(55.1593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5104 Loss: 0.6716255694627762 Val Loss: tensor(55.5078, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5105 Loss: 0.697880208492279 Val Loss: tensor(55.4400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5106 Loss: 0.7246022820472717 Val Loss: tensor(55.2801, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5107 Loss: 0.7389033883810043 Val Loss: tensor(55.6469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5108 Loss: 0.7366849035024643 Val Loss: tensor(55.2793, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5109 Loss: 0.7261844724416733 Val Loss: tensor(55.5354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5110 Loss: 0.7044881731271744 Val Loss: tensor(55.4609, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5111 Loss: 0.6884454190731049 Val Loss: tensor(55.2385, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5112 Loss: 0.6727143377065659 Val Loss: tensor(55.6095, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5113 Loss: 0.6652005761861801 Val Loss: tensor(55.1075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5114 Loss: 0.650896891951561 Val Loss: tensor(55.5751, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5115 Loss: 0.6481667906045914 Val Loss: tensor(55.2355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5116 Loss: 0.6508694887161255 Val Loss: tensor(55.3931, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5117 Loss: 0.6679170876741409 Val Loss: tensor(55.4529, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5118 Loss: 0.6859009712934494 Val Loss: tensor(55.2526, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5119 Loss: 0.6967481374740601 Val Loss: tensor(55.5514, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5120 Loss: 0.692451074719429 Val Loss: tensor(55.2850, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5121 Loss: 0.6826623231172562 Val Loss: tensor(55.4243, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5122 Loss: 0.665139690041542 Val Loss: tensor(55.4252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5123 Loss: 0.6548615396022797 Val Loss: tensor(55.2199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5124 Loss: 0.6429338604211807 Val Loss: tensor(55.5150, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5125 Loss: 0.6355549693107605 Val Loss: tensor(55.1452, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5126 Loss: 0.6242703050374985 Val Loss: tensor(55.4691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5127 Loss: 0.621907502412796 Val Loss: tensor(55.2400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5128 Loss: 0.6245114356279373 Val Loss: tensor(55.3358, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5129 Loss: 0.6366879343986511 Val Loss: tensor(55.3874, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5130 Loss: 0.6474123150110245 Val Loss: tensor(55.2450, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5131 Loss: 0.6539201885461807 Val Loss: tensor(55.4436, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5132 Loss: 0.6501356214284897 Val Loss: tensor(55.2766, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5133 Loss: 0.6450972110033035 Val Loss: tensor(55.3453, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5134 Loss: 0.6347480714321136 Val Loss: tensor(55.3826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5135 Loss: 0.6294949054718018 Val Loss: tensor(55.1977, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5136 Loss: 0.6203246414661407 Val Loss: tensor(55.4500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5137 Loss: 0.6160731464624405 Val Loss: tensor(55.1506, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5138 Loss: 0.6099526733160019 Val Loss: tensor(55.4130, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5139 Loss: 0.6133179366588593 Val Loss: tensor(55.2314, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5140 Loss: 0.6189402639865875 Val Loss: tensor(55.3080, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5141 Loss: 0.6292767971754074 Val Loss: tensor(55.3437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5142 Loss: 0.634802058339119 Val Loss: tensor(55.2454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5143 Loss: 0.6378600150346756 Val Loss: tensor(55.3699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5144 Loss: 0.6334891021251678 Val Loss: tensor(55.2879, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5145 Loss: 0.6306708604097366 Val Loss: tensor(55.2677, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5146 Loss: 0.6230757236480713 Val Loss: tensor(55.3822, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5147 Loss: 0.6196554005146027 Val Loss: tensor(55.1465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5148 Loss: 0.6120757311582565 Val Loss: tensor(55.4295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5149 Loss: 0.6124973446130753 Val Loss: tensor(55.1362, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5150 Loss: 0.6127937883138657 Val Loss: tensor(55.3770, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5151 Loss: 0.6220276057720184 Val Loss: tensor(55.2280, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5152 Loss: 0.6287445574998856 Val Loss: tensor(55.2821, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5153 Loss: 0.6361647695302963 Val Loss: tensor(55.3167, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5154 Loss: 0.636402428150177 Val Loss: tensor(55.2576, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5155 Loss: 0.6370561569929123 Val Loss: tensor(55.2944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5156 Loss: 0.6312251091003418 Val Loss: tensor(55.3246, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5157 Loss: 0.6281253844499588 Val Loss: tensor(55.1686, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5158 Loss: 0.6197667568922043 Val Loss: tensor(55.4066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5159 Loss: 0.6183602511882782 Val Loss: tensor(55.0895, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5160 Loss: 0.6149907410144806 Val Loss: tensor(55.4115, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5161 Loss: 0.6225685030221939 Val Loss: tensor(55.1339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5162 Loss: 0.6276314854621887 Val Loss: tensor(55.3262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5163 Loss: 0.6364858597517014 Val Loss: tensor(55.2351, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5164 Loss: 0.6379885077476501 Val Loss: tensor(55.2598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5165 Loss: 0.6400970071554184 Val Loss: tensor(55.2731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5166 Loss: 0.6351901143789291 Val Loss: tensor(55.2872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5167 Loss: 0.6323667764663696 Val Loss: tensor(55.1784, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5168 Loss: 0.6233659088611603 Val Loss: tensor(55.3673, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5169 Loss: 0.6194711923599243 Val Loss: tensor(55.0648, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5170 Loss: 0.6130796074867249 Val Loss: tensor(55.4067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5171 Loss: 0.6183538883924484 Val Loss: tensor(55.0660, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5172 Loss: 0.6216064542531967 Val Loss: tensor(55.3445, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5173 Loss: 0.6302294135093689 Val Loss: tensor(55.1567, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5174 Loss: 0.6313643157482147 Val Loss: tensor(55.2610, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5175 Loss: 0.6325940042734146 Val Loss: tensor(55.2256, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5176 Loss: 0.6280161142349243 Val Loss: tensor(55.2602, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5177 Loss: 0.6250349432229996 Val Loss: tensor(55.1645, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5178 Loss: 0.6162896156311035 Val Loss: tensor(55.3273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5179 Loss: 0.6105587035417557 Val Loss: tensor(55.0421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5180 Loss: 0.6028456091880798 Val Loss: tensor(55.3812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5181 Loss: 0.6066391170024872 Val Loss: tensor(55.0185, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5182 Loss: 0.609640508890152 Val Loss: tensor(55.3375, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5183 Loss: 0.6175949573516846 Val Loss: tensor(55.0963, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5184 Loss: 0.6181790977716446 Val Loss: tensor(55.2534, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5185 Loss: 0.6175980418920517 Val Loss: tensor(55.1747, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5186 Loss: 0.6135341823101044 Val Loss: tensor(55.2445, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5187 Loss: 0.6103703379631042 Val Loss: tensor(55.1295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5188 Loss: 0.6026847660541534 Val Loss: tensor(55.3054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5189 Loss: 0.5968335270881653 Val Loss: tensor(55.0102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5190 Loss: 0.5903835296630859 Val Loss: tensor(55.3627, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5191 Loss: 0.5956698358058929 Val Loss: tensor(54.9849, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5192 Loss: 0.6003711372613907 Val Loss: tensor(55.3220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5193 Loss: 0.6071864664554596 Val Loss: tensor(55.0585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5194 Loss: 0.6063200682401657 Val Loss: tensor(55.2466, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5195 Loss: 0.6036539524793625 Val Loss: tensor(55.1301, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5196 Loss: 0.6001135557889938 Val Loss: tensor(55.2521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5197 Loss: 0.5964369475841522 Val Loss: tensor(55.0731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5198 Loss: 0.588938370347023 Val Loss: tensor(55.3191, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5199 Loss: 0.584707498550415 Val Loss: tensor(54.9656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5200 Loss: 0.5831054449081421 Val Loss: tensor(55.3672, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5201 Loss: 0.593354806303978 Val Loss: tensor(54.9645, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5202 Loss: 0.5997173190116882 Val Loss: tensor(55.3084, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5203 Loss: 0.6025184839963913 Val Loss: tensor(55.0445, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5204 Loss: 0.599092960357666 Val Loss: tensor(55.2599, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5205 Loss: 0.5960145592689514 Val Loss: tensor(55.0875, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5206 Loss: 0.5925362557172775 Val Loss: tensor(55.3012, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5207 Loss: 0.5868248790502548 Val Loss: tensor(54.9965, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5208 Loss: 0.5784628689289093 Val Loss: tensor(55.3790, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5209 Loss: 0.5817481279373169 Val Loss: tensor(54.9324, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5210 Loss: 0.5908516645431519 Val Loss: tensor(55.3849, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5211 Loss: 0.6056736707687378 Val Loss: tensor(54.9840, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5212 Loss: 0.608941912651062 Val Loss: tensor(55.3052, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5213 Loss: 0.6072079986333847 Val Loss: tensor(55.0761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5214 Loss: 0.6046686470508575 Val Loss: tensor(55.3140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5215 Loss: 0.6030812710523605 Val Loss: tensor(55.0539, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5216 Loss: 0.5922100991010666 Val Loss: tensor(55.3786, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5217 Loss: 0.5859055072069168 Val Loss: tensor(54.9658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5218 Loss: 0.5845763087272644 Val Loss: tensor(55.4111, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5219 Loss: 0.6026480495929718 Val Loss: tensor(55.0024, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5220 Loss: 0.6141379177570343 Val Loss: tensor(55.2987, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5221 Loss: 0.6161424368619919 Val Loss: tensor(55.1174, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5222 Loss: 0.6155523508787155 Val Loss: tensor(55.2355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5223 Loss: 0.6191768646240234 Val Loss: tensor(55.1530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5224 Loss: 0.618711918592453 Val Loss: tensor(55.2724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5225 Loss: 0.6083563566207886 Val Loss: tensor(55.0443, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5226 Loss: 0.5945010930299759 Val Loss: tensor(55.3054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5227 Loss: 0.605124294757843 Val Loss: tensor(55.0698, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5228 Loss: 0.6247644275426865 Val Loss: tensor(55.1846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5229 Loss: 0.6312368810176849 Val Loss: tensor(55.2020, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5230 Loss: 0.6381646245718002 Val Loss: tensor(55.0508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5231 Loss: 0.6517614722251892 Val Loss: tensor(55.2962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5232 Loss: 0.6868074089288712 Val Loss: tensor(55.1154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5233 Loss: 0.6968858987092972 Val Loss: tensor(55.1776, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5234 Loss: 0.6911928653717041 Val Loss: tensor(55.2091, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5235 Loss: 0.693632498383522 Val Loss: tensor(55.1895, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5236 Loss: 0.7214727252721786 Val Loss: tensor(55.1760, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5237 Loss: 0.7386354357004166 Val Loss: tensor(55.3448, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5238 Loss: 0.756024494767189 Val Loss: tensor(54.9954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5239 Loss: 0.76748326420784 Val Loss: tensor(55.4876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5240 Loss: 0.8208221942186356 Val Loss: tensor(55.0534, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5241 Loss: 0.8475399315357208 Val Loss: tensor(55.3972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5242 Loss: 0.8705972731113434 Val Loss: tensor(55.1725, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5243 Loss: 0.8576725423336029 Val Loss: tensor(55.2920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5244 Loss: 0.8561697155237198 Val Loss: tensor(55.2622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5245 Loss: 0.8377147912979126 Val Loss: tensor(55.3170, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5246 Loss: 0.819525882601738 Val Loss: tensor(55.1562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5247 Loss: 0.7833662778139114 Val Loss: tensor(55.2906, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5248 Loss: 0.753532350063324 Val Loss: tensor(55.0328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5249 Loss: 0.7216601073741913 Val Loss: tensor(55.2447, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5250 Loss: 0.7083151638507843 Val Loss: tensor(54.9836, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5251 Loss: 0.6988956183195114 Val Loss: tensor(55.1518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5252 Loss: 0.6953989714384079 Val Loss: tensor(55.0657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5253 Loss: 0.6919152438640594 Val Loss: tensor(55.0286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5254 Loss: 0.676974430680275 Val Loss: tensor(55.2270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5255 Loss: 0.6642771363258362 Val Loss: tensor(54.9305, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5256 Loss: 0.64070825278759 Val Loss: tensor(55.1974, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5257 Loss: 0.6263745725154877 Val Loss: tensor(54.9884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5258 Loss: 0.5998897552490234 Val Loss: tensor(54.9607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5259 Loss: 0.5813022255897522 Val Loss: tensor(55.0967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5260 Loss: 0.5651790797710419 Val Loss: tensor(54.8278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5261 Loss: 0.5607907921075821 Val Loss: tensor(55.1006, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5262 Loss: 0.5681400150060654 Val Loss: tensor(54.9252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5263 Loss: 0.5793512016534805 Val Loss: tensor(54.9552, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5264 Loss: 0.5813073962926865 Val Loss: tensor(55.1186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5265 Loss: 0.5830350965261459 Val Loss: tensor(54.8392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5266 Loss: 0.5786989033222198 Val Loss: tensor(55.1972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5267 Loss: 0.5863905847072601 Val Loss: tensor(54.8967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5268 Loss: 0.5869676917791367 Val Loss: tensor(55.0154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5269 Loss: 0.5865217447280884 Val Loss: tensor(55.0764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5270 Loss: 0.5864343196153641 Val Loss: tensor(54.8558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5271 Loss: 0.5911467671394348 Val Loss: tensor(55.2101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5272 Loss: 0.6023703068494797 Val Loss: tensor(54.9030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5273 Loss: 0.6125862151384354 Val Loss: tensor(55.0585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5274 Loss: 0.6055686622858047 Val Loss: tensor(55.0863, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5275 Loss: 0.6033158898353577 Val Loss: tensor(54.8558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5276 Loss: 0.5992429405450821 Val Loss: tensor(55.2614, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5277 Loss: 0.6191422194242477 Val Loss: tensor(54.8645, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5278 Loss: 0.6354318857192993 Val Loss: tensor(55.1432, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5279 Loss: 0.6451898515224457 Val Loss: tensor(55.0296, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5280 Loss: 0.6496742367744446 Val Loss: tensor(54.9701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5281 Loss: 0.6564872711896896 Val Loss: tensor(55.2170, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5282 Loss: 0.6636443138122559 Val Loss: tensor(54.9954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5283 Loss: 0.6734617650508881 Val Loss: tensor(55.0901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5284 Loss: 0.6580193340778351 Val Loss: tensor(55.1319, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5285 Loss: 0.6452257633209229 Val Loss: tensor(54.8200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5286 Loss: 0.6286091208457947 Val Loss: tensor(55.2928, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5287 Loss: 0.6498510986566544 Val Loss: tensor(54.8069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5288 Loss: 0.6768938302993774 Val Loss: tensor(55.2316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5289 Loss: 0.7080562263727188 Val Loss: tensor(54.9826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5290 Loss: 0.7179227471351624 Val Loss: tensor(55.0449, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5291 Loss: 0.7164861261844635 Val Loss: tensor(55.1923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5292 Loss: 0.7095605880022049 Val Loss: tensor(55.0318, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5293 Loss: 0.7118948400020599 Val Loss: tensor(55.1224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5294 Loss: 0.6984627842903137 Val Loss: tensor(55.1668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5295 Loss: 0.6769787222146988 Val Loss: tensor(54.7893, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5296 Loss: 0.6432627141475677 Val Loss: tensor(55.3096, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5297 Loss: 0.6376930624246597 Val Loss: tensor(54.6786, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5298 Loss: 0.6497683078050613 Val Loss: tensor(55.2647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5299 Loss: 0.6915587782859802 Val Loss: tensor(54.8893, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5300 Loss: 0.7242022901773453 Val Loss: tensor(55.0384, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5301 Loss: 0.7338697165250778 Val Loss: tensor(55.2048, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5302 Loss: 0.727098822593689 Val Loss: tensor(54.9247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5303 Loss: 0.7163160741329193 Val Loss: tensor(55.2624, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5304 Loss: 0.7000862210988998 Val Loss: tensor(55.0345, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5305 Loss: 0.6787582486867905 Val Loss: tensor(54.9205, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5306 Loss: 0.6555431932210922 Val Loss: tensor(55.2445, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5307 Loss: 0.6357351839542389 Val Loss: tensor(54.6460, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5308 Loss: 0.6197429001331329 Val Loss: tensor(55.3214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5309 Loss: 0.6282391995191574 Val Loss: tensor(54.7272, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5310 Loss: 0.6528991907835007 Val Loss: tensor(55.1200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5311 Loss: 0.6814906299114227 Val Loss: tensor(55.0491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5312 Loss: 0.7068172544240952 Val Loss: tensor(54.8522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5313 Loss: 0.7064384371042252 Val Loss: tensor(55.3298, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5314 Loss: 0.6966812610626221 Val Loss: tensor(54.8328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5315 Loss: 0.682161495089531 Val Loss: tensor(55.1936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5316 Loss: 0.669390007853508 Val Loss: tensor(55.0440, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5317 Loss: 0.660616084933281 Val Loss: tensor(54.8147, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5318 Loss: 0.6526787281036377 Val Loss: tensor(55.2853, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5319 Loss: 0.6515593826770782 Val Loss: tensor(54.6707, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5320 Loss: 0.646548330783844 Val Loss: tensor(55.3077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5321 Loss: 0.648735761642456 Val Loss: tensor(54.8255, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5322 Loss: 0.6565443873405457 Val Loss: tensor(55.0314, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5323 Loss: 0.6658229976892471 Val Loss: tensor(55.1104, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5324 Loss: 0.6823264807462692 Val Loss: tensor(54.7811, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5325 Loss: 0.6798507869243622 Val Loss: tensor(55.2833, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5326 Loss: 0.6688352972269058 Val Loss: tensor(54.8147, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5327 Loss: 0.6503249257802963 Val Loss: tensor(55.1089, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5328 Loss: 0.6357474476099014 Val Loss: tensor(55.0322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5329 Loss: 0.6294480562210083 Val Loss: tensor(54.7988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5330 Loss: 0.6280767470598221 Val Loss: tensor(55.2200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5331 Loss: 0.6280584633350372 Val Loss: tensor(54.6875, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5332 Loss: 0.6198531538248062 Val Loss: tensor(55.2077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5333 Loss: 0.6125211864709854 Val Loss: tensor(54.8254, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5334 Loss: 0.6088429689407349 Val Loss: tensor(54.9818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5335 Loss: 0.6129100471735001 Val Loss: tensor(55.0498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5336 Loss: 0.623494103550911 Val Loss: tensor(54.7714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5337 Loss: 0.6234054565429688 Val Loss: tensor(55.1706, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5338 Loss: 0.616661548614502 Val Loss: tensor(54.7746, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5339 Loss: 0.6025407016277313 Val Loss: tensor(55.0553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5340 Loss: 0.5919751673936844 Val Loss: tensor(54.9464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5341 Loss: 0.5872877091169357 Val Loss: tensor(54.8205, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5342 Loss: 0.5872379839420319 Val Loss: tensor(55.1161, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5343 Loss: 0.5876126438379288 Val Loss: tensor(54.6907, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5344 Loss: 0.5813475549221039 Val Loss: tensor(55.1391, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5345 Loss: 0.5757702440023422 Val Loss: tensor(54.7626, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5346 Loss: 0.5729515701532364 Val Loss: tensor(54.9808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5347 Loss: 0.576629713177681 Val Loss: tensor(54.9431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5348 Loss: 0.5845266282558441 Val Loss: tensor(54.7927, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5349 Loss: 0.5857950747013092 Val Loss: tensor(55.0785, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5350 Loss: 0.5843850374221802 Val Loss: tensor(54.7547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5351 Loss: 0.5774695873260498 Val Loss: tensor(55.0303, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5352 Loss: 0.5721440613269806 Val Loss: tensor(54.8773, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5353 Loss: 0.5689617842435837 Val Loss: tensor(54.8295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5354 Loss: 0.5669359713792801 Val Loss: tensor(55.0419, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5355 Loss: 0.5677124112844467 Val Loss: tensor(54.6869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5356 Loss: 0.565328374505043 Val Loss: tensor(55.1101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5357 Loss: 0.5662965178489685 Val Loss: tensor(54.7213, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5358 Loss: 0.5685943961143494 Val Loss: tensor(54.9903, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5359 Loss: 0.5711453706026077 Val Loss: tensor(54.8666, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5360 Loss: 0.5754059702157974 Val Loss: tensor(54.8122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5361 Loss: 0.5738904774188995 Val Loss: tensor(55.0077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5362 Loss: 0.5745847672224045 Val Loss: tensor(54.7705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5363 Loss: 0.5732106119394302 Val Loss: tensor(54.9840, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5364 Loss: 0.5701043903827667 Val Loss: tensor(54.8646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5365 Loss: 0.5650090724229813 Val Loss: tensor(54.7907, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5366 Loss: 0.5581884235143661 Val Loss: tensor(55.0137, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5367 Loss: 0.5592741072177887 Val Loss: tensor(54.6702, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5368 Loss: 0.564167782664299 Val Loss: tensor(55.0884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5369 Loss: 0.5719175189733505 Val Loss: tensor(54.6975, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5370 Loss: 0.5778162777423859 Val Loss: tensor(54.9724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5371 Loss: 0.5738489925861359 Val Loss: tensor(54.8057, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5372 Loss: 0.5735136121511459 Val Loss: tensor(54.8434, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5373 Loss: 0.5713483542203903 Val Loss: tensor(54.9202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5374 Loss: 0.5742157548666 Val Loss: tensor(54.8460, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5375 Loss: 0.5735473185777664 Val Loss: tensor(54.8615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5376 Loss: 0.5661419630050659 Val Loss: tensor(54.9143, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5377 Loss: 0.5571863353252411 Val Loss: tensor(54.7005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5378 Loss: 0.5527775436639786 Val Loss: tensor(55.0180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5379 Loss: 0.5626877695322037 Val Loss: tensor(54.6646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5380 Loss: 0.5794445127248764 Val Loss: tensor(55.0361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5381 Loss: 0.5853985548019409 Val Loss: tensor(54.6936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5382 Loss: 0.5883365571498871 Val Loss: tensor(54.9498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5383 Loss: 0.579503059387207 Val Loss: tensor(54.7550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5384 Loss: 0.5855199843645096 Val Loss: tensor(54.9813, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5385 Loss: 0.5943994522094727 Val Loss: tensor(54.8009, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5386 Loss: 0.6035075485706329 Val Loss: tensor(55.0644, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5387 Loss: 0.6076996922492981 Val Loss: tensor(54.7555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5388 Loss: 0.6037962436676025 Val Loss: tensor(55.0744, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5389 Loss: 0.6181395053863525 Val Loss: tensor(54.7889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5390 Loss: 0.637093260884285 Val Loss: tensor(55.0237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5391 Loss: 0.6632682532072067 Val Loss: tensor(54.9244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5392 Loss: 0.6901339143514633 Val Loss: tensor(54.9452, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5393 Loss: 0.7107777148485184 Val Loss: tensor(55.0417, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5394 Loss: 0.7492059022188187 Val Loss: tensor(54.9826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5395 Loss: 0.8006771057844162 Val Loss: tensor(55.1377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5396 Loss: 0.8642497658729553 Val Loss: tensor(55.1004, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5397 Loss: 0.9313982874155045 Val Loss: tensor(55.2468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5398 Loss: 0.9972260594367981 Val Loss: tensor(55.1023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5399 Loss: 1.0395908951759338 Val Loss: tensor(55.4094, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5400 Loss: 1.1222069561481476 Val Loss: tensor(54.9791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5401 Loss: 1.1310048997402191 Val Loss: tensor(55.6028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5402 Loss: 1.2416698336601257 Val Loss: tensor(54.8888, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5403 Loss: 1.2150737047195435 Val Loss: tensor(55.6928, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5404 Loss: 1.3137288391590118 Val Loss: tensor(54.9581, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5405 Loss: 1.2369334995746613 Val Loss: tensor(55.5497, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5406 Loss: 1.2947416603565216 Val Loss: tensor(55.2295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5407 Loss: 1.2046389877796173 Val Loss: tensor(55.3636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5408 Loss: 1.259654015302658 Val Loss: tensor(55.6668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5409 Loss: 1.2309856712818146 Val Loss: tensor(55.4516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5410 Loss: 1.3564614951610565 Val Loss: tensor(56.0754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5411 Loss: 1.4119172096252441 Val Loss: tensor(55.8464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5412 Loss: 1.6066884696483612 Val Loss: tensor(56.2716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5413 Loss: 1.6840879619121552 Val Loss: tensor(56.1725, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5414 Loss: 1.831063598394394 Val Loss: tensor(56.1291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5415 Loss: 1.7403502464294434 Val Loss: tensor(55.8583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5416 Loss: 1.5847748816013336 Val Loss: tensor(55.7200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5417 Loss: 1.259000152349472 Val Loss: tensor(55.3755, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5418 Loss: 0.9708325862884521 Val Loss: tensor(55.4191, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5419 Loss: 0.783199280500412 Val Loss: tensor(55.4157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5420 Loss: 0.7332684099674225 Val Loss: tensor(55.2901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5421 Loss: 0.7659923583269119 Val Loss: tensor(55.5828, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5422 Loss: 0.7933366745710373 Val Loss: tensor(55.2659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5423 Loss: 0.8154968023300171 Val Loss: tensor(55.5154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5424 Loss: 0.7694626599550247 Val Loss: tensor(55.2984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5425 Loss: 0.7291602194309235 Val Loss: tensor(55.3762, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5426 Loss: 0.667875126004219 Val Loss: tensor(55.3002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5427 Loss: 0.6304543614387512 Val Loss: tensor(55.3053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5428 Loss: 0.6121586710214615 Val Loss: tensor(55.2945, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5429 Loss: 0.6063446998596191 Val Loss: tensor(55.3184, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5430 Loss: 0.6337320506572723 Val Loss: tensor(55.3536, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5431 Loss: 0.6599738895893097 Val Loss: tensor(55.3578, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5432 Loss: 0.7162231951951981 Val Loss: tensor(55.4384, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5433 Loss: 0.7556913644075394 Val Loss: tensor(55.4292, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5434 Loss: 0.8016717731952667 Val Loss: tensor(55.5114, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5435 Loss: 0.8236521631479263 Val Loss: tensor(55.5121, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5436 Loss: 0.829242467880249 Val Loss: tensor(55.5384, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5437 Loss: 0.8193541318178177 Val Loss: tensor(55.5193, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5438 Loss: 0.7883417457342148 Val Loss: tensor(55.5174, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5439 Loss: 0.7531637996435165 Val Loss: tensor(55.4280, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5440 Loss: 0.7101046591997147 Val Loss: tensor(55.4659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5441 Loss: 0.6713624447584152 Val Loss: tensor(55.3138, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5442 Loss: 0.6449262797832489 Val Loss: tensor(55.4177, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5443 Loss: 0.6348803043365479 Val Loss: tensor(55.2691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5444 Loss: 0.6453436315059662 Val Loss: tensor(55.4213, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5445 Loss: 0.681301161646843 Val Loss: tensor(55.3279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5446 Loss: 0.728869155049324 Val Loss: tensor(55.4974, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5447 Loss: 0.7902913838624954 Val Loss: tensor(55.4491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5448 Loss: 0.8512157052755356 Val Loss: tensor(55.6067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5449 Loss: 0.8956245332956314 Val Loss: tensor(55.5633, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5450 Loss: 0.9367822557687759 Val Loss: tensor(55.6780, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5451 Loss: 0.9392238110303879 Val Loss: tensor(55.6209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5452 Loss: 0.9417837113142014 Val Loss: tensor(55.6643, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5453 Loss: 0.9088198691606522 Val Loss: tensor(55.6141, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5454 Loss: 0.8786111027002335 Val Loss: tensor(55.5884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5455 Loss: 0.8347272127866745 Val Loss: tensor(55.5668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5456 Loss: 0.7918016463518143 Val Loss: tensor(55.5070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5457 Loss: 0.7574478387832642 Val Loss: tensor(55.5063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5458 Loss: 0.7196320742368698 Val Loss: tensor(55.4529, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5459 Loss: 0.6999644339084625 Val Loss: tensor(55.4514, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5460 Loss: 0.673975870013237 Val Loss: tensor(55.4251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5461 Loss: 0.6649977266788483 Val Loss: tensor(55.4098, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5462 Loss: 0.6500388532876968 Val Loss: tensor(55.4119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5463 Loss: 0.6478923112154007 Val Loss: tensor(55.3799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5464 Loss: 0.6422751694917679 Val Loss: tensor(55.4100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5465 Loss: 0.6461096107959747 Val Loss: tensor(55.3606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5466 Loss: 0.6495144814252853 Val Loss: tensor(55.4234, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5467 Loss: 0.6600559204816818 Val Loss: tensor(55.3558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5468 Loss: 0.6730045676231384 Val Loss: tensor(55.4578, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5469 Loss: 0.6913620680570602 Val Loss: tensor(55.3698, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5470 Loss: 0.71494160592556 Val Loss: tensor(55.5198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5471 Loss: 0.7423106729984283 Val Loss: tensor(55.4063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5472 Loss: 0.7786769866943359 Val Loss: tensor(55.6173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5473 Loss: 0.8166754841804504 Val Loss: tensor(55.4695, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5474 Loss: 0.8701236844062805 Val Loss: tensor(55.7621, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5475 Loss: 0.9214220494031906 Val Loss: tensor(55.5672, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5476 Loss: 0.9996820241212845 Val Loss: tensor(55.9721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5477 Loss: 1.0688335001468658 Val Loss: tensor(55.7113, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5478 Loss: 1.1844263672828674 Val Loss: tensor(56.2734, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5479 Loss: 1.2770823240280151 Val Loss: tensor(55.9152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5480 Loss: 1.4459917545318604 Val Loss: tensor(56.6867, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5481 Loss: 1.5614064931869507 Val Loss: tensor(56.1760, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5482 Loss: 1.7873345911502838 Val Loss: tensor(57.1665, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5483 Loss: 1.8927050530910492 Val Loss: tensor(56.4258, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5484 Loss: 2.116001307964325 Val Loss: tensor(57.4778, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5485 Loss: 2.1109475195407867 Val Loss: tensor(56.5113, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5486 Loss: 2.171935945749283 Val Loss: tensor(57.2644, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5487 Loss: 1.9575841426849365 Val Loss: tensor(56.3591, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5488 Loss: 1.7774953544139862 Val Loss: tensor(56.6142, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5489 Loss: 1.4532786011695862 Val Loss: tensor(56.0558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5490 Loss: 1.1983125805854797 Val Loss: tensor(56.0361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5491 Loss: 0.9626848697662354 Val Loss: tensor(55.7414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5492 Loss: 0.8031205087900162 Val Loss: tensor(55.6619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5493 Loss: 0.6985470056533813 Val Loss: tensor(55.5797, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5494 Loss: 0.6387020200490952 Val Loss: tensor(55.4723, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5495 Loss: 0.6064823269844055 Val Loss: tensor(55.5059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5496 Loss: 0.5871439576148987 Val Loss: tensor(55.3905, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5497 Loss: 0.5799888521432877 Val Loss: tensor(55.4600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5498 Loss: 0.5677383840084076 Val Loss: tensor(55.3656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5499 Loss: 0.56324702501297 Val Loss: tensor(55.4105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5500 Loss: 0.5517287403345108 Val Loss: tensor(55.3298, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5501 Loss: 0.5468211770057678 Val Loss: tensor(55.3649, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5502 Loss: 0.538310244679451 Val Loss: tensor(55.2928, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5503 Loss: 0.5350701957941055 Val Loss: tensor(55.3251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5504 Loss: 0.5303533673286438 Val Loss: tensor(55.2709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5505 Loss: 0.5283794105052948 Val Loss: tensor(55.2907, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5506 Loss: 0.5263544470071793 Val Loss: tensor(55.2564, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5507 Loss: 0.5248374193906784 Val Loss: tensor(55.2736, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5508 Loss: 0.523922935128212 Val Loss: tensor(55.2484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5509 Loss: 0.5225052535533905 Val Loss: tensor(55.2654, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5510 Loss: 0.5219121426343918 Val Loss: tensor(55.2447, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5511 Loss: 0.520692452788353 Val Loss: tensor(55.2543, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5512 Loss: 0.5202355533838272 Val Loss: tensor(55.2387, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5513 Loss: 0.5192884355783463 Val Loss: tensor(55.2409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5514 Loss: 0.5189213156700134 Val Loss: tensor(55.2289, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5515 Loss: 0.5182218253612518 Val Loss: tensor(55.2291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5516 Loss: 0.5179238170385361 Val Loss: tensor(55.2179, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5517 Loss: 0.5174017995595932 Val Loss: tensor(55.2196, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5518 Loss: 0.5171393752098083 Val Loss: tensor(55.2083, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5519 Loss: 0.516716256737709 Val Loss: tensor(55.2101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5520 Loss: 0.5164951682090759 Val Loss: tensor(55.2002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5521 Loss: 0.5161152780056 Val Loss: tensor(55.2017, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5522 Loss: 0.5159331560134888 Val Loss: tensor(55.1924, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5523 Loss: 0.5155844241380692 Val Loss: tensor(55.1940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5524 Loss: 0.5154363363981247 Val Loss: tensor(55.1846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5525 Loss: 0.5151182115077972 Val Loss: tensor(55.1866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5526 Loss: 0.5149552226066589 Val Loss: tensor(55.1770, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5527 Loss: 0.5146808922290802 Val Loss: tensor(55.1795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5528 Loss: 0.5145087093114853 Val Loss: tensor(55.1695, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5529 Loss: 0.5142863392829895 Val Loss: tensor(55.1724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5530 Loss: 0.5141027718782425 Val Loss: tensor(55.1621, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5531 Loss: 0.5139022767543793 Val Loss: tensor(55.1655, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5532 Loss: 0.5137272328138351 Val Loss: tensor(55.1548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5533 Loss: 0.5135481208562851 Val Loss: tensor(55.1586, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5534 Loss: 0.5133730173110962 Val Loss: tensor(55.1475, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5535 Loss: 0.5132192820310593 Val Loss: tensor(55.1518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5536 Loss: 0.5130420625209808 Val Loss: tensor(55.1402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5537 Loss: 0.5129056870937347 Val Loss: tensor(55.1451, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5538 Loss: 0.5127323865890503 Val Loss: tensor(55.1328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5539 Loss: 0.5126134306192398 Val Loss: tensor(55.1386, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5540 Loss: 0.5124425739049911 Val Loss: tensor(55.1253, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5541 Loss: 0.5123451799154282 Val Loss: tensor(55.1321, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5542 Loss: 0.512165829539299 Val Loss: tensor(55.1179, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5543 Loss: 0.5120817422866821 Val Loss: tensor(55.1259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5544 Loss: 0.5119112432003021 Val Loss: tensor(55.1104, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5545 Loss: 0.511844202876091 Val Loss: tensor(55.1198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5546 Loss: 0.5116832852363586 Val Loss: tensor(55.1027, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5547 Loss: 0.5116290301084518 Val Loss: tensor(55.1138, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5548 Loss: 0.5114765763282776 Val Loss: tensor(55.0949, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5549 Loss: 0.511445090174675 Val Loss: tensor(55.1081, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5550 Loss: 0.5113007128238678 Val Loss: tensor(55.0869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5551 Loss: 0.5112897902727127 Val Loss: tensor(55.1027, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5552 Loss: 0.511164054274559 Val Loss: tensor(55.0786, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5553 Loss: 0.5111825615167618 Val Loss: tensor(55.0975, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5554 Loss: 0.5110728293657303 Val Loss: tensor(55.0700, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5555 Loss: 0.511125311255455 Val Loss: tensor(55.0929, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5556 Loss: 0.5110623091459274 Val Loss: tensor(55.0611, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5557 Loss: 0.5111692696809769 Val Loss: tensor(55.0889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5558 Loss: 0.5111682713031769 Val Loss: tensor(55.0516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5559 Loss: 0.5113431662321091 Val Loss: tensor(55.0858, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5560 Loss: 0.5114334970712662 Val Loss: tensor(55.0416, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5561 Loss: 0.5117325335741043 Val Loss: tensor(55.0837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5562 Loss: 0.511971652507782 Val Loss: tensor(55.0309, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5563 Loss: 0.512450635433197 Val Loss: tensor(55.0832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5564 Loss: 0.5129265636205673 Val Loss: tensor(55.0193, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5565 Loss: 0.5136994272470474 Val Loss: tensor(55.0848, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5566 Loss: 0.5145735591650009 Val Loss: tensor(55.0069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5567 Loss: 0.5158205032348633 Val Loss: tensor(55.0894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5568 Loss: 0.5173480361700058 Val Loss: tensor(54.9934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5569 Loss: 0.5194004476070404 Val Loss: tensor(55.0984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5570 Loss: 0.5220150351524353 Val Loss: tensor(54.9792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5571 Loss: 0.5254195630550385 Val Loss: tensor(55.1138, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5572 Loss: 0.5298588275909424 Val Loss: tensor(54.9649, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5573 Loss: 0.5355483293533325 Val Loss: tensor(55.1385, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5574 Loss: 0.5430988818407059 Val Loss: tensor(54.9519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5575 Loss: 0.5526698529720306 Val Loss: tensor(55.1771, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5576 Loss: 0.5654377341270447 Val Loss: tensor(54.9434, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5577 Loss: 0.5814734250307083 Val Loss: tensor(55.2365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5578 Loss: 0.6029413938522339 Val Loss: tensor(54.9454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5579 Loss: 0.6294710040092468 Val Loss: tensor(55.3259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5580 Loss: 0.6648745834827423 Val Loss: tensor(54.9685, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5581 Loss: 0.7075356394052505 Val Loss: tensor(55.4572, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5582 Loss: 0.763711616396904 Val Loss: tensor(55.0284, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5583 Loss: 0.8286051750183105 Val Loss: tensor(55.6399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5584 Loss: 0.9115729033946991 Val Loss: tensor(55.1431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5585 Loss: 1.0012319833040237 Val Loss: tensor(55.8711, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5586 Loss: 1.1086817532777786 Val Loss: tensor(55.3193, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5587 Loss: 1.213059052824974 Val Loss: tensor(56.1143, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5588 Loss: 1.3195166885852814 Val Loss: tensor(55.5265, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5589 Loss: 1.4043917655944824 Val Loss: tensor(56.2846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5590 Loss: 1.4498422741889954 Val Loss: tensor(55.6857, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5591 Loss: 1.4624182283878326 Val Loss: tensor(56.2851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5592 Loss: 1.3905341625213623 Val Loss: tensor(55.7197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5593 Loss: 1.314996749162674 Val Loss: tensor(56.1113, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5594 Loss: 1.1649253368377686 Val Loss: tensor(55.6288, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5595 Loss: 1.0621581375598907 Val Loss: tensor(55.8803, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5596 Loss: 0.9365372806787491 Val Loss: tensor(55.4794, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5597 Loss: 0.8686512857675552 Val Loss: tensor(55.6882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5598 Loss: 0.8026636093854904 Val Loss: tensor(55.3516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5599 Loss: 0.7731435149908066 Val Loss: tensor(55.5308, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5600 Loss: 0.7419860661029816 Val Loss: tensor(55.2788, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5601 Loss: 0.7271436154842377 Val Loss: tensor(55.3919, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5602 Loss: 0.7057044059038162 Val Loss: tensor(55.2221, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5603 Loss: 0.6918298602104187 Val Loss: tensor(55.2886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5604 Loss: 0.6720069944858551 Val Loss: tensor(55.1521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5605 Loss: 0.6580417901277542 Val Loss: tensor(55.2278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5606 Loss: 0.6417645066976547 Val Loss: tensor(55.0817, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5607 Loss: 0.6299114525318146 Val Loss: tensor(55.1912, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5608 Loss: 0.6189667135477066 Val Loss: tensor(55.0289, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5609 Loss: 0.6095000207424164 Val Loss: tensor(55.1588, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5610 Loss: 0.6025105565786362 Val Loss: tensor(54.9955, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5611 Loss: 0.5945847481489182 Val Loss: tensor(55.1302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5612 Loss: 0.5895788669586182 Val Loss: tensor(54.9750, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5613 Loss: 0.5828595906496048 Val Loss: tensor(55.1125, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5614 Loss: 0.5788506120443344 Val Loss: tensor(54.9613, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5615 Loss: 0.5734828859567642 Val Loss: tensor(55.1042, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5616 Loss: 0.5701046884059906 Val Loss: tensor(54.9510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5617 Loss: 0.5661711394786835 Val Loss: tensor(55.0986, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5618 Loss: 0.5632523745298386 Val Loss: tensor(54.9420, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5619 Loss: 0.5605482161045074 Val Loss: tensor(55.0929, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5620 Loss: 0.5580866783857346 Val Loss: tensor(54.9333, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5621 Loss: 0.5563157200813293 Val Loss: tensor(55.0870, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5622 Loss: 0.5543213039636612 Val Loss: tensor(54.9249, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5623 Loss: 0.5533187836408615 Val Loss: tensor(55.0809, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5624 Loss: 0.5517634749412537 Val Loss: tensor(54.9167, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5625 Loss: 0.5514813363552094 Val Loss: tensor(55.0746, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5626 Loss: 0.5502895712852478 Val Loss: tensor(54.9091, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5627 Loss: 0.5506725907325745 Val Loss: tensor(55.0693, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5628 Loss: 0.5497845858335495 Val Loss: tensor(54.9021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5629 Loss: 0.5508062988519669 Val Loss: tensor(55.0656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5630 Loss: 0.5501539707183838 Val Loss: tensor(54.8954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5631 Loss: 0.551734134554863 Val Loss: tensor(55.0638, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5632 Loss: 0.5513001531362534 Val Loss: tensor(54.8892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5633 Loss: 0.5533699095249176 Val Loss: tensor(55.0635, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5634 Loss: 0.5531251579523087 Val Loss: tensor(54.8838, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5635 Loss: 0.5556689649820328 Val Loss: tensor(55.0646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5636 Loss: 0.5556211918592453 Val Loss: tensor(54.8794, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5637 Loss: 0.5586189925670624 Val Loss: tensor(55.0668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5638 Loss: 0.558771014213562 Val Loss: tensor(54.8757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5639 Loss: 0.562249943614006 Val Loss: tensor(55.0704, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5640 Loss: 0.5626229643821716 Val Loss: tensor(54.8728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5641 Loss: 0.5666197389364243 Val Loss: tensor(55.0754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5642 Loss: 0.5672403126955032 Val Loss: tensor(54.8703, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5643 Loss: 0.5718209594488144 Val Loss: tensor(55.0819, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5644 Loss: 0.5727237910032272 Val Loss: tensor(54.8683, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5645 Loss: 0.5779461711645126 Val Loss: tensor(55.0901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5646 Loss: 0.5791719555854797 Val Loss: tensor(54.8670, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5647 Loss: 0.5851094275712967 Val Loss: tensor(55.1001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5648 Loss: 0.5866982191801071 Val Loss: tensor(54.8662, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5649 Loss: 0.593451663851738 Val Loss: tensor(55.1124, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5650 Loss: 0.5954267233610153 Val Loss: tensor(54.8662, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5651 Loss: 0.6030953824520111 Val Loss: tensor(55.1269, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5652 Loss: 0.6054853051900864 Val Loss: tensor(54.8669, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5653 Loss: 0.6141856014728546 Val Loss: tensor(55.1438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5654 Loss: 0.616986870765686 Val Loss: tensor(54.8687, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5655 Loss: 0.6268274188041687 Val Loss: tensor(55.1631, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5656 Loss: 0.6300171613693237 Val Loss: tensor(54.8716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5657 Loss: 0.6410388946533203 Val Loss: tensor(55.1847, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5658 Loss: 0.6444970071315765 Val Loss: tensor(54.8757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5659 Loss: 0.6566824913024902 Val Loss: tensor(55.2080, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5660 Loss: 0.6601990014314651 Val Loss: tensor(54.8814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5661 Loss: 0.6733661741018295 Val Loss: tensor(55.2322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5662 Loss: 0.6765662133693695 Val Loss: tensor(54.8886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5663 Loss: 0.690341055393219 Val Loss: tensor(55.2556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5664 Loss: 0.6926994770765305 Val Loss: tensor(54.8970, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5665 Loss: 0.7064260542392731 Val Loss: tensor(55.2762, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5666 Loss: 0.7071910202503204 Val Loss: tensor(54.9064, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5667 Loss: 0.719965010881424 Val Loss: tensor(55.2913, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5668 Loss: 0.7183271050453186 Val Loss: tensor(54.9160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5669 Loss: 0.7290996760129929 Val Loss: tensor(55.2979, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5670 Loss: 0.724252000451088 Val Loss: tensor(54.9251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5671 Loss: 0.7319525480270386 Val Loss: tensor(55.2938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5672 Loss: 0.7233851104974747 Val Loss: tensor(54.9328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5673 Loss: 0.7272686213254929 Val Loss: tensor(55.2775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5674 Loss: 0.714970588684082 Val Loss: tensor(54.9387, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5675 Loss: 0.7148738354444504 Val Loss: tensor(55.2493, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5676 Loss: 0.6995023787021637 Val Loss: tensor(54.9429, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5677 Loss: 0.6959268748760223 Val Loss: tensor(55.2110, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5678 Loss: 0.6786697506904602 Val Loss: tensor(54.9462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5679 Loss: 0.6727218478918076 Val Loss: tensor(55.1658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5680 Loss: 0.6551232486963272 Val Loss: tensor(54.9498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5681 Loss: 0.6482209265232086 Val Loss: tensor(55.1173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5682 Loss: 0.631790429353714 Val Loss: tensor(54.9549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5683 Loss: 0.6253976821899414 Val Loss: tensor(55.0689, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5684 Loss: 0.6114281862974167 Val Loss: tensor(54.9635, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5685 Loss: 0.6069051921367645 Val Loss: tensor(55.0233, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5686 Loss: 0.5963980108499527 Val Loss: tensor(54.9769, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5687 Loss: 0.5950016379356384 Val Loss: tensor(54.9827, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5688 Loss: 0.5887064188718796 Val Loss: tensor(54.9968, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5689 Loss: 0.5917296409606934 Val Loss: tensor(54.9492, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5690 Loss: 0.5902379602193832 Val Loss: tensor(55.0247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5691 Loss: 0.5991737246513367 Val Loss: tensor(54.9247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5692 Loss: 0.6029797047376633 Val Loss: tensor(55.0623, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5693 Loss: 0.6196625977754593 Val Loss: tensor(54.9115, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5694 Loss: 0.6290690451860428 Val Loss: tensor(55.1109, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5695 Loss: 0.6555099338293076 Val Loss: tensor(54.9119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5696 Loss: 0.6702128797769547 Val Loss: tensor(55.1701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5697 Loss: 0.7079330831766129 Val Loss: tensor(54.9273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5698 Loss: 0.7262797206640244 Val Loss: tensor(55.2357, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5699 Loss: 0.7745082229375839 Val Loss: tensor(54.9559, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5700 Loss: 0.7923595607280731 Val Loss: tensor(55.2962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5701 Loss: 0.845021441578865 Val Loss: tensor(54.9888, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5702 Loss: 0.8547948002815247 Val Loss: tensor(55.3315, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5703 Loss: 0.8979334980249405 Val Loss: tensor(55.0093, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5704 Loss: 0.889997199177742 Val Loss: tensor(55.3188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5705 Loss: 0.9053749740123749 Val Loss: tensor(55.0002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5706 Loss: 0.8741243928670883 Val Loss: tensor(55.2497, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5707 Loss: 0.8531648218631744 Val Loss: tensor(54.9611, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5708 Loss: 0.8051559925079346 Val Loss: tensor(55.1436, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5709 Loss: 0.7608349323272705 Val Loss: tensor(54.9120, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5710 Loss: 0.7131144851446152 Val Loss: tensor(55.0361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5711 Loss: 0.6689442694187164 Val Loss: tensor(54.8732, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5712 Loss: 0.6350846737623215 Val Loss: tensor(54.9512, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5713 Loss: 0.604522630572319 Val Loss: tensor(54.8520, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5714 Loss: 0.586082249879837 Val Loss: tensor(54.8889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5715 Loss: 0.5692069828510284 Val Loss: tensor(54.8462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5716 Loss: 0.5609576553106308 Val Loss: tensor(54.8373, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5717 Loss: 0.5528227090835571 Val Loss: tensor(54.8502, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5718 Loss: 0.5500030070543289 Val Loss: tensor(54.7909, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5719 Loss: 0.5465098768472672 Val Loss: tensor(54.8584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5720 Loss: 0.546589806675911 Val Loss: tensor(54.7527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5721 Loss: 0.5457906424999237 Val Loss: tensor(54.8672, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5722 Loss: 0.5474370867013931 Val Loss: tensor(54.7269, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5723 Loss: 0.5486249923706055 Val Loss: tensor(54.8756, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5724 Loss: 0.5509480983018875 Val Loss: tensor(54.7128, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5725 Loss: 0.5537428706884384 Val Loss: tensor(54.8850, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5726 Loss: 0.5562219470739365 Val Loss: tensor(54.7066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5727 Loss: 0.5602098256349564 Val Loss: tensor(54.8963, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5728 Loss: 0.5625689774751663 Val Loss: tensor(54.7046, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5729 Loss: 0.5673549771308899 Val Loss: tensor(54.9092, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5730 Loss: 0.5694625973701477 Val Loss: tensor(54.7059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5731 Loss: 0.5746436268091202 Val Loss: tensor(54.9223, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5732 Loss: 0.5764241516590118 Val Loss: tensor(54.7106, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5733 Loss: 0.5816607028245926 Val Loss: tensor(54.9349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5734 Loss: 0.5831511318683624 Val Loss: tensor(54.7186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5735 Loss: 0.588251605629921 Val Loss: tensor(54.9469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5736 Loss: 0.5895566493272781 Val Loss: tensor(54.7291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5737 Loss: 0.5945396572351456 Val Loss: tensor(54.9588, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5738 Loss: 0.595921590924263 Val Loss: tensor(54.7408, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5739 Loss: 0.6010905802249908 Val Loss: tensor(54.9715, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5740 Loss: 0.6029153615236282 Val Loss: tensor(54.7525, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5741 Loss: 0.6087521314620972 Val Loss: tensor(54.9863, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5742 Loss: 0.6114454567432404 Val Loss: tensor(54.7640, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5743 Loss: 0.618616133928299 Val Loss: tensor(55.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5744 Loss: 0.6226339340209961 Val Loss: tensor(54.7753, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5745 Loss: 0.6319103091955185 Val Loss: tensor(55.0307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5746 Loss: 0.6377346813678741 Val Loss: tensor(54.7873, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5747 Loss: 0.6499315500259399 Val Loss: tensor(55.0644, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5748 Loss: 0.6579543948173523 Val Loss: tensor(54.8011, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5749 Loss: 0.6739462614059448 Val Loss: tensor(55.1079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5750 Loss: 0.6844034343957901 Val Loss: tensor(54.8180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5751 Loss: 0.7049166709184647 Val Loss: tensor(55.1622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5752 Loss: 0.7176494896411896 Val Loss: tensor(54.8392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5753 Loss: 0.742979571223259 Val Loss: tensor(55.2264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5754 Loss: 0.7570710927248001 Val Loss: tensor(54.8650, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5755 Loss: 0.7865452021360397 Val Loss: tensor(55.2966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5756 Loss: 0.7999180555343628 Val Loss: tensor(54.8939, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5757 Loss: 0.8311793357133865 Val Loss: tensor(55.3648, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5758 Loss: 0.8402412384748459 Val Loss: tensor(54.9224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5759 Loss: 0.868883952498436 Val Loss: tensor(55.4184, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5760 Loss: 0.8689381033182144 Val Loss: tensor(54.9449, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5761 Loss: 0.8890500068664551 Val Loss: tensor(55.4420, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5762 Loss: 0.8757580816745758 Val Loss: tensor(54.9571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5763 Loss: 0.8821288347244263 Val Loss: tensor(55.4243, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5764 Loss: 0.8541635125875473 Val Loss: tensor(54.9570, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5765 Loss: 0.8454939126968384 Val Loss: tensor(55.3638, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5766 Loss: 0.8066198825836182 Val Loss: tensor(54.9476, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5767 Loss: 0.7872258871793747 Val Loss: tensor(55.2729, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5768 Loss: 0.7455261498689651 Val Loss: tensor(54.9349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5769 Loss: 0.723411038517952 Val Loss: tensor(55.1717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5770 Loss: 0.6874077469110489 Val Loss: tensor(54.9258, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5771 Loss: 0.6700196713209152 Val Loss: tensor(55.0776, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5772 Loss: 0.6448038816452026 Val Loss: tensor(54.9264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5773 Loss: 0.6361659020185471 Val Loss: tensor(54.9998, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5774 Loss: 0.6223627328872681 Val Loss: tensor(54.9392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5775 Loss: 0.6230820864439011 Val Loss: tensor(54.9407, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5776 Loss: 0.6184400469064713 Val Loss: tensor(54.9622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5777 Loss: 0.626931443810463 Val Loss: tensor(54.8985, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5778 Loss: 0.6282169967889786 Val Loss: tensor(54.9898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5779 Loss: 0.6417395323514938 Val Loss: tensor(54.8699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5780 Loss: 0.6457243263721466 Val Loss: tensor(55.0152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5781 Loss: 0.6608570516109467 Val Loss: tensor(54.8504, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5782 Loss: 0.6644689440727234 Val Loss: tensor(55.0317, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5783 Loss: 0.6773306429386139 Val Loss: tensor(54.8347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5784 Loss: 0.6778458654880524 Val Loss: tensor(55.0341, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5785 Loss: 0.6846756041049957 Val Loss: tensor(54.8183, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5786 Loss: 0.6802104264497757 Val Loss: tensor(55.0196, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5787 Loss: 0.6784694343805313 Val Loss: tensor(54.7994, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5788 Loss: 0.6687406897544861 Val Loss: tensor(54.9892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5789 Loss: 0.6583229303359985 Val Loss: tensor(54.7790, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5790 Loss: 0.644927978515625 Val Loss: tensor(54.9468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5791 Loss: 0.6283988207578659 Val Loss: tensor(54.7610, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5792 Loss: 0.6142315566539764 Val Loss: tensor(54.8984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5793 Loss: 0.5957324653863907 Val Loss: tensor(54.7493, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5794 Loss: 0.583562895655632 Val Loss: tensor(54.8489, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5795 Loss: 0.5668820887804031 Val Loss: tensor(54.7462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5796 Loss: 0.5584600865840912 Val Loss: tensor(54.8015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5797 Loss: 0.5459987372159958 Val Loss: tensor(54.7528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5798 Loss: 0.5418519973754883 Val Loss: tensor(54.7577, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5799 Loss: 0.5345839112997055 Val Loss: tensor(54.7687, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5800 Loss: 0.5344941914081573 Val Loss: tensor(54.7188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5801 Loss: 0.5325691848993301 Val Loss: tensor(54.7927, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5802 Loss: 0.5360708087682724 Val Loss: tensor(54.6863, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5803 Loss: 0.5392800122499466 Val Loss: tensor(54.8233, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5804 Loss: 0.5458128601312637 Val Loss: tensor(54.6618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5805 Loss: 0.5538060963153839 Val Loss: tensor(54.8589, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5806 Loss: 0.5627139359712601 Val Loss: tensor(54.6465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5807 Loss: 0.5748272091150284 Val Loss: tensor(54.8979, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5808 Loss: 0.5853137224912643 Val Loss: tensor(54.6411, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5809 Loss: 0.6003847867250443 Val Loss: tensor(54.9388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5810 Loss: 0.6113433837890625 Val Loss: tensor(54.6454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5811 Loss: 0.627565398812294 Val Loss: tensor(54.9789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5812 Loss: 0.6375884264707565 Val Loss: tensor(54.6583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5813 Loss: 0.6527773439884186 Val Loss: tensor(55.0149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5814 Loss: 0.660244956612587 Val Loss: tensor(54.6774, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5815 Loss: 0.6722525060176849 Val Loss: tensor(55.0426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5816 Loss: 0.6756797581911087 Val Loss: tensor(54.7000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5817 Loss: 0.6830623149871826 Val Loss: tensor(55.0587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5818 Loss: 0.6816331744194031 Val Loss: tensor(54.7224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5819 Loss: 0.6841587871313095 Val Loss: tensor(55.0615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5820 Loss: 0.6781006008386612 Val Loss: tensor(54.7419, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5821 Loss: 0.6766800135374069 Val Loss: tensor(55.0522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5822 Loss: 0.6672904789447784 Val Loss: tensor(54.7563, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5823 Loss: 0.6635361164808273 Val Loss: tensor(55.0337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5824 Loss: 0.6525749564170837 Val Loss: tensor(54.7653, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5825 Loss: 0.6480910927057266 Val Loss: tensor(55.0101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5826 Loss: 0.6372266262769699 Val Loss: tensor(54.7694, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5827 Loss: 0.6332002580165863 Val Loss: tensor(54.9848, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5828 Loss: 0.6235411614179611 Val Loss: tensor(54.7703, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5829 Loss: 0.6205768287181854 Val Loss: tensor(54.9601, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5830 Loss: 0.6126386970281601 Val Loss: tensor(54.7692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5831 Loss: 0.6108909100294113 Val Loss: tensor(54.9372, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5832 Loss: 0.6047288030385971 Val Loss: tensor(54.7668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5833 Loss: 0.6040374785661697 Val Loss: tensor(54.9165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5834 Loss: 0.5994545221328735 Val Loss: tensor(54.7635, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5835 Loss: 0.5995524078607559 Val Loss: tensor(54.8978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5836 Loss: 0.5962391495704651 Val Loss: tensor(54.7591, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5837 Loss: 0.5968181043863297 Val Loss: tensor(54.8808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5838 Loss: 0.5944761633872986 Val Loss: tensor(54.7536, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5839 Loss: 0.5952552855014801 Val Loss: tensor(54.8648, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5840 Loss: 0.5936201214790344 Val Loss: tensor(54.7468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5841 Loss: 0.5943636745214462 Val Loss: tensor(54.8494, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5842 Loss: 0.5931921750307083 Val Loss: tensor(54.7389, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5843 Loss: 0.5937578827142715 Val Loss: tensor(54.8343, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5844 Loss: 0.5928653180599213 Val Loss: tensor(54.7299, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5845 Loss: 0.5931122004985809 Val Loss: tensor(54.8193, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5846 Loss: 0.592329278588295 Val Loss: tensor(54.7197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5847 Loss: 0.5922001302242279 Val Loss: tensor(54.8045, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5848 Loss: 0.5913952589035034 Val Loss: tensor(54.7086, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5849 Loss: 0.5908684730529785 Val Loss: tensor(54.7898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5850 Loss: 0.5899443328380585 Val Loss: tensor(54.6966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5851 Loss: 0.5890312939882278 Val Loss: tensor(54.7752, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5852 Loss: 0.5879513770341873 Val Loss: tensor(54.6838, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5853 Loss: 0.5866987407207489 Val Loss: tensor(54.7612, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5854 Loss: 0.5854525417089462 Val Loss: tensor(54.6705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5855 Loss: 0.5839410722255707 Val Loss: tensor(54.7477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5856 Loss: 0.58254773914814 Val Loss: tensor(54.6570, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5857 Loss: 0.5808551460504532 Val Loss: tensor(54.7350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5858 Loss: 0.5793745666742325 Val Loss: tensor(54.6435, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5859 Loss: 0.5775937139987946 Val Loss: tensor(54.7233, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5860 Loss: 0.5760861933231354 Val Loss: tensor(54.6302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5861 Loss: 0.5742959380149841 Val Loss: tensor(54.7126, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5862 Loss: 0.5728420913219452 Val Loss: tensor(54.6172, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5863 Loss: 0.5711044818162918 Val Loss: tensor(54.7031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5864 Loss: 0.5697794854640961 Val Loss: tensor(54.6046, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5865 Loss: 0.5681479275226593 Val Loss: tensor(54.6950, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5866 Loss: 0.5669963359832764 Val Loss: tensor(54.5924, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5867 Loss: 0.5655348598957062 Val Loss: tensor(54.6884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5868 Loss: 0.5646289139986038 Val Loss: tensor(54.5805, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5869 Loss: 0.5634001195430756 Val Loss: tensor(54.6836, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5870 Loss: 0.5628300756216049 Val Loss: tensor(54.5687, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5871 Loss: 0.5619426369667053 Val Loss: tensor(54.6811, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5872 Loss: 0.5617986768484116 Val Loss: tensor(54.5568, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5873 Loss: 0.5614474713802338 Val Loss: tensor(54.6819, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5874 Loss: 0.5619185864925385 Val Loss: tensor(54.5447, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5875 Loss: 0.5624287575483322 Val Loss: tensor(54.6872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5876 Loss: 0.5638152360916138 Val Loss: tensor(54.5322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5877 Loss: 0.565790206193924 Val Loss: tensor(54.6992, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5878 Loss: 0.5686337798833847 Val Loss: tensor(54.5198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5879 Loss: 0.5731112062931061 Val Loss: tensor(54.7209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5880 Loss: 0.5783522725105286 Val Loss: tensor(54.5083, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5881 Loss: 0.5871324092149734 Val Loss: tensor(54.7571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5882 Loss: 0.5963636785745621 Val Loss: tensor(54.5002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5883 Loss: 0.6123968660831451 Val Loss: tensor(54.8148, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5884 Loss: 0.6282564997673035 Val Loss: tensor(54.4995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5885 Loss: 0.656289130449295 Val Loss: tensor(54.9040, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5886 Loss: 0.6828669160604477 Val Loss: tensor(54.5139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5887 Loss: 0.7298624217510223 Val Loss: tensor(55.0379, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5888 Loss: 0.7728755474090576 Val Loss: tensor(54.5554, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5889 Loss: 0.8474928736686707 Val Loss: tensor(55.2308, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5890 Loss: 0.9131365418434143 Val Loss: tensor(54.6392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5891 Loss: 1.0219661593437195 Val Loss: tensor(55.4883, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5892 Loss: 1.1123349517583847 Val Loss: tensor(54.7761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5893 Loss: 1.2487716972827911 Val Loss: tensor(55.7850, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5894 Loss: 1.3506385385990143 Val Loss: tensor(54.9553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5895 Loss: 1.4742359817028046 Val Loss: tensor(56.0265, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5896 Loss: 1.5421985685825348 Val Loss: tensor(55.1314, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5897 Loss: 1.571780264377594 Val Loss: tensor(56.0486, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5898 Loss: 1.533205270767212 Val Loss: tensor(55.2493, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5899 Loss: 1.4215823709964752 Val Loss: tensor(55.7779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5900 Loss: 1.2667252123355865 Val Loss: tensor(55.2768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5901 Loss: 1.1037658751010895 Val Loss: tensor(55.4339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5902 Loss: 0.9485199451446533 Val Loss: tensor(55.1961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5903 Loss: 0.8431772142648697 Val Loss: tensor(55.2573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5904 Loss: 0.7608245015144348 Val Loss: tensor(55.0457, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5905 Loss: 0.7198545932769775 Val Loss: tensor(55.1806, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5906 Loss: 0.6838514357805252 Val Loss: tensor(54.9426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5907 Loss: 0.6659026443958282 Val Loss: tensor(55.0691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5908 Loss: 0.6401655226945877 Val Loss: tensor(54.9123, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5909 Loss: 0.6210294961929321 Val Loss: tensor(54.9243, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5910 Loss: 0.596662774682045 Val Loss: tensor(54.8747, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5911 Loss: 0.5788048803806305 Val Loss: tensor(54.8173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5912 Loss: 0.5592733472585678 Val Loss: tensor(54.8026, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5913 Loss: 0.5464044064283371 Val Loss: tensor(54.7648, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5914 Loss: 0.5330993235111237 Val Loss: tensor(54.7354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5915 Loss: 0.5254903137683868 Val Loss: tensor(54.7412, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5916 Loss: 0.517668753862381 Val Loss: tensor(54.6938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5917 Loss: 0.5142579823732376 Val Loss: tensor(54.7221, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5918 Loss: 0.510303720831871 Val Loss: tensor(54.6807, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5919 Loss: 0.5090723931789398 Val Loss: tensor(54.7050, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5920 Loss: 0.50727878510952 Val Loss: tensor(54.6819, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5921 Loss: 0.5065915584564209 Val Loss: tensor(54.6938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5922 Loss: 0.505867063999176 Val Loss: tensor(54.6812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5923 Loss: 0.505185067653656 Val Loss: tensor(54.6857, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5924 Loss: 0.5049788355827332 Val Loss: tensor(54.6749, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5925 Loss: 0.5044281333684921 Val Loss: tensor(54.6782, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5926 Loss: 0.5043376833200455 Val Loss: tensor(54.6662, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5927 Loss: 0.504069060087204 Val Loss: tensor(54.6722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5928 Loss: 0.5038596093654633 Val Loss: tensor(54.6578, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5929 Loss: 0.5038660317659378 Val Loss: tensor(54.6679, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5930 Loss: 0.5034956336021423 Val Loss: tensor(54.6496, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5931 Loss: 0.5037025064229965 Val Loss: tensor(54.6645, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5932 Loss: 0.5031840205192566 Val Loss: tensor(54.6418, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5933 Loss: 0.5035391300916672 Val Loss: tensor(54.6603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5934 Loss: 0.5029330253601074 Val Loss: tensor(54.6363, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5935 Loss: 0.5033846795558929 Val Loss: tensor(54.6542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5936 Loss: 0.5027540326118469 Val Loss: tensor(54.6334, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5937 Loss: 0.5032833814620972 Val Loss: tensor(54.6462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5938 Loss: 0.5027062445878983 Val Loss: tensor(54.6329, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5939 Loss: 0.5033194720745087 Val Loss: tensor(54.6370, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5940 Loss: 0.5029136538505554 Val Loss: tensor(54.6345, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5941 Loss: 0.503635436296463 Val Loss: tensor(54.6270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5942 Loss: 0.5035466402769089 Val Loss: tensor(54.6380, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5943 Loss: 0.5044352561235428 Val Loss: tensor(54.6164, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5944 Loss: 0.5047886818647385 Val Loss: tensor(54.6433, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5945 Loss: 0.5059187114238739 Val Loss: tensor(54.6053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5946 Loss: 0.5068387389183044 Val Loss: tensor(54.6504, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5947 Loss: 0.5082996934652328 Val Loss: tensor(54.5938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5948 Loss: 0.509878620505333 Val Loss: tensor(54.6593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5949 Loss: 0.5117330402135849 Val Loss: tensor(54.5823, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5950 Loss: 0.5140043944120407 Val Loss: tensor(54.6697, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5951 Loss: 0.5162584632635117 Val Loss: tensor(54.5709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5952 Loss: 0.5191892236471176 Val Loss: tensor(54.6813, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5953 Loss: 0.5218044072389603 Val Loss: tensor(54.5601, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5954 Loss: 0.5252551883459091 Val Loss: tensor(54.6938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5955 Loss: 0.5281109064817429 Val Loss: tensor(54.5501, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5956 Loss: 0.5318844318389893 Val Loss: tensor(54.7069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5957 Loss: 0.534762904047966 Val Loss: tensor(54.5414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5958 Loss: 0.5385729372501373 Val Loss: tensor(54.7198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5959 Loss: 0.5412304848432541 Val Loss: tensor(54.5348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5960 Loss: 0.5447363406419754 Val Loss: tensor(54.7318, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5961 Loss: 0.5468942672014236 Val Loss: tensor(54.5313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5962 Loss: 0.5497771948575974 Val Loss: tensor(54.7418, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5963 Loss: 0.5511783063411713 Val Loss: tensor(54.5320, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5964 Loss: 0.5530741363763809 Val Loss: tensor(54.7479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5965 Loss: 0.5535308569669724 Val Loss: tensor(54.5383, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5966 Loss: 0.5541951805353165 Val Loss: tensor(54.7482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5967 Loss: 0.5536648482084274 Val Loss: tensor(54.5506, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5968 Loss: 0.5529877692461014 Val Loss: tensor(54.7403, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5969 Loss: 0.551628977060318 Val Loss: tensor(54.5688, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5970 Loss: 0.5498601049184799 Val Loss: tensor(54.7227, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5971 Loss: 0.5481175631284714 Val Loss: tensor(54.5918, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5972 Loss: 0.5460060089826584 Val Loss: tensor(54.6952, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5973 Loss: 0.5447109788656235 Val Loss: tensor(54.6179, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5974 Loss: 0.5435820370912552 Val Loss: tensor(54.6599, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5975 Loss: 0.5440217554569244 Val Loss: tensor(54.6459, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5976 Loss: 0.5457499623298645 Val Loss: tensor(54.6211, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5977 Loss: 0.5495719313621521 Val Loss: tensor(54.6763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5978 Loss: 0.5563162714242935 Val Loss: tensor(54.5844, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5979 Loss: 0.56533482670784 Val Loss: tensor(54.7118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5980 Loss: 0.5791888386011124 Val Loss: tensor(54.5564, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5981 Loss: 0.5950400978326797 Val Loss: tensor(54.7567, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5982 Loss: 0.6176502108573914 Val Loss: tensor(54.5433, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5983 Loss: 0.6412143856287003 Val Loss: tensor(54.8154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5984 Loss: 0.6732119172811508 Val Loss: tensor(54.5515, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5985 Loss: 0.7037366330623627 Val Loss: tensor(54.8901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5986 Loss: 0.7436556071043015 Val Loss: tensor(54.5860, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5987 Loss: 0.7771584838628769 Val Loss: tensor(54.9758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5988 Loss: 0.8197417706251144 Val Loss: tensor(54.6477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5989 Loss: 0.8476324528455734 Val Loss: tensor(55.0559, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5990 Loss: 0.8823840618133545 Val Loss: tensor(54.7279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5991 Loss: 0.8928211629390717 Val Loss: tensor(55.1024, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5992 Loss: 0.9062596559524536 Val Loss: tensor(54.8060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5993 Loss: 0.8912919759750366 Val Loss: tensor(55.0920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5994 Loss: 0.8760252892971039 Val Loss: tensor(54.8560, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5995 Loss: 0.8402864336967468 Val Loss: tensor(55.0286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5996 Loss: 0.8037521094083786 Val Loss: tensor(54.8618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5997 Loss: 0.7619735598564148 Val Loss: tensor(54.9426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5998 Loss: 0.7208293825387955 Val Loss: tensor(54.8276, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5999 Loss: 0.685810312628746 Val Loss: tensor(54.8629, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6000 Loss: 0.652750551700592 Val Loss: tensor(54.7748, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6001 Loss: 0.6293594837188721 Val Loss: tensor(54.8000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6002 Loss: 0.6087256222963333 Val Loss: tensor(54.7288, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6003 Loss: 0.5962509512901306 Val Loss: tensor(54.7518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6004 Loss: 0.5869545489549637 Val Loss: tensor(54.7022, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6005 Loss: 0.5826259702444077 Val Loss: tensor(54.7147, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6006 Loss: 0.5815755426883698 Val Loss: tensor(54.6917, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6007 Loss: 0.5824925452470779 Val Loss: tensor(54.6885, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6008 Loss: 0.586545005440712 Val Loss: tensor(54.6878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6009 Loss: 0.5902252644300461 Val Loss: tensor(54.6755, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6010 Loss: 0.596942663192749 Val Loss: tensor(54.6820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6011 Loss: 0.6016311049461365 Val Loss: tensor(54.6767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6012 Loss: 0.6096247583627701 Val Loss: tensor(54.6704, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6013 Loss: 0.6149671971797943 Val Loss: tensor(54.6920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6014 Loss: 0.624239981174469 Val Loss: tensor(54.6543, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6015 Loss: 0.6318836212158203 Val Loss: tensor(54.7231, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6016 Loss: 0.643879845738411 Val Loss: tensor(54.6392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6017 Loss: 0.657650500535965 Val Loss: tensor(54.7755, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6018 Loss: 0.6748993545770645 Val Loss: tensor(54.6328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6019 Loss: 0.7003678381443024 Val Loss: tensor(54.8580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6020 Loss: 0.7257226854562759 Val Loss: tensor(54.6419, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6021 Loss: 0.7689058482646942 Val Loss: tensor(54.9795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6022 Loss: 0.8040377050638199 Val Loss: tensor(54.6711, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6023 Loss: 0.868612602353096 Val Loss: tensor(55.1427, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6024 Loss: 0.9115392118692398 Val Loss: tensor(54.7190, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6025 Loss: 0.994539275765419 Val Loss: tensor(55.3374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6026 Loss: 1.0367875844240189 Val Loss: tensor(54.7754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6027 Loss: 1.1238476634025574 Val Loss: tensor(55.5323, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6028 Loss: 1.1493603587150574 Val Loss: tensor(54.8250, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6029 Loss: 1.2134247422218323 Val Loss: tensor(55.6713, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6030 Loss: 1.2024465203285217 Val Loss: tensor(54.8611, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6031 Loss: 1.213140845298767 Val Loss: tensor(55.6847, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6032 Loss: 1.1536107659339905 Val Loss: tensor(54.8861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6033 Loss: 1.1043906211853027 Val Loss: tensor(55.5449, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6034 Loss: 1.0108778476715088 Val Loss: tensor(54.8921, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6035 Loss: 0.9401809275150299 Val Loss: tensor(55.3304, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6036 Loss: 0.8540694713592529 Val Loss: tensor(54.8716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6037 Loss: 0.8089461028575897 Val Loss: tensor(55.1509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6038 Loss: 0.7558394074440002 Val Loss: tensor(54.8428, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6039 Loss: 0.7438605278730392 Val Loss: tensor(55.0284, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6040 Loss: 0.7138031125068665 Val Loss: tensor(54.8212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6041 Loss: 0.7154112309217453 Val Loss: tensor(54.9384, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6042 Loss: 0.6916351020336151 Val Loss: tensor(54.7963, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6043 Loss: 0.6926807314157486 Val Loss: tensor(54.8737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6044 Loss: 0.6710701584815979 Val Loss: tensor(54.7553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6045 Loss: 0.6689974963665009 Val Loss: tensor(54.8365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6046 Loss: 0.6520186513662338 Val Loss: tensor(54.7058, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6047 Loss: 0.6484477370977402 Val Loss: tensor(54.8210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6048 Loss: 0.6374419927597046 Val Loss: tensor(54.6657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6049 Loss: 0.6344765573740005 Val Loss: tensor(54.8171, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6050 Loss: 0.6288216412067413 Val Loss: tensor(54.6434, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6051 Loss: 0.6278129518032074 Val Loss: tensor(54.8201, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6052 Loss: 0.626345157623291 Val Loss: tensor(54.6350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6053 Loss: 0.6276586949825287 Val Loss: tensor(54.8317, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6054 Loss: 0.6293206214904785 Val Loss: tensor(54.6299, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6055 Loss: 0.6330750584602356 Val Loss: tensor(54.8531, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6056 Loss: 0.6368026435375214 Val Loss: tensor(54.6194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6057 Loss: 0.6430957615375519 Val Loss: tensor(54.8809, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6058 Loss: 0.6475813239812851 Val Loss: tensor(54.6040, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6059 Loss: 0.6561375260353088 Val Loss: tensor(54.9103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6060 Loss: 0.6600803136825562 Val Loss: tensor(54.5887, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6061 Loss: 0.6700569689273834 Val Loss: tensor(54.9368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6062 Loss: 0.67242032289505 Val Loss: tensor(54.5783, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6063 Loss: 0.6825224608182907 Val Loss: tensor(54.9561, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6064 Loss: 0.6825572550296783 Val Loss: tensor(54.5739, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6065 Loss: 0.691360205411911 Val Loss: tensor(54.9655, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6066 Loss: 0.6886596083641052 Val Loss: tensor(54.5746, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6067 Loss: 0.6952187865972519 Val Loss: tensor(54.9644, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6068 Loss: 0.6896682530641556 Val Loss: tensor(54.5778, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6069 Loss: 0.6936947703361511 Val Loss: tensor(54.9541, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6070 Loss: 0.6856438368558884 Val Loss: tensor(54.5810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6071 Loss: 0.6874678283929825 Val Loss: tensor(54.9378, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6072 Loss: 0.677619680762291 Val Loss: tensor(54.5824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6073 Loss: 0.6778437048196793 Val Loss: tensor(54.9188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6074 Loss: 0.6670548766851425 Val Loss: tensor(54.5816, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6075 Loss: 0.6662957668304443 Val Loss: tensor(54.8995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6076 Loss: 0.6552926450967789 Val Loss: tensor(54.5791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6077 Loss: 0.6540566384792328 Val Loss: tensor(54.8810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6078 Loss: 0.643370509147644 Val Loss: tensor(54.5758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6079 Loss: 0.6419757008552551 Val Loss: tensor(54.8638, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6080 Loss: 0.6319359242916107 Val Loss: tensor(54.5721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6081 Loss: 0.63060262799263 Val Loss: tensor(54.8484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6082 Loss: 0.6213521510362625 Val Loss: tensor(54.5687, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6083 Loss: 0.6201950162649155 Val Loss: tensor(54.8348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6084 Loss: 0.6117875277996063 Val Loss: tensor(54.5656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6085 Loss: 0.6109132170677185 Val Loss: tensor(54.8232, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6086 Loss: 0.6033124774694443 Val Loss: tensor(54.5627, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6087 Loss: 0.6027547121047974 Val Loss: tensor(54.8133, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6088 Loss: 0.5959150642156601 Val Loss: tensor(54.5603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6089 Loss: 0.5956921726465225 Val Loss: tensor(54.8051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6090 Loss: 0.5895092338323593 Val Loss: tensor(54.5583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6091 Loss: 0.5896027386188507 Val Loss: tensor(54.7981, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6092 Loss: 0.5839947164058685 Val Loss: tensor(54.5567, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6093 Loss: 0.5843439698219299 Val Loss: tensor(54.7921, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6094 Loss: 0.5792021602392197 Val Loss: tensor(54.5556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6095 Loss: 0.5797682404518127 Val Loss: tensor(54.7866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6096 Loss: 0.5749752968549728 Val Loss: tensor(54.5548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6097 Loss: 0.5756868571043015 Val Loss: tensor(54.7813, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6098 Loss: 0.5711310058832169 Val Loss: tensor(54.5545, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6099 Loss: 0.5719141662120819 Val Loss: tensor(54.7758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6100 Loss: 0.567486122250557 Val Loss: tensor(54.5545, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6101 Loss: 0.5682911425828934 Val Loss: tensor(54.7699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6102 Loss: 0.5638931393623352 Val Loss: tensor(54.5549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6103 Loss: 0.5646572113037109 Val Loss: tensor(54.7631, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6104 Loss: 0.5601892173290253 Val Loss: tensor(54.5559, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6105 Loss: 0.560855895280838 Val Loss: tensor(54.7552, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6106 Loss: 0.5562871396541595 Val Loss: tensor(54.5575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6107 Loss: 0.5568207204341888 Val Loss: tensor(54.7458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6108 Loss: 0.5521036833524704 Val Loss: tensor(54.5601, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6109 Loss: 0.5525023192167282 Val Loss: tensor(54.7347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6110 Loss: 0.5476237386465073 Val Loss: tensor(54.5639, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6111 Loss: 0.5479099601507187 Val Loss: tensor(54.7217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6112 Loss: 0.5429413169622421 Val Loss: tensor(54.5694, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6113 Loss: 0.5431754887104034 Val Loss: tensor(54.7066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6114 Loss: 0.5382347106933594 Val Loss: tensor(54.5771, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6115 Loss: 0.5385610312223434 Val Loss: tensor(54.6896, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6116 Loss: 0.5338430106639862 Val Loss: tensor(54.5878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6117 Loss: 0.53448586165905 Val Loss: tensor(54.6706, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6118 Loss: 0.5303125083446503 Val Loss: tensor(54.6024, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6119 Loss: 0.5316485464572906 Val Loss: tensor(54.6503, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6120 Loss: 0.5284720808267593 Val Loss: tensor(54.6218, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6121 Loss: 0.5310483872890472 Val Loss: tensor(54.6295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6122 Loss: 0.5294307917356491 Val Loss: tensor(54.6472, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6123 Loss: 0.5340170711278915 Val Loss: tensor(54.6095, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6124 Loss: 0.5346630364656448 Val Loss: tensor(54.6796, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6125 Loss: 0.5422544032335281 Val Loss: tensor(54.5923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6126 Loss: 0.5458670109510422 Val Loss: tensor(54.7199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6127 Loss: 0.5576557219028473 Val Loss: tensor(54.5802, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6128 Loss: 0.5649042725563049 Val Loss: tensor(54.7687, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6129 Loss: 0.5821928977966309 Val Loss: tensor(54.5761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6130 Loss: 0.5935161262750626 Val Loss: tensor(54.8262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6131 Loss: 0.6174952387809753 Val Loss: tensor(54.5826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6132 Loss: 0.6329079121351242 Val Loss: tensor(54.8920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6133 Loss: 0.6644237041473389 Val Loss: tensor(54.6021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6134 Loss: 0.68320532143116 Val Loss: tensor(54.9655, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6135 Loss: 0.7222029864788055 Val Loss: tensor(54.6359, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6136 Loss: 0.7425307631492615 Val Loss: tensor(55.0444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6137 Loss: 0.7873942255973816 Val Loss: tensor(54.6832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6138 Loss: 0.8059386610984802 Val Loss: tensor(55.1242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6139 Loss: 0.8525138348340988 Val Loss: tensor(54.7401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6140 Loss: 0.8645579814910889 Val Loss: tensor(55.1972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6141 Loss: 0.9056419730186462 Val Loss: tensor(54.7984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6142 Loss: 0.9066972732543945 Val Loss: tensor(55.2538, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6143 Loss: 0.9337019324302673 Val Loss: tensor(54.8464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6144 Loss: 0.9225031137466431 Val Loss: tensor(55.2881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6145 Loss: 0.9302430748939514 Val Loss: tensor(54.8745, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6146 Loss: 0.9112721085548401 Val Loss: tensor(55.3016, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6147 Loss: 0.9024185240268707 Val Loss: tensor(54.8812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6148 Loss: 0.8838144242763519 Val Loss: tensor(55.2997, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6149 Loss: 0.8669614344835281 Val Loss: tensor(54.8736, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6150 Loss: 0.8546861261129379 Val Loss: tensor(55.2847, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6151 Loss: 0.8379798978567123 Val Loss: tensor(54.8603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6152 Loss: 0.8325447291135788 Val Loss: tensor(55.2580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6153 Loss: 0.8201188296079636 Val Loss: tensor(54.8438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6154 Loss: 0.818714901804924 Val Loss: tensor(55.2252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6155 Loss: 0.8113075792789459 Val Loss: tensor(54.8208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6156 Loss: 0.8113381713628769 Val Loss: tensor(55.1947, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6157 Loss: 0.8076964616775513 Val Loss: tensor(54.7899, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6158 Loss: 0.8075319230556488 Val Loss: tensor(55.1697, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6159 Loss: 0.8049949556589127 Val Loss: tensor(54.7566, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6160 Loss: 0.802745595574379 Val Loss: tensor(55.1454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6161 Loss: 0.7980831563472748 Val Loss: tensor(54.7282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6162 Loss: 0.7915107756853104 Val Loss: tensor(55.1140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6163 Loss: 0.7823541313409805 Val Loss: tensor(54.7075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6164 Loss: 0.7703208476305008 Val Loss: tensor(55.0714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6165 Loss: 0.7563251852989197 Val Loss: tensor(54.6912, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6166 Loss: 0.7397967875003815 Val Loss: tensor(55.0199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6167 Loss: 0.7225658893585205 Val Loss: tensor(54.6740, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6168 Loss: 0.7040783315896988 Val Loss: tensor(54.9653, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6169 Loss: 0.6861463487148285 Val Loss: tensor(54.6536, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6170 Loss: 0.6684435456991196 Val Loss: tensor(54.9136, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6171 Loss: 0.6521161049604416 Val Loss: tensor(54.6318, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6172 Loss: 0.6370829492807388 Val Loss: tensor(54.8689, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6173 Loss: 0.623762384057045 Val Loss: tensor(54.6125, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6174 Loss: 0.6120804846286774 Val Loss: tensor(54.8328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6175 Loss: 0.6021544337272644 Val Loss: tensor(54.5981, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6176 Loss: 0.5936215072870255 Val Loss: tensor(54.8048, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6177 Loss: 0.5867780148983002 Val Loss: tensor(54.5888, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6178 Loss: 0.5807943791151047 Val Loss: tensor(54.7839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6179 Loss: 0.5763806998729706 Val Loss: tensor(54.5833, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6180 Loss: 0.572325199842453 Val Loss: tensor(54.7692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6181 Loss: 0.5697063058614731 Val Loss: tensor(54.5803, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6182 Loss: 0.5670364797115326 Val Loss: tensor(54.7599, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6183 Loss: 0.5656968206167221 Val Loss: tensor(54.5784, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6184 Loss: 0.5640639066696167 Val Loss: tensor(54.7550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6185 Loss: 0.5636345744132996 Val Loss: tensor(54.5772, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6186 Loss: 0.5627734512090683 Val Loss: tensor(54.7537, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6187 Loss: 0.5630333423614502 Val Loss: tensor(54.5763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6188 Loss: 0.5628318637609482 Val Loss: tensor(54.7550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6189 Loss: 0.5636168420314789 Val Loss: tensor(54.5757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6190 Loss: 0.563976988196373 Val Loss: tensor(54.7586, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6191 Loss: 0.5652251094579697 Val Loss: tensor(54.5748, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6192 Loss: 0.5661361366510391 Val Loss: tensor(54.7641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6193 Loss: 0.5677730590105057 Val Loss: tensor(54.5738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6194 Loss: 0.569238930940628 Val Loss: tensor(54.7715, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6195 Loss: 0.5712482929229736 Val Loss: tensor(54.5722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6196 Loss: 0.5733158588409424 Val Loss: tensor(54.7808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6197 Loss: 0.5757008194923401 Val Loss: tensor(54.5701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6198 Loss: 0.5784346163272858 Val Loss: tensor(54.7923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6199 Loss: 0.581201821565628 Val Loss: tensor(54.5676, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6200 Loss: 0.5846986919641495 Val Loss: tensor(54.8060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6201 Loss: 0.5878829061985016 Val Loss: tensor(54.5647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6202 Loss: 0.5922224372625351 Val Loss: tensor(54.8222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6203 Loss: 0.5958384573459625 Val Loss: tensor(54.5616, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6204 Loss: 0.6011210530996323 Val Loss: tensor(54.8410, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6205 Loss: 0.6051744669675827 Val Loss: tensor(54.5585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6206 Loss: 0.6114618629217148 Val Loss: tensor(54.8624, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6207 Loss: 0.6159261465072632 Val Loss: tensor(54.5554, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6208 Loss: 0.6232287287712097 Val Loss: tensor(54.8862, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6209 Loss: 0.6280223429203033 Val Loss: tensor(54.5527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6210 Loss: 0.6362791061401367 Val Loss: tensor(54.9118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6211 Loss: 0.6412158608436584 Val Loss: tensor(54.5503, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6212 Loss: 0.650213211774826 Val Loss: tensor(54.9383, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6213 Loss: 0.6549859493970871 Val Loss: tensor(54.5484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6214 Loss: 0.6643674224615097 Val Loss: tensor(54.9641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6215 Loss: 0.6685226559638977 Val Loss: tensor(54.5469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6216 Loss: 0.6777167618274689 Val Loss: tensor(54.9869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6217 Loss: 0.6806739717721939 Val Loss: tensor(54.5457, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6218 Loss: 0.6889587342739105 Val Loss: tensor(55.0040, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6219 Loss: 0.6900649517774582 Val Loss: tensor(54.5448, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6220 Loss: 0.696630984544754 Val Loss: tensor(55.0125, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6221 Loss: 0.6952714025974274 Val Loss: tensor(54.5440, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6222 Loss: 0.6994064301252365 Val Loss: tensor(55.0098, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6223 Loss: 0.6951934248209 Val Loss: tensor(54.5435, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6224 Loss: 0.6964700073003769 Val Loss: tensor(54.9942, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6225 Loss: 0.6894359588623047 Val Loss: tensor(54.5438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6226 Loss: 0.6879494935274124 Val Loss: tensor(54.9657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6227 Loss: 0.6786763668060303 Val Loss: tensor(54.5460, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6228 Loss: 0.675058588385582 Val Loss: tensor(54.9259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6229 Loss: 0.6646615117788315 Val Loss: tensor(54.5518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6230 Loss: 0.6601400822401047 Val Loss: tensor(54.8779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6231 Loss: 0.6500914692878723 Val Loss: tensor(54.5634, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6232 Loss: 0.6463441252708435 Val Loss: tensor(54.8261, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6233 Loss: 0.6382157653570175 Val Loss: tensor(54.5831, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6234 Loss: 0.6372004747390747 Val Loss: tensor(54.7755, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6235 Loss: 0.6324021965265274 Val Loss: tensor(54.6134, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6236 Loss: 0.6362964808940887 Val Loss: tensor(54.7310, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6237 Loss: 0.6358726322650909 Val Loss: tensor(54.6555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6238 Loss: 0.6469392031431198 Val Loss: tensor(54.6978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6239 Loss: 0.6513156741857529 Val Loss: tensor(54.7097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6240 Loss: 0.6715307533740997 Val Loss: tensor(54.6804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6241 Loss: 0.6803102046251297 Val Loss: tensor(54.7739, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6242 Loss: 0.7107011377811432 Val Loss: tensor(54.6817, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6243 Loss: 0.7221929728984833 Val Loss: tensor(54.8425, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6244 Loss: 0.761385902762413 Val Loss: tensor(54.7005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6245 Loss: 0.772029310464859 Val Loss: tensor(54.9049, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6246 Loss: 0.8141288012266159 Val Loss: tensor(54.7279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6247 Loss: 0.8180712163448334 Val Loss: tensor(54.9454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6248 Loss: 0.8513679206371307 Val Loss: tensor(54.7461, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6249 Loss: 0.8416294306516647 Val Loss: tensor(54.9470, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6250 Loss: 0.8519993722438812 Val Loss: tensor(54.7361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6251 Loss: 0.8246956914663315 Val Loss: tensor(54.9021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6252 Loss: 0.8059636503458023 Val Loss: tensor(54.6916, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6253 Loss: 0.7657357007265091 Val Loss: tensor(54.8220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6254 Loss: 0.7276474088430405 Val Loss: tensor(54.6269, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6255 Loss: 0.6865657866001129 Val Loss: tensor(54.7324, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6256 Loss: 0.647045686841011 Val Loss: tensor(54.5638, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6257 Loss: 0.615285336971283 Val Loss: tensor(54.6556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6258 Loss: 0.5857875645160675 Val Loss: tensor(54.5165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6259 Loss: 0.5655601769685745 Val Loss: tensor(54.5996, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6260 Loss: 0.5474839359521866 Val Loss: tensor(54.4873, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6261 Loss: 0.5358784943819046 Val Loss: tensor(54.5614, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6262 Loss: 0.5259337574243546 Val Loss: tensor(54.4721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6263 Loss: 0.5194482207298279 Val Loss: tensor(54.5358, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6264 Loss: 0.5141750276088715 Val Loss: tensor(54.4657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6265 Loss: 0.5105745047330856 Val Loss: tensor(54.5201, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6266 Loss: 0.5078006088733673 Val Loss: tensor(54.4642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6267 Loss: 0.5058416128158569 Val Loss: tensor(54.5133, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6268 Loss: 0.5044806003570557 Val Loss: tensor(54.4646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6269 Loss: 0.5034804791212082 Val Loss: tensor(54.5139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6270 Loss: 0.503023773431778 Val Loss: tensor(54.4659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6271 Loss: 0.5026266872882843 Val Loss: tensor(54.5198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6272 Loss: 0.5028171390295029 Val Loss: tensor(54.4670, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6273 Loss: 0.5028726011514664 Val Loss: tensor(54.5287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6274 Loss: 0.5036028623580933 Val Loss: tensor(54.4673, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6275 Loss: 0.5041233152151108 Val Loss: tensor(54.5394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6276 Loss: 0.5053640753030777 Val Loss: tensor(54.4660, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6277 Loss: 0.5064782053232193 Val Loss: tensor(54.5521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6278 Loss: 0.5082890838384628 Val Loss: tensor(54.4632, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6279 Loss: 0.5101804286241531 Val Loss: tensor(54.5679, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6280 Loss: 0.5127095580101013 Val Loss: tensor(54.4593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6281 Loss: 0.5156871229410172 Val Loss: tensor(54.5874, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6282 Loss: 0.5191673189401627 Val Loss: tensor(54.4546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6283 Loss: 0.5235930979251862 Val Loss: tensor(54.6115, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6284 Loss: 0.5283716320991516 Val Loss: tensor(54.4500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6285 Loss: 0.5347179174423218 Val Loss: tensor(54.6412, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6286 Loss: 0.5412424504756927 Val Loss: tensor(54.4464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6287 Loss: 0.5500970631837845 Val Loss: tensor(54.6776, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6288 Loss: 0.5589258372783661 Val Loss: tensor(54.4454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6289 Loss: 0.5708920806646347 Val Loss: tensor(54.7220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6290 Loss: 0.582652747631073 Val Loss: tensor(54.4488, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6291 Loss: 0.5983031690120697 Val Loss: tensor(54.7754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6292 Loss: 0.6135393530130386 Val Loss: tensor(54.4587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6293 Loss: 0.6331949681043625 Val Loss: tensor(54.8380, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6294 Loss: 0.6521254628896713 Val Loss: tensor(54.4772, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6295 Loss: 0.6755634695291519 Val Loss: tensor(54.9089, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6296 Loss: 0.6976476907730103 Val Loss: tensor(54.5057, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6297 Loss: 0.7236897200345993 Val Loss: tensor(54.9848, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6298 Loss: 0.7471408098936081 Val Loss: tensor(54.5444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6299 Loss: 0.7733420580625534 Val Loss: tensor(55.0593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6300 Loss: 0.7947695255279541 Val Loss: tensor(54.5903, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6301 Loss: 0.8175079375505447 Val Loss: tensor(55.1230, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6302 Loss: 0.832171231508255 Val Loss: tensor(54.6373, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6303 Loss: 0.8475551754236221 Val Loss: tensor(55.1656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6304 Loss: 0.8509121090173721 Val Loss: tensor(54.6766, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6305 Loss: 0.8562794029712677 Val Loss: tensor(55.1794, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6306 Loss: 0.846187636256218 Val Loss: tensor(54.6997, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6307 Loss: 0.8415166884660721 Val Loss: tensor(55.1632, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6308 Loss: 0.8197183609008789 Val Loss: tensor(54.7022, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6309 Loss: 0.807578518986702 Val Loss: tensor(55.1225, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6310 Loss: 0.7791075855493546 Val Loss: tensor(54.6860, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6311 Loss: 0.763383761048317 Val Loss: tensor(55.0667, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6312 Loss: 0.7340305298566818 Val Loss: tensor(54.6585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6313 Loss: 0.7183240503072739 Val Loss: tensor(55.0051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6314 Loss: 0.6924079954624176 Val Loss: tensor(54.6294, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6315 Loss: 0.6791795343160629 Val Loss: tensor(54.9440, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6316 Loss: 0.6587124168872833 Val Loss: tensor(54.6059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6317 Loss: 0.6490888744592667 Val Loss: tensor(54.8866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6318 Loss: 0.6341886818408966 Val Loss: tensor(54.5915, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6319 Loss: 0.6283389180898666 Val Loss: tensor(54.8338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6320 Loss: 0.6181633472442627 Val Loss: tensor(54.5863, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6321 Loss: 0.615683913230896 Val Loss: tensor(54.7861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6322 Loss: 0.6091886460781097 Val Loss: tensor(54.5889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6323 Loss: 0.6094615608453751 Val Loss: tensor(54.7432, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6324 Loss: 0.6057432740926743 Val Loss: tensor(54.5975, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6325 Loss: 0.6081671416759491 Val Loss: tensor(54.7050, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6326 Loss: 0.6065139770507812 Val Loss: tensor(54.6105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6327 Loss: 0.6105894297361374 Val Loss: tensor(54.6713, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6328 Loss: 0.6103713810443878 Val Loss: tensor(54.6264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6329 Loss: 0.6156040132045746 Val Loss: tensor(54.6417, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6330 Loss: 0.6161983609199524 Val Loss: tensor(54.6433, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6331 Loss: 0.6219994127750397 Val Loss: tensor(54.6150, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6332 Loss: 0.6226784735918045 Val Loss: tensor(54.6591, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6333 Loss: 0.6282733678817749 Val Loss: tensor(54.5898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6334 Loss: 0.6282535791397095 Val Loss: tensor(54.6716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6335 Loss: 0.6327340602874756 Val Loss: tensor(54.5649, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6336 Loss: 0.6312809884548187 Val Loss: tensor(54.6789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6337 Loss: 0.633671373128891 Val Loss: tensor(54.5391, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6338 Loss: 0.6303001642227173 Val Loss: tensor(54.6798, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6339 Loss: 0.6299746632575989 Val Loss: tensor(54.5129, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6340 Loss: 0.6246708929538727 Val Loss: tensor(54.6744, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6341 Loss: 0.6214735358953476 Val Loss: tensor(54.4872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6342 Loss: 0.6147272288799286 Val Loss: tensor(54.6642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6343 Loss: 0.609117403626442 Val Loss: tensor(54.4636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6344 Loss: 0.6017349660396576 Val Loss: tensor(54.6509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6345 Loss: 0.5945975631475449 Val Loss: tensor(54.4434, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6346 Loss: 0.5874279886484146 Val Loss: tensor(54.6368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6347 Loss: 0.5797898471355438 Val Loss: tensor(54.4276, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6348 Loss: 0.573416069149971 Val Loss: tensor(54.6233, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6349 Loss: 0.5660799443721771 Val Loss: tensor(54.4162, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6350 Loss: 0.5607994198799133 Val Loss: tensor(54.6113, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6351 Loss: 0.5542747974395752 Val Loss: tensor(54.4090, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6352 Loss: 0.5501079857349396 Val Loss: tensor(54.6011, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6353 Loss: 0.5445869863033295 Val Loss: tensor(54.4056, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6354 Loss: 0.5413942486047745 Val Loss: tensor(54.5923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6355 Loss: 0.5368963778018951 Val Loss: tensor(54.4054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6356 Loss: 0.5344929546117783 Val Loss: tensor(54.5846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6357 Loss: 0.5309221297502518 Val Loss: tensor(54.4074, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6358 Loss: 0.5291458517313004 Val Loss: tensor(54.5776, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6359 Loss: 0.5263921022415161 Val Loss: tensor(54.4110, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6360 Loss: 0.5251312553882599 Val Loss: tensor(54.5715, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6361 Loss: 0.5231156647205353 Val Loss: tensor(54.4153, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6362 Loss: 0.5223371982574463 Val Loss: tensor(54.5660, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6363 Loss: 0.5210561454296112 Val Loss: tensor(54.4198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6364 Loss: 0.52077616751194 Val Loss: tensor(54.5618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6365 Loss: 0.5202437341213226 Val Loss: tensor(54.4239, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6366 Loss: 0.5205963253974915 Val Loss: tensor(54.5594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6367 Loss: 0.5209289789199829 Val Loss: tensor(54.4277, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6368 Loss: 0.5221299976110458 Val Loss: tensor(54.5598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6369 Loss: 0.5235262811183929 Val Loss: tensor(54.4311, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6370 Loss: 0.5259553790092468 Val Loss: tensor(54.5644, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6371 Loss: 0.5287457406520844 Val Loss: tensor(54.4348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6372 Loss: 0.5330178737640381 Val Loss: tensor(54.5749, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6373 Loss: 0.5377149879932404 Val Loss: tensor(54.4398, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6374 Loss: 0.5447899103164673 Val Loss: tensor(54.5940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6375 Loss: 0.5522181987762451 Val Loss: tensor(54.4476, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6376 Loss: 0.5635024160146713 Val Loss: tensor(54.6253, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6377 Loss: 0.5748437494039536 Val Loss: tensor(54.4603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6378 Loss: 0.5923815816640854 Val Loss: tensor(54.6737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6379 Loss: 0.6092675179243088 Val Loss: tensor(54.4804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6380 Loss: 0.6357393115758896 Val Loss: tensor(54.7456, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6381 Loss: 0.6600491851568222 Val Loss: tensor(54.5103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6382 Loss: 0.6983547508716583 Val Loss: tensor(54.8478, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6383 Loss: 0.73178631067276 Val Loss: tensor(54.5515, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6384 Loss: 0.7838964015245438 Val Loss: tensor(54.9848, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6385 Loss: 0.8266480565071106 Val Loss: tensor(54.6027, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6386 Loss: 0.8912187665700912 Val Loss: tensor(55.1537, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6387 Loss: 0.9398553818464279 Val Loss: tensor(54.6584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6388 Loss: 1.0088684260845184 Val Loss: tensor(55.3365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6389 Loss: 1.0537013858556747 Val Loss: tensor(54.7090, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6390 Loss: 1.1099909096956253 Val Loss: tensor(55.4924, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6391 Loss: 1.1335913091897964 Val Loss: tensor(54.7427, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6392 Loss: 1.1539841890335083 Val Loss: tensor(55.5620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6393 Loss: 1.1365659683942795 Val Loss: tensor(54.7513, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6394 Loss: 1.1062450259923935 Val Loss: tensor(55.4961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6395 Loss: 1.0429862588644028 Val Loss: tensor(54.7333, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6396 Loss: 0.9748958647251129 Val Loss: tensor(55.3095, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6397 Loss: 0.8903199434280396 Val Loss: tensor(54.6933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6398 Loss: 0.8191636800765991 Val Loss: tensor(55.0843, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6399 Loss: 0.7490506619215012 Val Loss: tensor(54.6460, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6400 Loss: 0.7004881948232651 Val Loss: tensor(54.8961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6401 Loss: 0.6614220142364502 Val Loss: tensor(54.6169, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6402 Loss: 0.6401266306638718 Val Loss: tensor(54.7684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6403 Loss: 0.6269408464431763 Val Loss: tensor(54.6217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6404 Loss: 0.6257931143045425 Val Loss: tensor(54.6913, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6405 Loss: 0.6256166845560074 Val Loss: tensor(54.6524, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6406 Loss: 0.6348316967487335 Val Loss: tensor(54.6479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6407 Loss: 0.63717420399189 Val Loss: tensor(54.6870, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6408 Loss: 0.647923544049263 Val Loss: tensor(54.6222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6409 Loss: 0.6471575200557709 Val Loss: tensor(54.7074, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6410 Loss: 0.6530312150716782 Val Loss: tensor(54.6002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6411 Loss: 0.6471454948186874 Val Loss: tensor(54.7080, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6412 Loss: 0.6455903947353363 Val Loss: tensor(54.5733, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6413 Loss: 0.6351576000452042 Val Loss: tensor(54.6931, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6414 Loss: 0.6273597031831741 Val Loss: tensor(54.5405, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6415 Loss: 0.6146644353866577 Val Loss: tensor(54.6694, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6416 Loss: 0.6034458726644516 Val Loss: tensor(54.5069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6417 Loss: 0.5910845547914505 Val Loss: tensor(54.6422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6418 Loss: 0.5789957493543625 Val Loss: tensor(54.4785, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6419 Loss: 0.5685924887657166 Val Loss: tensor(54.6142, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6420 Loss: 0.5571942180395126 Val Loss: tensor(54.4581, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6421 Loss: 0.5492503494024277 Val Loss: tensor(54.5868, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6422 Loss: 0.5393223762512207 Val Loss: tensor(54.4454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6423 Loss: 0.5337063372135162 Val Loss: tensor(54.5608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6424 Loss: 0.5255775153636932 Val Loss: tensor(54.4396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6425 Loss: 0.5219473838806152 Val Loss: tensor(54.5362, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6426 Loss: 0.5156679004430771 Val Loss: tensor(54.4402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6427 Loss: 0.5136250555515289 Val Loss: tensor(54.5132, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6428 Loss: 0.5090945512056351 Val Loss: tensor(54.4466, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6429 Loss: 0.5082813650369644 Val Loss: tensor(54.4914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6430 Loss: 0.5053363144397736 Val Loss: tensor(54.4581, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6431 Loss: 0.5054028183221817 Val Loss: tensor(54.4708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6432 Loss: 0.5038871020078659 Val Loss: tensor(54.4734, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6433 Loss: 0.5045437663793564 Val Loss: tensor(54.4512, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6434 Loss: 0.5043263882398605 Val Loss: tensor(54.4907, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6435 Loss: 0.5053034573793411 Val Loss: tensor(54.4325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6436 Loss: 0.5062750428915024 Val Loss: tensor(54.5088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6437 Loss: 0.5073706358671188 Val Loss: tensor(54.4149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6438 Loss: 0.5094203650951385 Val Loss: tensor(54.5264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6439 Loss: 0.5104827582836151 Val Loss: tensor(54.3992, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6440 Loss: 0.5134519189596176 Val Loss: tensor(54.5426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6441 Loss: 0.5143429934978485 Val Loss: tensor(54.3862, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6442 Loss: 0.5179996341466904 Val Loss: tensor(54.5568, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6443 Loss: 0.5186340808868408 Val Loss: tensor(54.3766, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6444 Loss: 0.5226653814315796 Val Loss: tensor(54.5686, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6445 Loss: 0.5229322612285614 Val Loss: tensor(54.3712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6446 Loss: 0.5269500613212585 Val Loss: tensor(54.5776, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6447 Loss: 0.526759997010231 Val Loss: tensor(54.3706, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6448 Loss: 0.5303404629230499 Val Loss: tensor(54.5831, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6449 Loss: 0.5295867770910263 Val Loss: tensor(54.3747, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6450 Loss: 0.5323593765497208 Val Loss: tensor(54.5842, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6451 Loss: 0.5309739857912064 Val Loss: tensor(54.3834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6452 Loss: 0.5326655805110931 Val Loss: tensor(54.5797, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6453 Loss: 0.5306363999843597 Val Loss: tensor(54.3960, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6454 Loss: 0.5311487764120102 Val Loss: tensor(54.5689, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6455 Loss: 0.5285374373197556 Val Loss: tensor(54.4110, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6456 Loss: 0.5279520750045776 Val Loss: tensor(54.5516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6457 Loss: 0.5249532163143158 Val Loss: tensor(54.4268, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6458 Loss: 0.523530051112175 Val Loss: tensor(54.5290, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6459 Loss: 0.5204596221446991 Val Loss: tensor(54.4416, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6460 Loss: 0.5185652077198029 Val Loss: tensor(54.5026, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6461 Loss: 0.515821099281311 Val Loss: tensor(54.4543, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6462 Loss: 0.5138513594865799 Val Loss: tensor(54.4751, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6463 Loss: 0.5118872225284576 Val Loss: tensor(54.4645, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6464 Loss: 0.5102074444293976 Val Loss: tensor(54.4490, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6465 Loss: 0.5094341337680817 Val Loss: tensor(54.4727, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6466 Loss: 0.5084298402070999 Val Loss: tensor(54.4274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6467 Loss: 0.5092512518167496 Val Loss: tensor(54.4806, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6468 Loss: 0.5095122903585434 Val Loss: tensor(54.4135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6469 Loss: 0.5124386101961136 Val Loss: tensor(54.4898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6470 Loss: 0.5149459540843964 Val Loss: tensor(54.4121, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6471 Loss: 0.5206872522830963 Val Loss: tensor(54.5016, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6472 Loss: 0.5270784050226212 Val Loss: tensor(54.4296, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6473 Loss: 0.5365803688764572 Val Loss: tensor(54.5145, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6474 Loss: 0.549085944890976 Val Loss: tensor(54.4742, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6475 Loss: 0.5633691549301147 Val Loss: tensor(54.5225, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6476 Loss: 0.5844020992517471 Val Loss: tensor(54.5532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6477 Loss: 0.60475192964077 Val Loss: tensor(54.5157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6478 Loss: 0.6365115940570831 Val Loss: tensor(54.6708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6479 Loss: 0.6660089194774628 Val Loss: tensor(54.4892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6480 Loss: 0.7112449556589127 Val Loss: tensor(54.8274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6481 Loss: 0.7572326064109802 Val Loss: tensor(54.4565, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6482 Loss: 0.8199795633554459 Val Loss: tensor(55.0251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6483 Loss: 0.8931276947259903 Val Loss: tensor(54.4532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6484 Loss: 0.9740510433912277 Val Loss: tensor(55.2617, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6485 Loss: 1.0779404491186142 Val Loss: tensor(54.5196, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6486 Loss: 1.1596480011940002 Val Loss: tensor(55.4981, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6487 Loss: 1.2683474719524384 Val Loss: tensor(54.6647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6488 Loss: 1.301137238740921 Val Loss: tensor(55.6165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6489 Loss: 1.344602257013321 Val Loss: tensor(54.8377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6490 Loss: 1.2800729870796204 Val Loss: tensor(55.4933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6491 Loss: 1.2091702222824097 Val Loss: tensor(54.9490, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6492 Loss: 1.0899803042411804 Val Loss: tensor(55.2049, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6493 Loss: 0.9710279703140259 Val Loss: tensor(54.9318, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6494 Loss: 0.8769384622573853 Val Loss: tensor(54.9519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6495 Loss: 0.7864396721124649 Val Loss: tensor(54.7867, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6496 Loss: 0.7313536703586578 Val Loss: tensor(54.8147, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6497 Loss: 0.6795210838317871 Val Loss: tensor(54.6171, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6498 Loss: 0.6539612710475922 Val Loss: tensor(54.7611, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6499 Loss: 0.6353601664304733 Val Loss: tensor(54.5269, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6500 Loss: 0.6302448362112045 Val Loss: tensor(54.7251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6501 Loss: 0.6311112344264984 Val Loss: tensor(54.5209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6502 Loss: 0.6341104209423065 Val Loss: tensor(54.6729, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6503 Loss: 0.6378599554300308 Val Loss: tensor(54.5479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6504 Loss: 0.6395415365695953 Val Loss: tensor(54.6083, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6505 Loss: 0.63786780834198 Val Loss: tensor(54.5661, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6506 Loss: 0.6344825327396393 Val Loss: tensor(54.5473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6507 Loss: 0.6267509311437607 Val Loss: tensor(54.5538, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6508 Loss: 0.6193650215864182 Val Loss: tensor(54.5034, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6509 Loss: 0.608704224228859 Val Loss: tensor(54.5219, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6510 Loss: 0.5998719781637192 Val Loss: tensor(54.4737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6511 Loss: 0.589121088385582 Val Loss: tensor(54.4882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6512 Loss: 0.5805609375238419 Val Loss: tensor(54.4525, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6513 Loss: 0.571288451552391 Val Loss: tensor(54.4605, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6514 Loss: 0.5639312565326691 Val Loss: tensor(54.4385, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6515 Loss: 0.5566981434822083 Val Loss: tensor(54.4417, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6516 Loss: 0.5506028831005096 Val Loss: tensor(54.4286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6517 Loss: 0.5450184643268585 Val Loss: tensor(54.4302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6518 Loss: 0.53965725004673 Val Loss: tensor(54.4173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6519 Loss: 0.5350389927625656 Val Loss: tensor(54.4210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6520 Loss: 0.5300849974155426 Val Loss: tensor(54.4031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6521 Loss: 0.5260971039533615 Val Loss: tensor(54.4121, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6522 Loss: 0.5215649902820587 Val Loss: tensor(54.3884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6523 Loss: 0.5182324945926666 Val Loss: tensor(54.4030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6524 Loss: 0.5142523050308228 Val Loss: tensor(54.3776, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6525 Loss: 0.5116645991802216 Val Loss: tensor(54.3929, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6526 Loss: 0.5083485692739487 Val Loss: tensor(54.3722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6527 Loss: 0.5065568834543228 Val Loss: tensor(54.3816, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6528 Loss: 0.5039954781532288 Val Loss: tensor(54.3719, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6529 Loss: 0.5029685795307159 Val Loss: tensor(54.3690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6530 Loss: 0.5012842267751694 Val Loss: tensor(54.3766, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6531 Loss: 0.5010032802820206 Val Loss: tensor(54.3554, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6532 Loss: 0.5003557056188583 Val Loss: tensor(54.3859, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6533 Loss: 0.500867560505867 Val Loss: tensor(54.3413, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6534 Loss: 0.5014627128839493 Val Loss: tensor(54.4000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6535 Loss: 0.5028750747442245 Val Loss: tensor(54.3270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6536 Loss: 0.5049514770507812 Val Loss: tensor(54.4187, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6537 Loss: 0.5074561685323715 Val Loss: tensor(54.3127, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6538 Loss: 0.5113159269094467 Val Loss: tensor(54.4424, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6539 Loss: 0.5151496827602386 Val Loss: tensor(54.2994, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6540 Loss: 0.5211173743009567 Val Loss: tensor(54.4712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6541 Loss: 0.5265821367502213 Val Loss: tensor(54.2880, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6542 Loss: 0.5349561870098114 Val Loss: tensor(54.5053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6543 Loss: 0.5423140525817871 Val Loss: tensor(54.2801, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6544 Loss: 0.5533479452133179 Val Loss: tensor(54.5444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6545 Loss: 0.5628001987934113 Val Loss: tensor(54.2775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6546 Loss: 0.576518326997757 Val Loss: tensor(54.5881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6547 Loss: 0.5880258977413177 Val Loss: tensor(54.2822, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6548 Loss: 0.6041408479213715 Val Loss: tensor(54.6350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6549 Loss: 0.6172133535146713 Val Loss: tensor(54.2961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6550 Loss: 0.6349931061267853 Val Loss: tensor(54.6827, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6551 Loss: 0.6485263109207153 Val Loss: tensor(54.3200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6552 Loss: 0.6666992455720901 Val Loss: tensor(54.7279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6553 Loss: 0.6789461821317673 Val Loss: tensor(54.3530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6554 Loss: 0.6958629190921783 Val Loss: tensor(54.7663, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6555 Loss: 0.7047351449728012 Val Loss: tensor(54.3919, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6556 Loss: 0.7188494503498077 Val Loss: tensor(54.7947, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6557 Loss: 0.7225590348243713 Val Loss: tensor(54.4316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6558 Loss: 0.7330406755208969 Val Loss: tensor(54.8117, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6559 Loss: 0.7309756278991699 Val Loss: tensor(54.4661, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6560 Loss: 0.7380916029214859 Val Loss: tensor(54.8188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6561 Loss: 0.7311441153287888 Val Loss: tensor(54.4909, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6562 Loss: 0.7360871434211731 Val Loss: tensor(54.8196, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6563 Loss: 0.7263052463531494 Val Loss: tensor(54.5046, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6564 Loss: 0.7305030822753906 Val Loss: tensor(54.8181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6565 Loss: 0.7201369553804398 Val Loss: tensor(54.5082, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6566 Loss: 0.7245960086584091 Val Loss: tensor(54.8168, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6567 Loss: 0.7152098417282104 Val Loss: tensor(54.5041, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6568 Loss: 0.7201195508241653 Val Loss: tensor(54.8157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6569 Loss: 0.7121920734643936 Val Loss: tensor(54.4946, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6570 Loss: 0.7169329226016998 Val Loss: tensor(54.8128, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6571 Loss: 0.7100222408771515 Val Loss: tensor(54.4811, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6572 Loss: 0.7136331498622894 Val Loss: tensor(54.8047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6573 Loss: 0.7068078517913818 Val Loss: tensor(54.4653, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6574 Loss: 0.7085614204406738 Val Loss: tensor(54.7884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6575 Loss: 0.7010479718446732 Val Loss: tensor(54.4496, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6576 Loss: 0.700962021946907 Val Loss: tensor(54.7623, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6577 Loss: 0.6926856935024261 Val Loss: tensor(54.4389, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6578 Loss: 0.6917164176702499 Val Loss: tensor(54.7265, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6579 Loss: 0.6836380809545517 Val Loss: tensor(54.4397, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6580 Loss: 0.6835004538297653 Val Loss: tensor(54.6836, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6581 Loss: 0.6776246726512909 Val Loss: tensor(54.4594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6582 Loss: 0.6806258261203766 Val Loss: tensor(54.6378, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6583 Loss: 0.6796968430280685 Val Loss: tensor(54.5046, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6584 Loss: 0.6887498050928116 Val Loss: tensor(54.5934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6585 Loss: 0.6955120414495468 Val Loss: tensor(54.5791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6586 Loss: 0.7140396982431412 Val Loss: tensor(54.5536, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6587 Loss: 0.72979636490345 Val Loss: tensor(54.6810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6588 Loss: 0.7607899010181427 Val Loss: tensor(54.5185, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6589 Loss: 0.7832994312047958 Val Loss: tensor(54.7992, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6590 Loss: 0.8270123153924942 Val Loss: tensor(54.4855, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6591 Loss: 0.8489155620336533 Val Loss: tensor(54.9131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6592 Loss: 0.8994576781988144 Val Loss: tensor(54.4537, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6593 Loss: 0.9096723347902298 Val Loss: tensor(54.9987, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6594 Loss: 0.9531269520521164 Val Loss: tensor(54.4275, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6595 Loss: 0.9419144243001938 Val Loss: tensor(55.0356, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6596 Loss: 0.9603216201066971 Val Loss: tensor(54.4139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6597 Loss: 0.9261229634284973 Val Loss: tensor(55.0116, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6598 Loss: 0.9103280305862427 Val Loss: tensor(54.4131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6599 Loss: 0.8627180457115173 Val Loss: tensor(54.9286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6600 Loss: 0.8223926573991776 Val Loss: tensor(54.4121, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6601 Loss: 0.7755734473466873 Val Loss: tensor(54.8074, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6602 Loss: 0.7302222549915314 Val Loss: tensor(54.3951, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6603 Loss: 0.6925002038478851 Val Loss: tensor(54.6780, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6604 Loss: 0.6550561934709549 Val Loss: tensor(54.3618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6605 Loss: 0.6273749023675919 Val Loss: tensor(54.5618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6606 Loss: 0.6006295531988144 Val Loss: tensor(54.3277, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6607 Loss: 0.5811439752578735 Val Loss: tensor(54.4672, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6608 Loss: 0.5629121661186218 Val Loss: tensor(54.3068, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6609 Loss: 0.5499482601881027 Val Loss: tensor(54.3958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6610 Loss: 0.5375125110149384 Val Loss: tensor(54.3021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6611 Loss: 0.5298286080360413 Val Loss: tensor(54.3465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6612 Loss: 0.5214869230985641 Val Loss: tensor(54.3105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6613 Loss: 0.5178884863853455 Val Loss: tensor(54.3149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6614 Loss: 0.5126689821481705 Val Loss: tensor(54.3279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6615 Loss: 0.5120187550783157 Val Loss: tensor(54.2939, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6616 Loss: 0.5091795027256012 Val Loss: tensor(54.3509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6617 Loss: 0.5105110555887222 Val Loss: tensor(54.2767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6618 Loss: 0.5094647854566574 Val Loss: tensor(54.3770, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6619 Loss: 0.5119954347610474 Val Loss: tensor(54.2605, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6620 Loss: 0.5123602747917175 Val Loss: tensor(54.4036, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6621 Loss: 0.5154601633548737 Val Loss: tensor(54.2457, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6622 Loss: 0.5170167535543442 Val Loss: tensor(54.4287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6623 Loss: 0.5201092064380646 Val Loss: tensor(54.2337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6624 Loss: 0.5226438790559769 Val Loss: tensor(54.4506, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6625 Loss: 0.5252259224653244 Val Loss: tensor(54.2258, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6626 Loss: 0.5283696800470352 Val Loss: tensor(54.4675, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6627 Loss: 0.5299684107303619 Val Loss: tensor(54.2224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6628 Loss: 0.5332932323217392 Val Loss: tensor(54.4779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6629 Loss: 0.5335179567337036 Val Loss: tensor(54.2239, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6630 Loss: 0.5365235656499863 Val Loss: tensor(54.4809, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6631 Loss: 0.535171627998352 Val Loss: tensor(54.2307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6632 Loss: 0.5373858958482742 Val Loss: tensor(54.4763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6633 Loss: 0.5345023274421692 Val Loss: tensor(54.2430, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6634 Loss: 0.5356358289718628 Val Loss: tensor(54.4642, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6635 Loss: 0.5315141528844833 Val Loss: tensor(54.2600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6636 Loss: 0.5314999669790268 Val Loss: tensor(54.4457, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6637 Loss: 0.5266836732625961 Val Loss: tensor(54.2805, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6638 Loss: 0.5257108211517334 Val Loss: tensor(54.4218, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6639 Loss: 0.5208130031824112 Val Loss: tensor(54.3029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6640 Loss: 0.5192786306142807 Val Loss: tensor(54.3942, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6641 Loss: 0.5149445980787277 Val Loss: tensor(54.3257, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6642 Loss: 0.5133043229579926 Val Loss: tensor(54.3649, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6643 Loss: 0.5100957602262497 Val Loss: tensor(54.3471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6644 Loss: 0.5087228119373322 Val Loss: tensor(54.3358, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6645 Loss: 0.5070207417011261 Val Loss: tensor(54.3657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6646 Loss: 0.5061284452676773 Val Loss: tensor(54.3091, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6647 Loss: 0.506033182144165 Val Loss: tensor(54.3808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6648 Loss: 0.5056170076131821 Val Loss: tensor(54.2867, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6649 Loss: 0.5069713145494461 Val Loss: tensor(54.3917, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6650 Loss: 0.5068471431732178 Val Loss: tensor(54.2705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6651 Loss: 0.5092314034700394 Val Loss: tensor(54.3982, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6652 Loss: 0.5091331005096436 Val Loss: tensor(54.2625, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6653 Loss: 0.5120050013065338 Val Loss: tensor(54.3998, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6654 Loss: 0.511665090918541 Val Loss: tensor(54.2648, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6655 Loss: 0.5145095884799957 Val Loss: tensor(54.3953, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6656 Loss: 0.5138776302337646 Val Loss: tensor(54.2791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6657 Loss: 0.516436830163002 Val Loss: tensor(54.3825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6658 Loss: 0.5157915949821472 Val Loss: tensor(54.3066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6659 Loss: 0.5182563662528992 Val Loss: tensor(54.3594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6660 Loss: 0.5184810310602188 Val Loss: tensor(54.3473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6661 Loss: 0.5217480659484863 Val Loss: tensor(54.3253, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6662 Loss: 0.52447509765625 Val Loss: tensor(54.4005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6663 Loss: 0.530328780412674 Val Loss: tensor(54.2834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6664 Loss: 0.5380900949239731 Val Loss: tensor(54.4651, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6665 Loss: 0.5492017567157745 Val Loss: tensor(54.2409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6666 Loss: 0.5650605261325836 Val Loss: tensor(54.5428, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6667 Loss: 0.584608867764473 Val Loss: tensor(54.2086, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6668 Loss: 0.6111729443073273 Val Loss: tensor(54.6362, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6669 Loss: 0.6419102102518082 Val Loss: tensor(54.1988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6670 Loss: 0.6796199977397919 Val Loss: tensor(54.7473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6671 Loss: 0.7221167981624603 Val Loss: tensor(54.2226, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6672 Loss: 0.7671874761581421 Val Loss: tensor(54.8692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6673 Loss: 0.816821426153183 Val Loss: tensor(54.2860, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6674 Loss: 0.8594934344291687 Val Loss: tensor(54.9800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6675 Loss: 0.9036000967025757 Val Loss: tensor(54.3839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6676 Loss: 0.9299242049455643 Val Loss: tensor(55.0456, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6677 Loss: 0.9503534138202667 Val Loss: tensor(54.4948, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6678 Loss: 0.9508580714464188 Val Loss: tensor(55.0416, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6679 Loss: 0.9369486272335052 Val Loss: tensor(54.5856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6680 Loss: 0.9164980947971344 Val Loss: tensor(54.9799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6681 Loss: 0.8769145160913467 Val Loss: tensor(54.6292, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6682 Loss: 0.8499543368816376 Val Loss: tensor(54.8998, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6683 Loss: 0.8042559921741486 Val Loss: tensor(54.6169, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6684 Loss: 0.7813501954078674 Val Loss: tensor(54.8354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6685 Loss: 0.7434804886579514 Val Loss: tensor(54.5639, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6686 Loss: 0.7277360260486603 Val Loss: tensor(54.7954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6687 Loss: 0.7021325528621674 Val Loss: tensor(54.4989, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6688 Loss: 0.6926125586032867 Val Loss: tensor(54.7707, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6689 Loss: 0.677907258272171 Val Loss: tensor(54.4444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6690 Loss: 0.6720143854618073 Val Loss: tensor(54.7491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6691 Loss: 0.6641064584255219 Val Loss: tensor(54.4071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6692 Loss: 0.6593932509422302 Val Loss: tensor(54.7224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6693 Loss: 0.6539418250322342 Val Loss: tensor(54.3845, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6694 Loss: 0.6493513435125351 Val Loss: tensor(54.6861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6695 Loss: 0.6433212012052536 Val Loss: tensor(54.3735, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6696 Loss: 0.6393103897571564 Val Loss: tensor(54.6387, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6697 Loss: 0.6318710446357727 Val Loss: tensor(54.3751, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6698 Loss: 0.6295922100543976 Val Loss: tensor(54.5814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6699 Loss: 0.6219488382339478 Val Loss: tensor(54.3938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6700 Loss: 0.6226482391357422 Val Loss: tensor(54.5181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6701 Loss: 0.6173053979873657 Val Loss: tensor(54.4355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6702 Loss: 0.6226349323987961 Val Loss: tensor(54.4551, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6703 Loss: 0.6225753128528595 Val Loss: tensor(54.5035, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6704 Loss: 0.63546983897686 Val Loss: tensor(54.3998, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6705 Loss: 0.6433200240135193 Val Loss: tensor(54.5965, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6706 Loss: 0.6678276062011719 Val Loss: tensor(54.3579, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6707 Loss: 0.6844228506088257 Val Loss: tensor(54.7081, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6708 Loss: 0.7234839498996735 Val Loss: tensor(54.3315, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6709 Loss: 0.7458766251802444 Val Loss: tensor(54.8252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6710 Loss: 0.7970682382583618 Val Loss: tensor(54.3197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6711 Loss: 0.8171254098415375 Val Loss: tensor(54.9282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6712 Loss: 0.8686891943216324 Val Loss: tensor(54.3197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6713 Loss: 0.8744791746139526 Val Loss: tensor(54.9901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6714 Loss: 0.9068315178155899 Val Loss: tensor(54.3287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6715 Loss: 0.889155387878418 Val Loss: tensor(54.9867, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6716 Loss: 0.8870013505220413 Val Loss: tensor(54.3436, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6717 Loss: 0.848460003733635 Val Loss: tensor(54.9135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6718 Loss: 0.8159380853176117 Val Loss: tensor(54.3578, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6719 Loss: 0.7710877060890198 Val Loss: tensor(54.7947, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6720 Loss: 0.7288315445184708 Val Loss: tensor(54.3619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6721 Loss: 0.6917614042758942 Val Loss: tensor(54.6672, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6722 Loss: 0.6576176583766937 Val Loss: tensor(54.3562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6723 Loss: 0.6334307640790939 Val Loss: tensor(54.5556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6724 Loss: 0.6128799319267273 Val Loss: tensor(54.3550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6725 Loss: 0.6002341061830521 Val Loss: tensor(54.4657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6726 Loss: 0.5903940498828888 Val Loss: tensor(54.3709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6727 Loss: 0.5863005667924881 Val Loss: tensor(54.3944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6728 Loss: 0.5823720842599869 Val Loss: tensor(54.4056, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6729 Loss: 0.5844431519508362 Val Loss: tensor(54.3399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6730 Loss: 0.5833117961883545 Val Loss: tensor(54.4527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6731 Loss: 0.5899524837732315 Val Loss: tensor(54.3019, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6732 Loss: 0.5904229283332825 Val Loss: tensor(54.5047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6733 Loss: 0.6004087626934052 Val Loss: tensor(54.2781, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6734 Loss: 0.6020230799913406 Val Loss: tensor(54.5571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6735 Loss: 0.6141937524080276 Val Loss: tensor(54.2637, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6736 Loss: 0.6164758503437042 Val Loss: tensor(54.6077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6737 Loss: 0.629402756690979 Val Loss: tensor(54.2546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6738 Loss: 0.6317616701126099 Val Loss: tensor(54.6532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6739 Loss: 0.6436630338430405 Val Loss: tensor(54.2491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6740 Loss: 0.6454197317361832 Val Loss: tensor(54.6886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6741 Loss: 0.6543290615081787 Val Loss: tensor(54.2476, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6742 Loss: 0.6546465456485748 Val Loss: tensor(54.7087, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6743 Loss: 0.6588291078805923 Val Loss: tensor(54.2498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6744 Loss: 0.656896710395813 Val Loss: tensor(54.7090, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6745 Loss: 0.655422106385231 Val Loss: tensor(54.2547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6746 Loss: 0.6508861631155014 Val Loss: tensor(54.6884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6747 Loss: 0.644023209810257 Val Loss: tensor(54.2612, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6748 Loss: 0.6372431218624115 Val Loss: tensor(54.6492, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6749 Loss: 0.626484751701355 Val Loss: tensor(54.2695, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6750 Loss: 0.6185189485549927 Val Loss: tensor(54.5967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6751 Loss: 0.6060668379068375 Val Loss: tensor(54.2810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6752 Loss: 0.5982940495014191 Val Loss: tensor(54.5378, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6753 Loss: 0.586260512471199 Val Loss: tensor(54.2981, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6754 Loss: 0.5800249874591827 Val Loss: tensor(54.4783, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6755 Loss: 0.5699687749147415 Val Loss: tensor(54.3223, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6756 Loss: 0.5662515163421631 Val Loss: tensor(54.4229, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6757 Loss: 0.558951660990715 Val Loss: tensor(54.3544, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6758 Loss: 0.5583768337965012 Val Loss: tensor(54.3737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6759 Loss: 0.5540248155593872 Val Loss: tensor(54.3939, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6760 Loss: 0.5568706691265106 Val Loss: tensor(54.3319, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6761 Loss: 0.5552882105112076 Val Loss: tensor(54.4394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6762 Loss: 0.5615206211805344 Val Loss: tensor(54.2973, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6763 Loss: 0.5623781383037567 Val Loss: tensor(54.4887, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6764 Loss: 0.5716509222984314 Val Loss: tensor(54.2696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6765 Loss: 0.5744543522596359 Val Loss: tensor(54.5388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6766 Loss: 0.5859918892383575 Val Loss: tensor(54.2488, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6767 Loss: 0.5900715291500092 Val Loss: tensor(54.5866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6768 Loss: 0.6025238037109375 Val Loss: tensor(54.2351, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6769 Loss: 0.6069400310516357 Val Loss: tensor(54.6275, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6770 Loss: 0.6183707416057587 Val Loss: tensor(54.2294, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6771 Loss: 0.6219992488622665 Val Loss: tensor(54.6564, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6772 Loss: 0.6301160305738449 Val Loss: tensor(54.2320, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6773 Loss: 0.6317319273948669 Val Loss: tensor(54.6683, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6774 Loss: 0.6346124410629272 Val Loss: tensor(54.2424, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6775 Loss: 0.6333252340555191 Val Loss: tensor(54.6600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6776 Loss: 0.630168080329895 Val Loss: tensor(54.2594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6777 Loss: 0.6258499920368195 Val Loss: tensor(54.6316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6778 Loss: 0.6175347417593002 Val Loss: tensor(54.2809, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6779 Loss: 0.61099012196064 Val Loss: tensor(54.5872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6780 Loss: 0.5997813194990158 Val Loss: tensor(54.3053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6781 Loss: 0.5925232470035553 Val Loss: tensor(54.5331, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6782 Loss: 0.58108751475811 Val Loss: tensor(54.3324, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6783 Loss: 0.5748494118452072 Val Loss: tensor(54.4759, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6784 Loss: 0.5653728544712067 Val Loss: tensor(54.3633, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6785 Loss: 0.5616145581007004 Val Loss: tensor(54.4208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6786 Loss: 0.5552599132061005 Val Loss: tensor(54.3985, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6787 Loss: 0.5549268424510956 Val Loss: tensor(54.3708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6788 Loss: 0.5519334077835083 Val Loss: tensor(54.4377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6789 Loss: 0.5553790032863617 Val Loss: tensor(54.3271, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6790 Loss: 0.5553925633430481 Val Loss: tensor(54.4794, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6791 Loss: 0.5623672902584076 Val Loss: tensor(54.2902, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6792 Loss: 0.5646641999483109 Val Loss: tensor(54.5208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6793 Loss: 0.5743229538202286 Val Loss: tensor(54.2607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6794 Loss: 0.5779410600662231 Val Loss: tensor(54.5583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6795 Loss: 0.5887736082077026 Val Loss: tensor(54.2396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6796 Loss: 0.5925982445478439 Val Loss: tensor(54.5879, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6797 Loss: 0.6024315655231476 Val Loss: tensor(54.2284, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6798 Loss: 0.6051803976297379 Val Loss: tensor(54.6048, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6799 Loss: 0.6115324348211288 Val Loss: tensor(54.2290, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6800 Loss: 0.6119988560676575 Val Loss: tensor(54.6045, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6801 Loss: 0.6129055917263031 Val Loss: tensor(54.2417, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6802 Loss: 0.6103445738554001 Val Loss: tensor(54.5846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6803 Loss: 0.6052934378385544 Val Loss: tensor(54.2653, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6804 Loss: 0.5998773723840714 Val Loss: tensor(54.5461, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6805 Loss: 0.5902805775403976 Val Loss: tensor(54.2969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6806 Loss: 0.5832440406084061 Val Loss: tensor(54.4944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6807 Loss: 0.57195083796978 Val Loss: tensor(54.3330, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6808 Loss: 0.5652769356966019 Val Loss: tensor(54.4365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6809 Loss: 0.5553866624832153 Val Loss: tensor(54.3717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6810 Loss: 0.5512119382619858 Val Loss: tensor(54.3792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6811 Loss: 0.5450531393289566 Val Loss: tensor(54.4131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6812 Loss: 0.5451698154211044 Val Loss: tensor(54.3271, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6813 Loss: 0.5438698083162308 Val Loss: tensor(54.4577, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6814 Loss: 0.549348309636116 Val Loss: tensor(54.2829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6815 Loss: 0.552906408905983 Val Loss: tensor(54.5052, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6816 Loss: 0.5638401210308075 Val Loss: tensor(54.2488, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6817 Loss: 0.5714229792356491 Val Loss: tensor(54.5531, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6818 Loss: 0.5866321176290512 Val Loss: tensor(54.2269, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6819 Loss: 0.5966082513332367 Val Loss: tensor(54.5965, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6820 Loss: 0.6134313344955444 Val Loss: tensor(54.2195, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6821 Loss: 0.6233729422092438 Val Loss: tensor(54.6285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6822 Loss: 0.6377553045749664 Val Loss: tensor(54.2284, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6823 Loss: 0.644633024930954 Val Loss: tensor(54.6412, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6824 Loss: 0.6521497368812561 Val Loss: tensor(54.2530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6825 Loss: 0.6533662974834442 Val Loss: tensor(54.6287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6826 Loss: 0.651244506239891 Val Loss: tensor(54.2888, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6827 Loss: 0.6459543406963348 Val Loss: tensor(54.5907, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6828 Loss: 0.6348088383674622 Val Loss: tensor(54.3278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6829 Loss: 0.6246171593666077 Val Loss: tensor(54.5341, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6830 Loss: 0.608385905623436 Val Loss: tensor(54.3620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6831 Loss: 0.5965757220983505 Val Loss: tensor(54.4697, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6832 Loss: 0.5802883207798004 Val Loss: tensor(54.3883, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6833 Loss: 0.5701699405908585 Val Loss: tensor(54.4074, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6834 Loss: 0.557503879070282 Val Loss: tensor(54.4086, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6835 Loss: 0.5512002557516098 Val Loss: tensor(54.3524, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6836 Loss: 0.543516531586647 Val Loss: tensor(54.4271, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6837 Loss: 0.5416208207607269 Val Loss: tensor(54.3060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6838 Loss: 0.5384763777256012 Val Loss: tensor(54.4457, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6839 Loss: 0.5402243435382843 Val Loss: tensor(54.2684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6840 Loss: 0.5404020994901657 Val Loss: tensor(54.4626, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6841 Loss: 0.54418084025383 Val Loss: tensor(54.2403, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6842 Loss: 0.546270564198494 Val Loss: tensor(54.4738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6843 Loss: 0.5500689148902893 Val Loss: tensor(54.2237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6844 Loss: 0.5527784079313278 Val Loss: tensor(54.4746, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6845 Loss: 0.5547206103801727 Val Loss: tensor(54.2214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6846 Loss: 0.5569158643484116 Val Loss: tensor(54.4616, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6847 Loss: 0.555877223610878 Val Loss: tensor(54.2368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6848 Loss: 0.5569680333137512 Val Loss: tensor(54.4333, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6849 Loss: 0.5532446205615997 Val Loss: tensor(54.2717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6850 Loss: 0.5537584871053696 Val Loss: tensor(54.3913, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6851 Loss: 0.5495509505271912 Val Loss: tensor(54.3260, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6852 Loss: 0.5516565442085266 Val Loss: tensor(54.3401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6853 Loss: 0.5509635806083679 Val Loss: tensor(54.3980, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6854 Loss: 0.5588371008634567 Val Loss: tensor(54.2881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6855 Loss: 0.5667836368083954 Val Loss: tensor(54.4866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6856 Loss: 0.5861472487449646 Val Loss: tensor(54.2473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6857 Loss: 0.6077099144458771 Val Loss: tensor(54.5934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6858 Loss: 0.644425168633461 Val Loss: tensor(54.2316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6859 Loss: 0.68242247402668 Val Loss: tensor(54.7206, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6860 Loss: 0.7394067943096161 Val Loss: tensor(54.2546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6861 Loss: 0.7913217097520828 Val Loss: tensor(54.8654, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6862 Loss: 0.863705039024353 Val Loss: tensor(54.3217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6863 Loss: 0.9183854758739471 Val Loss: tensor(55.0116, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6864 Loss: 0.9893069118261337 Val Loss: tensor(54.4220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6865 Loss: 1.0279888361692429 Val Loss: tensor(55.1310, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6866 Loss: 1.072726547718048 Val Loss: tensor(54.5241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6867 Loss: 1.080519437789917 Val Loss: tensor(55.2007, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6868 Loss: 1.08474600315094 Val Loss: tensor(54.5897, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6869 Loss: 1.0660604387521744 Val Loss: tensor(55.2212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6870 Loss: 1.0414681285619736 Val Loss: tensor(54.5987, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6871 Loss: 1.016610026359558 Val Loss: tensor(55.2112, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6872 Loss: 0.9883380383253098 Val Loss: tensor(54.5631, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6873 Loss: 0.9718080908060074 Val Loss: tensor(55.1846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6874 Loss: 0.9549044221639633 Val Loss: tensor(54.5126, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6875 Loss: 0.9451372027397156 Val Loss: tensor(55.1394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6876 Loss: 0.9387680143117905 Val Loss: tensor(54.4699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6877 Loss: 0.9243416786193848 Val Loss: tensor(55.0614, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6878 Loss: 0.917572408914566 Val Loss: tensor(54.4383, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6879 Loss: 0.8881746530532837 Val Loss: tensor(54.9401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6880 Loss: 0.8696549236774445 Val Loss: tensor(54.4127, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6881 Loss: 0.8262004256248474 Val Loss: tensor(54.7841, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6882 Loss: 0.7947071343660355 Val Loss: tensor(54.3948, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6883 Loss: 0.7508756369352341 Val Loss: tensor(54.6229, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6884 Loss: 0.7173086404800415 Val Loss: tensor(54.3951, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6885 Loss: 0.6888812631368637 Val Loss: tensor(54.4908, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6886 Loss: 0.6665206402540207 Val Loss: tensor(54.4241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6887 Loss: 0.6596857458353043 Val Loss: tensor(54.4060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6888 Loss: 0.6532613337039948 Val Loss: tensor(54.4818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6889 Loss: 0.6638238281011581 Val Loss: tensor(54.3678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6890 Loss: 0.6688980758190155 Val Loss: tensor(54.5586, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6891 Loss: 0.6893579661846161 Val Loss: tensor(54.3693, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6892 Loss: 0.6984613835811615 Val Loss: tensor(54.6421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6893 Loss: 0.7218717187643051 Val Loss: tensor(54.4015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6894 Loss: 0.7277849316596985 Val Loss: tensor(54.7185, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6895 Loss: 0.7470501810312271 Val Loss: tensor(54.4469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6896 Loss: 0.7442164719104767 Val Loss: tensor(54.7734, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6897 Loss: 0.7529359608888626 Val Loss: tensor(54.4844, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6898 Loss: 0.7393323332071304 Val Loss: tensor(54.7975, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6899 Loss: 0.7348878383636475 Val Loss: tensor(54.5018, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6900 Loss: 0.7126343250274658 Val Loss: tensor(54.7900, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6901 Loss: 0.6973419040441513 Val Loss: tensor(54.4990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6902 Loss: 0.6711158752441406 Val Loss: tensor(54.7565, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6903 Loss: 0.6503515392541885 Val Loss: tensor(54.4837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6904 Loss: 0.6250744313001633 Val Loss: tensor(54.7062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6905 Loss: 0.6043449938297272 Val Loss: tensor(54.4632, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6906 Loss: 0.5833375602960587 Val Loss: tensor(54.6498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6907 Loss: 0.5661579668521881 Val Loss: tensor(54.4420, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6908 Loss: 0.5505565702915192 Val Loss: tensor(54.5965, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6909 Loss: 0.5379888862371445 Val Loss: tensor(54.4221, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6910 Loss: 0.5272912383079529 Val Loss: tensor(54.5516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6911 Loss: 0.5188210010528564 Val Loss: tensor(54.4049, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6912 Loss: 0.5118371099233627 Val Loss: tensor(54.5168, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6913 Loss: 0.5063441842794418 Val Loss: tensor(54.3918, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6914 Loss: 0.5019250065088272 Val Loss: tensor(54.4906, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6915 Loss: 0.4983622282743454 Val Loss: tensor(54.3832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6916 Loss: 0.49558793008327484 Val Loss: tensor(54.4712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6917 Loss: 0.4932021498680115 Val Loss: tensor(54.3785, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6918 Loss: 0.4914541244506836 Val Loss: tensor(54.4562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6919 Loss: 0.48977263271808624 Val Loss: tensor(54.3767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6920 Loss: 0.4886429011821747 Val Loss: tensor(54.4441, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6921 Loss: 0.48739030957221985 Val Loss: tensor(54.3767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6922 Loss: 0.48664960265159607 Val Loss: tensor(54.4337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6923 Loss: 0.4856623113155365 Val Loss: tensor(54.3777, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6924 Loss: 0.4851628541946411 Val Loss: tensor(54.4242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6925 Loss: 0.4843761771917343 Val Loss: tensor(54.3793, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6926 Loss: 0.4840215593576431 Val Loss: tensor(54.4153, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6927 Loss: 0.483388751745224 Val Loss: tensor(54.3814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6928 Loss: 0.48313650488853455 Val Loss: tensor(54.4067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6929 Loss: 0.4826405346393585 Val Loss: tensor(54.3837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6930 Loss: 0.482442706823349 Val Loss: tensor(54.3984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6931 Loss: 0.482077419757843 Val Loss: tensor(54.3861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6932 Loss: 0.48190760612487793 Val Loss: tensor(54.3903, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6933 Loss: 0.48168058693408966 Val Loss: tensor(54.3887, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6934 Loss: 0.4815211743116379 Val Loss: tensor(54.3826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6935 Loss: 0.4814230799674988 Val Loss: tensor(54.3911, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6936 Loss: 0.48126134276390076 Val Loss: tensor(54.3752, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6937 Loss: 0.48127374053001404 Val Loss: tensor(54.3932, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6938 Loss: 0.48109300434589386 Val Loss: tensor(54.3684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6939 Loss: 0.48122912645339966 Val Loss: tensor(54.3948, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6940 Loss: 0.48100318014621735 Val Loss: tensor(54.3625, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6941 Loss: 0.4812341034412384 Val Loss: tensor(54.3956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6942 Loss: 0.4809715449810028 Val Loss: tensor(54.3576, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6943 Loss: 0.481277734041214 Val Loss: tensor(54.3953, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6944 Loss: 0.48094120621681213 Val Loss: tensor(54.3539, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6945 Loss: 0.4813072979450226 Val Loss: tensor(54.3937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6946 Loss: 0.48091331124305725 Val Loss: tensor(54.3517, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6947 Loss: 0.48130638897418976 Val Loss: tensor(54.3905, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6948 Loss: 0.4808620810508728 Val Loss: tensor(54.3511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6949 Loss: 0.4812423437833786 Val Loss: tensor(54.3856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6950 Loss: 0.4807528853416443 Val Loss: tensor(54.3523, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6951 Loss: 0.4811239093542099 Val Loss: tensor(54.3787, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6952 Loss: 0.4806142598390579 Val Loss: tensor(54.3555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6953 Loss: 0.4809570610523224 Val Loss: tensor(54.3698, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6954 Loss: 0.4804886430501938 Val Loss: tensor(54.3604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6955 Loss: 0.480794295668602 Val Loss: tensor(54.3590, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6956 Loss: 0.4804542660713196 Val Loss: tensor(54.3670, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6957 Loss: 0.48075583577156067 Val Loss: tensor(54.3465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6958 Loss: 0.48063015937805176 Val Loss: tensor(54.3750, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6959 Loss: 0.48097488284111023 Val Loss: tensor(54.3328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6960 Loss: 0.48120371997356415 Val Loss: tensor(54.3841, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6961 Loss: 0.4816708564758301 Val Loss: tensor(54.3186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6962 Loss: 0.48240363597869873 Val Loss: tensor(54.3941, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6963 Loss: 0.4830927401781082 Val Loss: tensor(54.3049, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6964 Loss: 0.4844942092895508 Val Loss: tensor(54.4046, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6965 Loss: 0.48550906777381897 Val Loss: tensor(54.2927, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6966 Loss: 0.48771630227565765 Val Loss: tensor(54.4154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6967 Loss: 0.4891587942838669 Val Loss: tensor(54.2831, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6968 Loss: 0.49220794439315796 Val Loss: tensor(54.4260, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6969 Loss: 0.4941478818655014 Val Loss: tensor(54.2770, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6970 Loss: 0.49802419543266296 Val Loss: tensor(54.4364, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6971 Loss: 0.500441312789917 Val Loss: tensor(54.2750, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6972 Loss: 0.5050192028284073 Val Loss: tensor(54.4461, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6973 Loss: 0.5078491866588593 Val Loss: tensor(54.2767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6974 Loss: 0.5129741281270981 Val Loss: tensor(54.4555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6975 Loss: 0.516152635216713 Val Loss: tensor(54.2813, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6976 Loss: 0.5217681378126144 Val Loss: tensor(54.4652, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6977 Loss: 0.5254144817590714 Val Loss: tensor(54.2871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6978 Loss: 0.5318123400211334 Val Loss: tensor(54.4776, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6979 Loss: 0.5364463329315186 Val Loss: tensor(54.2929, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6980 Loss: 0.5445657521486282 Val Loss: tensor(54.4965, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6981 Loss: 0.5514629930257797 Val Loss: tensor(54.2989, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6982 Loss: 0.5632070451974869 Val Loss: tensor(54.5279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6983 Loss: 0.5747797638177872 Val Loss: tensor(54.3079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6984 Loss: 0.5934099555015564 Val Loss: tensor(54.5804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6985 Loss: 0.6135651618242264 Val Loss: tensor(54.3270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6986 Loss: 0.6439513713121414 Val Loss: tensor(54.6649, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6987 Loss: 0.6783325672149658 Val Loss: tensor(54.3680, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6988 Loss: 0.7267677932977676 Val Loss: tensor(54.7941, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6989 Loss: 0.782026082277298 Val Loss: tensor(54.4460, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6990 Loss: 0.8543812185525894 Val Loss: tensor(54.9781, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6991 Loss: 0.9349304437637329 Val Loss: tensor(54.5736, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6992 Loss: 1.0309327393770218 Val Loss: tensor(55.2142, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6993 Loss: 1.1308239102363586 Val Loss: tensor(54.7437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6994 Loss: 1.2332999259233475 Val Loss: tensor(55.4682, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6995 Loss: 1.324681118130684 Val Loss: tensor(54.9105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6996 Loss: 1.3913567662239075 Val Loss: tensor(55.6577, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6997 Loss: 1.424950510263443 Val Loss: tensor(54.9994, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6998 Loss: 1.4088167548179626 Val Loss: tensor(55.6792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6999 Loss: 1.3514891564846039 Val Loss: tensor(54.9714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7000 Loss: 1.2551261186599731 Val Loss: tensor(55.5004, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7001 Loss: 1.1385459005832672 Val Loss: tensor(54.8619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7002 Loss: 1.0187647640705109 Val Loss: tensor(55.2218, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7003 Loss: 0.9093648493289948 Val Loss: tensor(54.7355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7004 Loss: 0.8146515786647797 Val Loss: tensor(54.9757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7005 Loss: 0.7495382130146027 Val Loss: tensor(54.6576, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7006 Loss: 0.7020047903060913 Val Loss: tensor(54.8099, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7007 Loss: 0.6858872920274734 Val Loss: tensor(54.6702, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7008 Loss: 0.6836782097816467 Val Loss: tensor(54.7014, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7009 Loss: 0.6961108297109604 Val Loss: tensor(54.7477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7010 Loss: 0.7163099497556686 Val Loss: tensor(54.6193, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7011 Loss: 0.7257840782403946 Val Loss: tensor(54.8171, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7012 Loss: 0.7439460158348083 Val Loss: tensor(54.5544, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7013 Loss: 0.7321833372116089 Val Loss: tensor(54.8213, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7014 Loss: 0.7334861010313034 Val Loss: tensor(54.5064, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7015 Loss: 0.7027944475412369 Val Loss: tensor(54.7571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7016 Loss: 0.6869206130504608 Val Loss: tensor(54.4731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7017 Loss: 0.6512525975704193 Val Loss: tensor(54.6669, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7018 Loss: 0.6291274726390839 Val Loss: tensor(54.4500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7019 Loss: 0.6004666537046432 Val Loss: tensor(54.5897, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7020 Loss: 0.5823206603527069 Val Loss: tensor(54.4342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7021 Loss: 0.5643281191587448 Val Loss: tensor(54.5328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7022 Loss: 0.5526755452156067 Val Loss: tensor(54.4258, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7023 Loss: 0.5429621785879135 Val Loss: tensor(54.4877, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7024 Loss: 0.5354003310203552 Val Loss: tensor(54.4244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7025 Loss: 0.5304941236972809 Val Loss: tensor(54.4470, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7026 Loss: 0.5247446596622467 Val Loss: tensor(54.4291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7027 Loss: 0.5225728154182434 Val Loss: tensor(54.4072, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7028 Loss: 0.5180850028991699 Val Loss: tensor(54.4400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7029 Loss: 0.5176753997802734 Val Loss: tensor(54.3682, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7030 Loss: 0.5147360861301422 Val Loss: tensor(54.4563, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7031 Loss: 0.5154566019773483 Val Loss: tensor(54.3326, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7032 Loss: 0.5144052356481552 Val Loss: tensor(54.4750, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7033 Loss: 0.5156679004430771 Val Loss: tensor(54.3047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7034 Loss: 0.5165075659751892 Val Loss: tensor(54.4905, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7035 Loss: 0.5176073163747787 Val Loss: tensor(54.2887, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7036 Loss: 0.5198827832937241 Val Loss: tensor(54.4972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7037 Loss: 0.5202266722917557 Val Loss: tensor(54.2876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7038 Loss: 0.5231548696756363 Val Loss: tensor(54.4920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7039 Loss: 0.5224844217300415 Val Loss: tensor(54.3018, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7040 Loss: 0.5253174006938934 Val Loss: tensor(54.4746, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7041 Loss: 0.5238214433193207 Val Loss: tensor(54.3300, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7042 Loss: 0.526175245642662 Val Loss: tensor(54.4462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7043 Loss: 0.5244044363498688 Val Loss: tensor(54.3692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7044 Loss: 0.526467502117157 Val Loss: tensor(54.4089, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7045 Loss: 0.5251239240169525 Val Loss: tensor(54.4155, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7046 Loss: 0.5276230573654175 Val Loss: tensor(54.3659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7047 Loss: 0.5274403393268585 Val Loss: tensor(54.4644, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7048 Loss: 0.5312286615371704 Val Loss: tensor(54.3214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7049 Loss: 0.5327471047639847 Val Loss: tensor(54.5115, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7050 Loss: 0.5383008122444153 Val Loss: tensor(54.2801, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7051 Loss: 0.5416910201311111 Val Loss: tensor(54.5533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7052 Loss: 0.5487235188484192 Val Loss: tensor(54.2471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7053 Loss: 0.5534992516040802 Val Loss: tensor(54.5871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7054 Loss: 0.5609365552663803 Val Loss: tensor(54.2277, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7055 Loss: 0.5659376680850983 Val Loss: tensor(54.6095, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7056 Loss: 0.5721831768751144 Val Loss: tensor(54.2283, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7057 Loss: 0.5757306963205338 Val Loss: tensor(54.6154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7058 Loss: 0.579174280166626 Val Loss: tensor(54.2547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7059 Loss: 0.5795947313308716 Val Loss: tensor(54.5972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7060 Loss: 0.5791654735803604 Val Loss: tensor(54.3085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7061 Loss: 0.575886607170105 Val Loss: tensor(54.5493, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7062 Loss: 0.5718747526407242 Val Loss: tensor(54.3836, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7063 Loss: 0.5668056458234787 Val Loss: tensor(54.4743, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7064 Loss: 0.5616993308067322 Val Loss: tensor(54.4679, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7065 Loss: 0.5592732131481171 Val Loss: tensor(54.3871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7066 Loss: 0.5574266314506531 Val Loss: tensor(54.5504, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7067 Loss: 0.5626375079154968 Val Loss: tensor(54.3083, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7068 Loss: 0.5680126398801804 Val Loss: tensor(54.6265, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7069 Loss: 0.5836387723684311 Val Loss: tensor(54.2538, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7070 Loss: 0.5971469283103943 Val Loss: tensor(54.6959, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7071 Loss: 0.622137650847435 Val Loss: tensor(54.2311, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7072 Loss: 0.6405591368675232 Val Loss: tensor(54.7558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7073 Loss: 0.6695781201124191 Val Loss: tensor(54.2418, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7074 Loss: 0.6861664801836014 Val Loss: tensor(54.7960, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7075 Loss: 0.7102946639060974 Val Loss: tensor(54.2846, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7076 Loss: 0.7172585725784302 Val Loss: tensor(54.8004, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7077 Loss: 0.7272032797336578 Val Loss: tensor(54.3521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7078 Loss: 0.7200899422168732 Val Loss: tensor(54.7597, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7079 Loss: 0.7120925039052963 Val Loss: tensor(54.4264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7080 Loss: 0.6936345547437668 Val Loss: tensor(54.6839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7081 Loss: 0.6731248795986176 Val Loss: tensor(54.4845, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7082 Loss: 0.6514172106981277 Val Loss: tensor(54.5985, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7083 Loss: 0.6287212669849396 Val Loss: tensor(54.5128, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7084 Loss: 0.6111276894807816 Val Loss: tensor(54.5263, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7085 Loss: 0.5938767641782761 Val Loss: tensor(54.5158, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7086 Loss: 0.5835442543029785 Val Loss: tensor(54.4750, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7087 Loss: 0.5738290697336197 Val Loss: tensor(54.5071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7088 Loss: 0.5700265467166901 Val Loss: tensor(54.4404, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7089 Loss: 0.5662425607442856 Val Loss: tensor(54.4966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7090 Loss: 0.5662568211555481 Val Loss: tensor(54.4171, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7091 Loss: 0.5661594569683075 Val Loss: tensor(54.4852, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7092 Loss: 0.5675182640552521 Val Loss: tensor(54.4056, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7093 Loss: 0.5701488554477692 Val Loss: tensor(54.4705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7094 Loss: 0.5723919868469238 Val Loss: tensor(54.4119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7095 Loss: 0.5787918716669083 Val Loss: tensor(54.4533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7096 Loss: 0.5842833071947098 Val Loss: tensor(54.4458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7097 Loss: 0.5973858088254929 Val Loss: tensor(54.4384, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7098 Loss: 0.6112640500068665 Val Loss: tensor(54.5189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7099 Loss: 0.6355257034301758 Val Loss: tensor(54.4330, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7100 Loss: 0.6646947264671326 Val Loss: tensor(54.6432, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7101 Loss: 0.7050516605377197 Val Loss: tensor(54.4428, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7102 Loss: 0.7562173455953598 Val Loss: tensor(54.8278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7103 Loss: 0.8159976899623871 Val Loss: tensor(54.4715, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7104 Loss: 0.8917269855737686 Val Loss: tensor(55.0712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7105 Loss: 0.9688644856214523 Val Loss: tensor(54.5195, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7106 Loss: 1.0605881065130234 Val Loss: tensor(55.3475, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7107 Loss: 1.1415323317050934 Val Loss: tensor(54.5777, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7108 Loss: 1.2207384556531906 Val Loss: tensor(55.5825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7109 Loss: 1.274326965212822 Val Loss: tensor(54.6238, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7110 Loss: 1.2925161868333817 Val Loss: tensor(55.6449, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7111 Loss: 1.2771040797233582 Val Loss: tensor(54.6338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7112 Loss: 1.2022133320569992 Val Loss: tensor(55.4394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7113 Loss: 1.1071256250143051 Val Loss: tensor(54.5998, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7114 Loss: 0.9806599020957947 Val Loss: tensor(55.0674, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7115 Loss: 0.8637437075376511 Val Loss: tensor(54.5368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7116 Loss: 0.7630855143070221 Val Loss: tensor(54.7382, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7117 Loss: 0.6898112297058105 Val Loss: tensor(54.4815, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7118 Loss: 0.6434048563241959 Val Loss: tensor(54.5442, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7119 Loss: 0.621198982000351 Val Loss: tensor(54.4686, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7120 Loss: 0.6138515770435333 Val Loss: tensor(54.4672, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7121 Loss: 0.6160915195941925 Val Loss: tensor(54.5030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7122 Loss: 0.6253530830144882 Val Loss: tensor(54.4539, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7123 Loss: 0.6286682039499283 Val Loss: tensor(54.5548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7124 Loss: 0.6381122767925262 Val Loss: tensor(54.4548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7125 Loss: 0.632533386349678 Val Loss: tensor(54.5873, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7126 Loss: 0.6333078742027283 Val Loss: tensor(54.4456, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7127 Loss: 0.6191426515579224 Val Loss: tensor(54.5940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7128 Loss: 0.6115548461675644 Val Loss: tensor(54.4247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7129 Loss: 0.5942014306783676 Val Loss: tensor(54.5859, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7130 Loss: 0.5835355222225189 Val Loss: tensor(54.4023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7131 Loss: 0.5678000450134277 Val Loss: tensor(54.5698, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7132 Loss: 0.5576966404914856 Val Loss: tensor(54.3869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7133 Loss: 0.5452570468187332 Val Loss: tensor(54.5464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7134 Loss: 0.5365150272846222 Val Loss: tensor(54.3785, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7135 Loss: 0.5271757245063782 Val Loss: tensor(54.5165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7136 Loss: 0.5197835564613342 Val Loss: tensor(54.3731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7137 Loss: 0.5130685418844223 Val Loss: tensor(54.4847, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7138 Loss: 0.5070856809616089 Val Loss: tensor(54.3679, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7139 Loss: 0.5026098936796188 Val Loss: tensor(54.4556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7140 Loss: 0.4980205148458481 Val Loss: tensor(54.3639, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7141 Loss: 0.49528607726097107 Val Loss: tensor(54.4310, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7142 Loss: 0.49190811812877655 Val Loss: tensor(54.3633, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7143 Loss: 0.4904123991727829 Val Loss: tensor(54.4100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7144 Loss: 0.487991601228714 Val Loss: tensor(54.3667, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7145 Loss: 0.4872988909482956 Val Loss: tensor(54.3918, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7146 Loss: 0.48558594286441803 Val Loss: tensor(54.3733, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7147 Loss: 0.48536840081214905 Val Loss: tensor(54.3759, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7148 Loss: 0.4841827303171158 Val Loss: tensor(54.3813, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7149 Loss: 0.4841993749141693 Val Loss: tensor(54.3620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7150 Loss: 0.48344406485557556 Val Loss: tensor(54.3895, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7151 Loss: 0.4835369288921356 Val Loss: tensor(54.3497, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7152 Loss: 0.4831722378730774 Val Loss: tensor(54.3969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7153 Loss: 0.4832113981246948 Val Loss: tensor(54.3391, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7154 Loss: 0.4832035005092621 Val Loss: tensor(54.4028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7155 Loss: 0.48313112556934357 Val Loss: tensor(54.3304, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7156 Loss: 0.4834251403808594 Val Loss: tensor(54.4066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7157 Loss: 0.4831940084695816 Val Loss: tensor(54.3242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7158 Loss: 0.4837278872728348 Val Loss: tensor(54.4076, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7159 Loss: 0.48328766226768494 Val Loss: tensor(54.3210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7160 Loss: 0.4839647561311722 Val Loss: tensor(54.4055, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7161 Loss: 0.483325332403183 Val Loss: tensor(54.3214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7162 Loss: 0.48405151069164276 Val Loss: tensor(54.4000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7163 Loss: 0.48321250081062317 Val Loss: tensor(54.3256, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7164 Loss: 0.48389454185962677 Val Loss: tensor(54.3908, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7165 Loss: 0.48293833434581757 Val Loss: tensor(54.3339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7166 Loss: 0.48350629210472107 Val Loss: tensor(54.3779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7167 Loss: 0.4825376868247986 Val Loss: tensor(54.3459, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7168 Loss: 0.48297111690044403 Val Loss: tensor(54.3615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7169 Loss: 0.48217108845710754 Val Loss: tensor(54.3610, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7170 Loss: 0.48252032697200775 Val Loss: tensor(54.3423, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7171 Loss: 0.4821223169565201 Val Loss: tensor(54.3782, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7172 Loss: 0.48248155415058136 Val Loss: tensor(54.3213, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7173 Loss: 0.48272116482257843 Val Loss: tensor(54.3962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7174 Loss: 0.4832143038511276 Val Loss: tensor(54.3003, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7175 Loss: 0.48429083824157715 Val Loss: tensor(54.4134, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7176 Loss: 0.48500320315361023 Val Loss: tensor(54.2814, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7177 Loss: 0.48703186213970184 Val Loss: tensor(54.4284, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7178 Loss: 0.4879813492298126 Val Loss: tensor(54.2674, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7179 Loss: 0.49088431894779205 Val Loss: tensor(54.4392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7180 Loss: 0.49195875227451324 Val Loss: tensor(54.2615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7181 Loss: 0.4954119473695755 Val Loss: tensor(54.4440, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7182 Loss: 0.4963577538728714 Val Loss: tensor(54.2664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7183 Loss: 0.49981261789798737 Val Loss: tensor(54.4399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7184 Loss: 0.5002675950527191 Val Loss: tensor(54.2841, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7185 Loss: 0.5030415207147598 Val Loss: tensor(54.4242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7186 Loss: 0.5026641935110092 Val Loss: tensor(54.3144, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7187 Loss: 0.5042948126792908 Val Loss: tensor(54.3947, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7188 Loss: 0.5030511766672134 Val Loss: tensor(54.3547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7189 Loss: 0.5036923438310623 Val Loss: tensor(54.3516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7190 Loss: 0.5022328495979309 Val Loss: tensor(54.4004, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7191 Loss: 0.5029005110263824 Val Loss: tensor(54.2993, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7192 Loss: 0.5028705149888992 Val Loss: tensor(54.4477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7193 Loss: 0.5052862465381622 Val Loss: tensor(54.2457, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7194 Loss: 0.5090226083993912 Val Loss: tensor(54.4945, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7195 Loss: 0.5151956230401993 Val Loss: tensor(54.2020, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7196 Loss: 0.5248210281133652 Val Loss: tensor(54.5413, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7197 Loss: 0.5363097190856934 Val Loss: tensor(54.1808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7198 Loss: 0.5523639470338821 Val Loss: tensor(54.5869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7199 Loss: 0.5691998898983002 Val Loss: tensor(54.1954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7200 Loss: 0.5890548378229141 Val Loss: tensor(54.6232, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7201 Loss: 0.6083458513021469 Val Loss: tensor(54.2541, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7202 Loss: 0.6254696100950241 Val Loss: tensor(54.6318, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7203 Loss: 0.6402372270822525 Val Loss: tensor(54.3505, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7204 Loss: 0.6467822194099426 Val Loss: tensor(54.5925, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7205 Loss: 0.6483555734157562 Val Loss: tensor(54.4575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7206 Loss: 0.6413967311382294 Val Loss: tensor(54.5037, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7207 Loss: 0.6270398050546646 Val Loss: tensor(54.5409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7208 Loss: 0.612233117222786 Val Loss: tensor(54.3922, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7209 Loss: 0.59018574655056 Val Loss: tensor(54.5837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7210 Loss: 0.5769223421812057 Val Loss: tensor(54.2956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7211 Loss: 0.5603495985269547 Val Loss: tensor(54.5937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7212 Loss: 0.5554006546735764 Val Loss: tensor(54.2432, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7213 Loss: 0.5528159141540527 Val Loss: tensor(54.5883, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7214 Loss: 0.5593871176242828 Val Loss: tensor(54.2555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7215 Loss: 0.5717029422521591 Val Loss: tensor(54.5756, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7216 Loss: 0.5907440930604935 Val Loss: tensor(54.3496, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7217 Loss: 0.6137213259935379 Val Loss: tensor(54.5425, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7218 Loss: 0.6443898677825928 Val Loss: tensor(54.5323, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7219 Loss: 0.6755731552839279 Val Loss: tensor(54.4685, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7220 Loss: 0.7186679542064667 Val Loss: tensor(54.7881, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7221 Loss: 0.7696134746074677 Val Loss: tensor(54.3761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7222 Loss: 0.8376176804304123 Val Loss: tensor(55.0942, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7223 Loss: 0.9394388198852539 Val Loss: tensor(54.3594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7224 Loss: 1.0527532696723938 Val Loss: tensor(55.4508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7225 Loss: 1.234032154083252 Val Loss: tensor(54.5228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7226 Loss: 1.378455489873886 Val Loss: tensor(55.8239, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7227 Loss: 1.6054471731185913 Val Loss: tensor(54.8633, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7228 Loss: 1.6788491010665894 Val Loss: tensor(56.0008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7229 Loss: 1.7860151678323746 Val Loss: tensor(55.1790, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7230 Loss: 1.6581093519926071 Val Loss: tensor(55.7349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7231 Loss: 1.5054515600204468 Val Loss: tensor(55.2217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7232 Loss: 1.2779766023159027 Val Loss: tensor(55.2611, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7233 Loss: 1.0575702786445618 Val Loss: tensor(54.9707, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7234 Loss: 0.8959884941577911 Val Loss: tensor(54.9308, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7235 Loss: 0.7668173313140869 Val Loss: tensor(54.6209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7236 Loss: 0.6841854900121689 Val Loss: tensor(54.7721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7237 Loss: 0.6424553692340851 Val Loss: tensor(54.4418, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7238 Loss: 0.6248188316822052 Val Loss: tensor(54.7088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7239 Loss: 0.6316167116165161 Val Loss: tensor(54.4608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7240 Loss: 0.6361182332038879 Val Loss: tensor(54.5934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7241 Loss: 0.6406125873327255 Val Loss: tensor(54.5414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7242 Loss: 0.6398559957742691 Val Loss: tensor(54.4612, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7243 Loss: 0.6281743943691254 Val Loss: tensor(54.5835, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7244 Loss: 0.6221409738063812 Val Loss: tensor(54.3916, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7245 Loss: 0.6042231917381287 Val Loss: tensor(54.5724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7246 Loss: 0.5933281034231186 Val Loss: tensor(54.3738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7247 Loss: 0.5753284692764282 Val Loss: tensor(54.5149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7248 Loss: 0.5634201020002365 Val Loss: tensor(54.3867, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7249 Loss: 0.5494568794965744 Val Loss: tensor(54.4673, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7250 Loss: 0.5406473726034164 Val Loss: tensor(54.4000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7251 Loss: 0.5317709743976593 Val Loss: tensor(54.4396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7252 Loss: 0.5256056189537048 Val Loss: tensor(54.3978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7253 Loss: 0.5202181786298752 Val Loss: tensor(54.4235, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7254 Loss: 0.5152343511581421 Val Loss: tensor(54.3873, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7255 Loss: 0.5119977742433548 Val Loss: tensor(54.4156, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7256 Loss: 0.507769376039505 Val Loss: tensor(54.3797, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7257 Loss: 0.5060343146324158 Val Loss: tensor(54.4060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7258 Loss: 0.5025106519460678 Val Loss: tensor(54.3802, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7259 Loss: 0.501747339963913 Val Loss: tensor(54.3900, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7260 Loss: 0.49896708130836487 Val Loss: tensor(54.3870, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7261 Loss: 0.498810812830925 Val Loss: tensor(54.3721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7262 Loss: 0.49674929678440094 Val Loss: tensor(54.3974, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7263 Loss: 0.4969591349363327 Val Loss: tensor(54.3567, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7264 Loss: 0.4955257773399353 Val Loss: tensor(54.4063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7265 Loss: 0.49586305022239685 Val Loss: tensor(54.3459, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7266 Loss: 0.4948795288801193 Val Loss: tensor(54.4101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7267 Loss: 0.4951106756925583 Val Loss: tensor(54.3395, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7268 Loss: 0.49439188838005066 Val Loss: tensor(54.4097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7269 Loss: 0.4943976402282715 Val Loss: tensor(54.3360, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7270 Loss: 0.4938591420650482 Val Loss: tensor(54.4065, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7271 Loss: 0.49360528588294983 Val Loss: tensor(54.3344, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7272 Loss: 0.49318960309028625 Val Loss: tensor(54.4019, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7273 Loss: 0.4927147179841995 Val Loss: tensor(54.3344, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7274 Loss: 0.49236489832401276 Val Loss: tensor(54.3958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7275 Loss: 0.49172070622444153 Val Loss: tensor(54.3363, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7276 Loss: 0.4914141297340393 Val Loss: tensor(54.3882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7277 Loss: 0.490684375166893 Val Loss: tensor(54.3400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7278 Loss: 0.4903806895017624 Val Loss: tensor(54.3789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7279 Loss: 0.489655002951622 Val Loss: tensor(54.3456, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7280 Loss: 0.4893401116132736 Val Loss: tensor(54.3683, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7281 Loss: 0.48870934545993805 Val Loss: tensor(54.3522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7282 Loss: 0.48835496604442596 Val Loss: tensor(54.3574, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7283 Loss: 0.4879404157400131 Val Loss: tensor(54.3589, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7284 Loss: 0.487530916929245 Val Loss: tensor(54.3471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7285 Loss: 0.4874058812856674 Val Loss: tensor(54.3645, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7286 Loss: 0.48697151243686676 Val Loss: tensor(54.3389, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7287 Loss: 0.48723310232162476 Val Loss: tensor(54.3676, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7288 Loss: 0.48681044578552246 Val Loss: tensor(54.3342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7289 Loss: 0.4875057637691498 Val Loss: tensor(54.3671, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7290 Loss: 0.4871745705604553 Val Loss: tensor(54.3347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7291 Loss: 0.488367423415184 Val Loss: tensor(54.3620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7292 Loss: 0.4882827699184418 Val Loss: tensor(54.3418, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7293 Loss: 0.4900400936603546 Val Loss: tensor(54.3517, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7294 Loss: 0.4904784858226776 Val Loss: tensor(54.3567, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7295 Loss: 0.49297404289245605 Val Loss: tensor(54.3363, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7296 Loss: 0.4943910092115402 Val Loss: tensor(54.3804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7297 Loss: 0.49794910848140717 Val Loss: tensor(54.3167, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7298 Loss: 0.5010652989149094 Val Loss: tensor(54.4128, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7299 Loss: 0.5062657743692398 Val Loss: tensor(54.2950, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7300 Loss: 0.5120689570903778 Val Loss: tensor(54.4537, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7301 Loss: 0.5197717994451523 Val Loss: tensor(54.2747, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7302 Loss: 0.529445618391037 Val Loss: tensor(54.5019, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7303 Loss: 0.5406186580657959 Val Loss: tensor(54.2608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7304 Loss: 0.5552312731742859 Val Loss: tensor(54.5562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7305 Loss: 0.5707156658172607 Val Loss: tensor(54.2592, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7306 Loss: 0.5907037109136581 Val Loss: tensor(54.6141, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7307 Loss: 0.610484391450882 Val Loss: tensor(54.2758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7308 Loss: 0.6349455118179321 Val Loss: tensor(54.6714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7309 Loss: 0.6574609875679016 Val Loss: tensor(54.3141, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7310 Loss: 0.6837119460105896 Val Loss: tensor(54.7215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7311 Loss: 0.7052216231822968 Val Loss: tensor(54.3725, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7312 Loss: 0.729080468416214 Val Loss: tensor(54.7564, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7313 Loss: 0.7441680282354355 Val Loss: tensor(54.4421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7314 Loss: 0.7614659667015076 Val Loss: tensor(54.7705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7315 Loss: 0.7652446180582047 Val Loss: tensor(54.5082, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7316 Loss: 0.7739044278860092 Val Loss: tensor(54.7643, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7317 Loss: 0.7649108916521072 Val Loss: tensor(54.5561, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7318 Loss: 0.765851616859436 Val Loss: tensor(54.7449, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7319 Loss: 0.7470814138650894 Val Loss: tensor(54.5772, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7320 Loss: 0.7432432770729065 Val Loss: tensor(54.7217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7321 Loss: 0.7201621979475021 Val Loss: tensor(54.5720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7322 Loss: 0.7145858854055405 Val Loss: tensor(54.7015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7323 Loss: 0.6922906339168549 Val Loss: tensor(54.5477, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7324 Loss: 0.6870794147253036 Val Loss: tensor(54.6870, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7325 Loss: 0.6686942875385284 Val Loss: tensor(54.5140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7326 Loss: 0.6648896187543869 Val Loss: tensor(54.6778, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7327 Loss: 0.6514983475208282 Val Loss: tensor(54.4793, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7328 Loss: 0.6493835002183914 Val Loss: tensor(54.6719, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7329 Loss: 0.6407100409269333 Val Loss: tensor(54.4487, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7330 Loss: 0.6401733458042145 Val Loss: tensor(54.6665, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7331 Loss: 0.635205015540123 Val Loss: tensor(54.4256, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7332 Loss: 0.6360883563756943 Val Loss: tensor(54.6588, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7333 Loss: 0.6336477845907211 Val Loss: tensor(54.4118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7334 Loss: 0.6360077112913132 Val Loss: tensor(54.6463, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7335 Loss: 0.6351645886898041 Val Loss: tensor(54.4093, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7336 Loss: 0.6393510848283768 Val Loss: tensor(54.6277, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7337 Loss: 0.6397427022457123 Val Loss: tensor(54.4199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7338 Loss: 0.6464992612600327 Val Loss: tensor(54.6035, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7339 Loss: 0.648349404335022 Val Loss: tensor(54.4457, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7340 Loss: 0.6589241921901703 Val Loss: tensor(54.5755, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7341 Loss: 0.6628935039043427 Val Loss: tensor(54.4871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7342 Loss: 0.6790526956319809 Val Loss: tensor(54.5463, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7343 Loss: 0.6855989992618561 Val Loss: tensor(54.5423, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7344 Loss: 0.7091586142778397 Val Loss: tensor(54.5174, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7345 Loss: 0.7173816114664078 Val Loss: tensor(54.6051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7346 Loss: 0.7487737983465195 Val Loss: tensor(54.4870, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7347 Loss: 0.7548109591007233 Val Loss: tensor(54.6633, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7348 Loss: 0.7903371900320053 Val Loss: tensor(54.4481, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7349 Loss: 0.7867039740085602 Val Loss: tensor(54.6986, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7350 Loss: 0.8163745850324631 Val Loss: tensor(54.3926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7351 Loss: 0.7952117323875427 Val Loss: tensor(54.6936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7352 Loss: 0.8063097894191742 Val Loss: tensor(54.3227, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7353 Loss: 0.7671669274568558 Val Loss: tensor(54.6454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7354 Loss: 0.7553753852844238 Val Loss: tensor(54.2575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7355 Loss: 0.7094965130090714 Val Loss: tensor(54.5721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7356 Loss: 0.6850997060537338 Val Loss: tensor(54.2180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7357 Loss: 0.6470625102519989 Val Loss: tensor(54.5005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7358 Loss: 0.6247216016054153 Val Loss: tensor(54.2085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7359 Loss: 0.6007378846406937 Val Loss: tensor(54.4471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7360 Loss: 0.5872975438833237 Val Loss: tensor(54.2180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7361 Loss: 0.5746322423219681 Val Loss: tensor(54.4116, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7362 Loss: 0.5689346045255661 Val Loss: tensor(54.2354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7363 Loss: 0.5621824413537979 Val Loss: tensor(54.3811, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7364 Loss: 0.5600056201219559 Val Loss: tensor(54.2561, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7365 Loss: 0.5551153123378754 Val Loss: tensor(54.3422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7366 Loss: 0.5526665449142456 Val Loss: tensor(54.2783, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7367 Loss: 0.5478582382202148 Val Loss: tensor(54.2917, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7368 Loss: 0.5436186045408249 Val Loss: tensor(54.2988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7369 Loss: 0.5387773811817169 Val Loss: tensor(54.2377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7370 Loss: 0.5334087163209915 Val Loss: tensor(54.3140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7371 Loss: 0.5289700776338577 Val Loss: tensor(54.1918, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7372 Loss: 0.5240105390548706 Val Loss: tensor(54.3221, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7373 Loss: 0.5202133059501648 Val Loss: tensor(54.1605, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7374 Loss: 0.5167784541845322 Val Loss: tensor(54.3230, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7375 Loss: 0.5135740488767624 Val Loss: tensor(54.1454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7376 Loss: 0.5119736790657043 Val Loss: tensor(54.3177, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7377 Loss: 0.509208008646965 Val Loss: tensor(54.1465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7378 Loss: 0.5091053992509842 Val Loss: tensor(54.3070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7379 Loss: 0.5066932141780853 Val Loss: tensor(54.1637, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7380 Loss: 0.5074475258588791 Val Loss: tensor(54.2904, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7381 Loss: 0.5054144561290741 Val Loss: tensor(54.1953, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7382 Loss: 0.506591722369194 Val Loss: tensor(54.2669, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7383 Loss: 0.505141019821167 Val Loss: tensor(54.2372, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7384 Loss: 0.5068129003047943 Val Loss: tensor(54.2360, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7385 Loss: 0.5065363496541977 Val Loss: tensor(54.2844, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7386 Loss: 0.5093411356210709 Val Loss: tensor(54.1997, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7387 Loss: 0.5112269073724747 Val Loss: tensor(54.3322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7388 Loss: 0.5160248279571533 Val Loss: tensor(54.1622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7389 Loss: 0.5211252123117447 Val Loss: tensor(54.3781, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7390 Loss: 0.5285421460866928 Val Loss: tensor(54.1300, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7391 Loss: 0.5372418165206909 Val Loss: tensor(54.4212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7392 Loss: 0.5471379607915878 Val Loss: tensor(54.1104, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7393 Loss: 0.5585535168647766 Val Loss: tensor(54.4608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7394 Loss: 0.5697375386953354 Val Loss: tensor(54.1105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7395 Loss: 0.581585481762886 Val Loss: tensor(54.4936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7396 Loss: 0.5917517393827438 Val Loss: tensor(54.1346, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7397 Loss: 0.601055696606636 Val Loss: tensor(54.5138, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7398 Loss: 0.6073753535747528 Val Loss: tensor(54.1808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7399 Loss: 0.6118278652429581 Val Loss: tensor(54.5148, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7400 Loss: 0.612272173166275 Val Loss: tensor(54.2398, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7401 Loss: 0.6115426868200302 Val Loss: tensor(54.4959, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7402 Loss: 0.6063089817762375 Val Loss: tensor(54.2962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7403 Loss: 0.6023380011320114 Val Loss: tensor(54.4640, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7404 Loss: 0.5942028015851974 Val Loss: tensor(54.3368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7405 Loss: 0.5900048464536667 Val Loss: tensor(54.4317, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7406 Loss: 0.5829412490129471 Val Loss: tensor(54.3555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7407 Loss: 0.581126257777214 Val Loss: tensor(54.4105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7408 Loss: 0.5785930454730988 Val Loss: tensor(54.3558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7409 Loss: 0.58103808760643 Val Loss: tensor(54.4091, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7410 Loss: 0.5854672640562057 Val Loss: tensor(54.3468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7411 Loss: 0.5943314284086227 Val Loss: tensor(54.4368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7412 Loss: 0.6083021461963654 Val Loss: tensor(54.3399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7413 Loss: 0.6273930817842484 Val Loss: tensor(54.5079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7414 Loss: 0.6549963504076004 Val Loss: tensor(54.3437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7415 Loss: 0.6905592828989029 Val Loss: tensor(54.6410, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7416 Loss: 0.7371137738227844 Val Loss: tensor(54.3595, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7417 Loss: 0.7968164682388306 Val Loss: tensor(54.8523, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7418 Loss: 0.8665698766708374 Val Loss: tensor(54.3800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7419 Loss: 0.9556478559970856 Val Loss: tensor(55.1437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7420 Loss: 1.0484639555215836 Val Loss: tensor(54.3992, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7421 Loss: 1.16107639670372 Val Loss: tensor(55.4843, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7422 Loss: 1.2668491452932358 Val Loss: tensor(54.4264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7423 Loss: 1.3682785332202911 Val Loss: tensor(55.7658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7424 Loss: 1.4495750218629837 Val Loss: tensor(54.4805, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7425 Loss: 1.458278313279152 Val Loss: tensor(55.7574, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7426 Loss: 1.4342484027147293 Val Loss: tensor(54.5540, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7427 Loss: 1.2914388477802277 Val Loss: tensor(55.3205, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7428 Loss: 1.135741412639618 Val Loss: tensor(54.5904, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7429 Loss: 0.9548140615224838 Val Loss: tensor(54.7678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7430 Loss: 0.8166123777627945 Val Loss: tensor(54.5854, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7431 Loss: 0.729454442858696 Val Loss: tensor(54.4570, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7432 Loss: 0.702433243393898 Val Loss: tensor(54.5993, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7433 Loss: 0.7012446969747543 Val Loss: tensor(54.4104, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7434 Loss: 0.7341441065073013 Val Loss: tensor(54.6747, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7435 Loss: 0.7686854749917984 Val Loss: tensor(54.5208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7436 Loss: 0.7976401150226593 Val Loss: tensor(54.7871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7437 Loss: 0.8265461027622223 Val Loss: tensor(54.6285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7438 Loss: 0.8173908442258835 Val Loss: tensor(54.8438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7439 Loss: 0.8082920163869858 Val Loss: tensor(54.6276, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7440 Loss: 0.7597353309392929 Val Loss: tensor(54.8098, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7441 Loss: 0.7196064442396164 Val Loss: tensor(54.5543, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7442 Loss: 0.6639851927757263 Val Loss: tensor(54.7438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7443 Loss: 0.6255331188440323 Val Loss: tensor(54.4804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7444 Loss: 0.5864953398704529 Val Loss: tensor(54.6700, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7445 Loss: 0.5620263516902924 Val Loss: tensor(54.4412, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7446 Loss: 0.5386263132095337 Val Loss: tensor(54.5891, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7447 Loss: 0.524035781621933 Val Loss: tensor(54.4311, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7448 Loss: 0.5102784782648087 Val Loss: tensor(54.5118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7449 Loss: 0.5016837418079376 Val Loss: tensor(54.4259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7450 Loss: 0.4940960854291916 Val Loss: tensor(54.4519, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7451 Loss: 0.4894806891679764 Val Loss: tensor(54.4118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7452 Loss: 0.4857439398765564 Val Loss: tensor(54.4152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7453 Loss: 0.48336997628211975 Val Loss: tensor(54.3916, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7454 Loss: 0.481751024723053 Val Loss: tensor(54.3954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7455 Loss: 0.4804494082927704 Val Loss: tensor(54.3760, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7456 Loss: 0.47985148429870605 Val Loss: tensor(54.3832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7457 Loss: 0.4790404587984085 Val Loss: tensor(54.3704, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7458 Loss: 0.47882692515850067 Val Loss: tensor(54.3736, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7459 Loss: 0.4782588332891464 Val Loss: tensor(54.3716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7460 Loss: 0.4781002700328827 Val Loss: tensor(54.3665, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7461 Loss: 0.47769297659397125 Val Loss: tensor(54.3739, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7462 Loss: 0.4774677902460098 Val Loss: tensor(54.3620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7463 Loss: 0.4772004932165146 Val Loss: tensor(54.3738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7464 Loss: 0.4768955558538437 Val Loss: tensor(54.3593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7465 Loss: 0.47674766182899475 Val Loss: tensor(54.3712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7466 Loss: 0.47637830674648285 Val Loss: tensor(54.3580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7467 Loss: 0.4763365834951401 Val Loss: tensor(54.3667, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7468 Loss: 0.4759173095226288 Val Loss: tensor(54.3580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7469 Loss: 0.4759565442800522 Val Loss: tensor(54.3614, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7470 Loss: 0.4755222499370575 Val Loss: tensor(54.3590, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7471 Loss: 0.47560665011405945 Val Loss: tensor(54.3555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7472 Loss: 0.47520095109939575 Val Loss: tensor(54.3608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7473 Loss: 0.4753226637840271 Val Loss: tensor(54.3492, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7474 Loss: 0.47498464584350586 Val Loss: tensor(54.3631, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7475 Loss: 0.47510823607444763 Val Loss: tensor(54.3427, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7476 Loss: 0.47486625611782074 Val Loss: tensor(54.3660, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7477 Loss: 0.47498854994773865 Val Loss: tensor(54.3362, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7478 Loss: 0.4748697578907013 Val Loss: tensor(54.3690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7479 Loss: 0.474944531917572 Val Loss: tensor(54.3301, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7480 Loss: 0.4749829173088074 Val Loss: tensor(54.3717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7481 Loss: 0.47498784959316254 Val Loss: tensor(54.3246, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7482 Loss: 0.4751824885606766 Val Loss: tensor(54.3737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7483 Loss: 0.475081130862236 Val Loss: tensor(54.3203, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7484 Loss: 0.4754316657781601 Val Loss: tensor(54.3742, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7485 Loss: 0.4751957952976227 Val Loss: tensor(54.3176, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7486 Loss: 0.4756830781698227 Val Loss: tensor(54.3727, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7487 Loss: 0.4752914160490036 Val Loss: tensor(54.3173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7488 Loss: 0.47585229575634 Val Loss: tensor(54.3684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7489 Loss: 0.4752946197986603 Val Loss: tensor(54.3197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7490 Loss: 0.47588081657886505 Val Loss: tensor(54.3612, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7491 Loss: 0.4751795083284378 Val Loss: tensor(54.3252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7492 Loss: 0.4757331758737564 Val Loss: tensor(54.3508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7493 Loss: 0.47497786581516266 Val Loss: tensor(54.3338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7494 Loss: 0.4754853695631027 Val Loss: tensor(54.3371, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7495 Loss: 0.4748033732175827 Val Loss: tensor(54.3450, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7496 Loss: 0.47529707849025726 Val Loss: tensor(54.3210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7497 Loss: 0.47490859031677246 Val Loss: tensor(54.3583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7498 Loss: 0.4754718542098999 Val Loss: tensor(54.3034, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7499 Loss: 0.475633829832077 Val Loss: tensor(54.3727, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7500 Loss: 0.4764014333486557 Val Loss: tensor(54.2859, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7501 Loss: 0.477387398481369 Val Loss: tensor(54.3868, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7502 Loss: 0.47845667600631714 Val Loss: tensor(54.2707, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7503 Loss: 0.48043736815452576 Val Loss: tensor(54.3992, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7504 Loss: 0.4818267673254013 Val Loss: tensor(54.2603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7505 Loss: 0.4847627580165863 Val Loss: tensor(54.4084, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7506 Loss: 0.48632341623306274 Val Loss: tensor(54.2571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7507 Loss: 0.4898567497730255 Val Loss: tensor(54.4122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7508 Loss: 0.49122387170791626 Val Loss: tensor(54.2629, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7509 Loss: 0.49469679594039917 Val Loss: tensor(54.4086, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7510 Loss: 0.49532388150691986 Val Loss: tensor(54.2781, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7511 Loss: 0.49793241918087006 Val Loss: tensor(54.3952, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7512 Loss: 0.49728138744831085 Val Loss: tensor(54.3011, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7513 Loss: 0.4983974099159241 Val Loss: tensor(54.3712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7514 Loss: 0.4962620586156845 Val Loss: tensor(54.3285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7515 Loss: 0.49579252302646637 Val Loss: tensor(54.3375, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7516 Loss: 0.4926725775003433 Val Loss: tensor(54.3561, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7517 Loss: 0.49124616384506226 Val Loss: tensor(54.2984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7518 Loss: 0.48845280706882477 Val Loss: tensor(54.3802, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7519 Loss: 0.4872216731309891 Val Loss: tensor(54.2605, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7520 Loss: 0.48647409677505493 Val Loss: tensor(54.3995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7521 Loss: 0.4866753816604614 Val Loss: tensor(54.2327, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7522 Loss: 0.48937200009822845 Val Loss: tensor(54.4139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7523 Loss: 0.4918527156114578 Val Loss: tensor(54.2241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7524 Loss: 0.49826009571552277 Val Loss: tensor(54.4229, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7525 Loss: 0.5032367408275604 Val Loss: tensor(54.2431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7526 Loss: 0.5122222304344177 Val Loss: tensor(54.4227, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7527 Loss: 0.5192333161830902 Val Loss: tensor(54.2943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7528 Loss: 0.5288679152727127 Val Loss: tensor(54.4044, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7529 Loss: 0.5371450483798981 Val Loss: tensor(54.3748, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7530 Loss: 0.5465567111968994 Val Loss: tensor(54.3591, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7531 Loss: 0.5566455870866776 Val Loss: tensor(54.4745, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7532 Loss: 0.5689655840396881 Val Loss: tensor(54.2887, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7533 Loss: 0.5858016312122345 Val Loss: tensor(54.5857, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7534 Loss: 0.6099311858415604 Val Loss: tensor(54.2157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7535 Loss: 0.643827423453331 Val Loss: tensor(54.7148, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7536 Loss: 0.6919796764850616 Val Loss: tensor(54.1824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7537 Loss: 0.7518204748630524 Val Loss: tensor(54.8751, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7538 Loss: 0.8316322416067123 Val Loss: tensor(54.2400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7539 Loss: 0.9102809429168701 Val Loss: tensor(55.0495, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7540 Loss: 1.0096110850572586 Val Loss: tensor(54.4243, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7541 Loss: 1.0699672251939774 Val Loss: tensor(55.1491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7542 Loss: 1.1388086825609207 Val Loss: tensor(54.6971, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7543 Loss: 1.129966378211975 Val Loss: tensor(55.0613, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7544 Loss: 1.1059761941432953 Val Loss: tensor(54.9215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7545 Loss: 1.0337833166122437 Val Loss: tensor(54.8231, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7546 Loss: 0.9360688030719757 Val Loss: tensor(54.9945, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7547 Loss: 0.8635086119174957 Val Loss: tensor(54.6052, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7548 Loss: 0.7705597132444382 Val Loss: tensor(54.9227, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7549 Loss: 0.7231328636407852 Val Loss: tensor(54.5010, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7550 Loss: 0.6611313968896866 Val Loss: tensor(54.7557, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7551 Loss: 0.6309749484062195 Val Loss: tensor(54.4869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7552 Loss: 0.5945910662412643 Val Loss: tensor(54.5825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7553 Loss: 0.5779436677694321 Val Loss: tensor(54.4942, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7554 Loss: 0.563762754201889 Val Loss: tensor(54.4773, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7555 Loss: 0.5582109987735748 Val Loss: tensor(54.4892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7556 Loss: 0.5586554706096649 Val Loss: tensor(54.4396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7557 Loss: 0.557432621717453 Val Loss: tensor(54.4764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7558 Loss: 0.5635579526424408 Val Loss: tensor(54.4341, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7559 Loss: 0.5622677654027939 Val Loss: tensor(54.4637, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7560 Loss: 0.569195881485939 Val Loss: tensor(54.4333, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7561 Loss: 0.5679886043071747 Val Loss: tensor(54.4596, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7562 Loss: 0.5747270286083221 Val Loss: tensor(54.4222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7563 Loss: 0.5754969716072083 Val Loss: tensor(54.4739, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7564 Loss: 0.583392322063446 Val Loss: tensor(54.4008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7565 Loss: 0.5880345106124878 Val Loss: tensor(54.5053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7566 Loss: 0.5984766632318497 Val Loss: tensor(54.3765, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7567 Loss: 0.6086169481277466 Val Loss: tensor(54.5518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7568 Loss: 0.6217852383852005 Val Loss: tensor(54.3572, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7569 Loss: 0.638244017958641 Val Loss: tensor(54.6133, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7570 Loss: 0.6534090638160706 Val Loss: tensor(54.3473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7571 Loss: 0.6752625554800034 Val Loss: tensor(54.6844, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7572 Loss: 0.6907486319541931 Val Loss: tensor(54.3476, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7573 Loss: 0.7150012105703354 Val Loss: tensor(54.7547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7574 Loss: 0.7281081676483154 Val Loss: tensor(54.3546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7575 Loss: 0.7502372115850449 Val Loss: tensor(54.8115, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7576 Loss: 0.7574992775917053 Val Loss: tensor(54.3640, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7577 Loss: 0.7726176530122757 Val Loss: tensor(54.8393, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7578 Loss: 0.7707833796739578 Val Loss: tensor(54.3733, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7579 Loss: 0.7752169072628021 Val Loss: tensor(54.8245, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7580 Loss: 0.7629382014274597 Val Loss: tensor(54.3859, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7581 Loss: 0.7564065605401993 Val Loss: tensor(54.7616, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7582 Loss: 0.7365090847015381 Val Loss: tensor(54.4124, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7583 Loss: 0.7244552373886108 Val Loss: tensor(54.6610, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7584 Loss: 0.7052355855703354 Val Loss: tensor(54.4693, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7585 Loss: 0.7000187486410141 Val Loss: tensor(54.5515, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7586 Loss: 0.6943751871585846 Val Loss: tensor(54.5758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7587 Loss: 0.714206725358963 Val Loss: tensor(54.4767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7588 Loss: 0.7351603358983994 Val Loss: tensor(54.7463, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7589 Loss: 0.7992820888757706 Val Loss: tensor(54.4835, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7590 Loss: 0.8508564531803131 Val Loss: tensor(54.9761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7591 Loss: 0.9665411412715912 Val Loss: tensor(54.5921, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7592 Loss: 1.0310103744268417 Val Loss: tensor(55.2192, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7593 Loss: 1.1700651049613953 Val Loss: tensor(54.7526, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7594 Loss: 1.1988413035869598 Val Loss: tensor(55.3804, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7595 Loss: 1.284345269203186 Val Loss: tensor(54.8469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7596 Loss: 1.2235170751810074 Val Loss: tensor(55.3680, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7597 Loss: 1.189519852399826 Val Loss: tensor(54.8060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7598 Loss: 1.0558914244174957 Val Loss: tensor(55.1892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7599 Loss: 0.9463649839162827 Val Loss: tensor(54.6866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7600 Loss: 0.8255071938037872 Val Loss: tensor(54.9579, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7601 Loss: 0.7350904047489166 Val Loss: tensor(54.5653, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7602 Loss: 0.6670794636011124 Val Loss: tensor(54.7641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7603 Loss: 0.6205784231424332 Val Loss: tensor(54.4713, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7604 Loss: 0.5870068818330765 Val Loss: tensor(54.6093, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7605 Loss: 0.5649186670780182 Val Loss: tensor(54.4121, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7606 Loss: 0.5440205186605453 Val Loss: tensor(54.4754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7607 Loss: 0.530357152223587 Val Loss: tensor(54.3748, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7608 Loss: 0.5155068933963776 Val Loss: tensor(54.3770, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7609 Loss: 0.5054116994142532 Val Loss: tensor(54.3463, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7610 Loss: 0.49693819880485535 Val Loss: tensor(54.3266, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7611 Loss: 0.49037444591522217 Val Loss: tensor(54.3262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7612 Loss: 0.48706255853176117 Val Loss: tensor(54.3104, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7613 Loss: 0.48334480822086334 Val Loss: tensor(54.3168, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7614 Loss: 0.48275670409202576 Val Loss: tensor(54.3080, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7615 Loss: 0.48063020408153534 Val Loss: tensor(54.3209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7616 Loss: 0.480994775891304 Val Loss: tensor(54.3078, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7617 Loss: 0.479582741856575 Val Loss: tensor(54.3345, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7618 Loss: 0.48014092445373535 Val Loss: tensor(54.3045, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7619 Loss: 0.4791168123483658 Val Loss: tensor(54.3472, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7620 Loss: 0.47965072095394135 Val Loss: tensor(54.2983, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7621 Loss: 0.4789169430732727 Val Loss: tensor(54.3550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7622 Loss: 0.47938738763332367 Val Loss: tensor(54.2929, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7623 Loss: 0.47892075777053833 Val Loss: tensor(54.3598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7624 Loss: 0.47927315533161163 Val Loss: tensor(54.2900, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7625 Loss: 0.47899767756462097 Val Loss: tensor(54.3632, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7626 Loss: 0.4792308956384659 Val Loss: tensor(54.2889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7627 Loss: 0.47911952435970306 Val Loss: tensor(54.3654, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7628 Loss: 0.4792096018791199 Val Loss: tensor(54.2885, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7629 Loss: 0.47922083735466003 Val Loss: tensor(54.3666, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7630 Loss: 0.47915078699588776 Val Loss: tensor(54.2889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7631 Loss: 0.4792614132165909 Val Loss: tensor(54.3664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7632 Loss: 0.47900858521461487 Val Loss: tensor(54.2903, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7633 Loss: 0.47918787598609924 Val Loss: tensor(54.3645, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7634 Loss: 0.47874589264392853 Val Loss: tensor(54.2934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7635 Loss: 0.4789365828037262 Val Loss: tensor(54.3604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7636 Loss: 0.4783306121826172 Val Loss: tensor(54.2985, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7637 Loss: 0.4784826785326004 Val Loss: tensor(54.3540, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7638 Loss: 0.4777802675962448 Val Loss: tensor(54.3061, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7639 Loss: 0.47787536680698395 Val Loss: tensor(54.3451, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7640 Loss: 0.47714337706565857 Val Loss: tensor(54.3159, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7641 Loss: 0.47719742357730865 Val Loss: tensor(54.3338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7642 Loss: 0.47658705711364746 Val Loss: tensor(54.3275, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7643 Loss: 0.47663991153240204 Val Loss: tensor(54.3206, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7644 Loss: 0.47629329562187195 Val Loss: tensor(54.3405, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7645 Loss: 0.4764004796743393 Val Loss: tensor(54.3062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7646 Loss: 0.47648386657238007 Val Loss: tensor(54.3541, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7647 Loss: 0.47671401500701904 Val Loss: tensor(54.2914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7648 Loss: 0.4773462265729904 Val Loss: tensor(54.3674, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7649 Loss: 0.4777275174856186 Val Loss: tensor(54.2777, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7650 Loss: 0.4789547026157379 Val Loss: tensor(54.3795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7651 Loss: 0.47944724559783936 Val Loss: tensor(54.2662, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7652 Loss: 0.4812055379152298 Val Loss: tensor(54.3894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7653 Loss: 0.4816681444644928 Val Loss: tensor(54.2587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7654 Loss: 0.4837334156036377 Val Loss: tensor(54.3956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7655 Loss: 0.48394775390625 Val Loss: tensor(54.2565, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7656 Loss: 0.48601485788822174 Val Loss: tensor(54.3967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7657 Loss: 0.48571981489658356 Val Loss: tensor(54.2609, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7658 Loss: 0.4874187707901001 Val Loss: tensor(54.3912, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7659 Loss: 0.4864347130060196 Val Loss: tensor(54.2726, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7660 Loss: 0.48751577734947205 Val Loss: tensor(54.3778, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7661 Loss: 0.48585858941078186 Val Loss: tensor(54.2914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7662 Loss: 0.48634956777095795 Val Loss: tensor(54.3562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7663 Loss: 0.4844147264957428 Val Loss: tensor(54.3162, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7664 Loss: 0.48471297323703766 Val Loss: tensor(54.3271, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7665 Loss: 0.4833833575248718 Val Loss: tensor(54.3459, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7666 Loss: 0.4843010902404785 Val Loss: tensor(54.2931, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7667 Loss: 0.4848075956106186 Val Loss: tensor(54.3789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7668 Loss: 0.48744209110736847 Val Loss: tensor(54.2588, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7669 Loss: 0.4911221116781235 Val Loss: tensor(54.4145, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7670 Loss: 0.4965050667524338 Val Loss: tensor(54.2300, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7671 Loss: 0.5042681694030762 Val Loss: tensor(54.4521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7672 Loss: 0.5129877030849457 Val Loss: tensor(54.2134, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7673 Loss: 0.5246993601322174 Val Loss: tensor(54.4908, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7674 Loss: 0.536378487944603 Val Loss: tensor(54.2153, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7675 Loss: 0.5503894984722137 Val Loss: tensor(54.5264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7676 Loss: 0.5632431358098984 Val Loss: tensor(54.2404, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7677 Loss: 0.5763047784566879 Val Loss: tensor(54.5509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7678 Loss: 0.5869806408882141 Val Loss: tensor(54.2883, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7679 Loss: 0.5950050354003906 Val Loss: tensor(54.5526, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7680 Loss: 0.5993293672800064 Val Loss: tensor(54.3512, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7681 Loss: 0.599489763379097 Val Loss: tensor(54.5227, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7682 Loss: 0.5947708636522293 Val Loss: tensor(54.4146, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7683 Loss: 0.5877963006496429 Val Loss: tensor(54.4634, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7684 Loss: 0.575664684176445 Val Loss: tensor(54.4647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7685 Loss: 0.5659290999174118 Val Loss: tensor(54.3893, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7686 Loss: 0.5525026470422745 Val Loss: tensor(54.4965, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7687 Loss: 0.5454794317483902 Val Loss: tensor(54.3214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7688 Loss: 0.5379734486341476 Val Loss: tensor(54.5152, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7689 Loss: 0.5377557128667831 Val Loss: tensor(54.2795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7690 Loss: 0.5409271270036697 Val Loss: tensor(54.5313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7691 Loss: 0.5502555072307587 Val Loss: tensor(54.2820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7692 Loss: 0.565826877951622 Val Loss: tensor(54.5527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7693 Loss: 0.5876804739236832 Val Loss: tensor(54.3496, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7694 Loss: 0.615786999464035 Val Loss: tensor(54.5740, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7695 Loss: 0.654150515794754 Val Loss: tensor(54.5011, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7696 Loss: 0.695009782910347 Val Loss: tensor(54.5727, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7697 Loss: 0.7544736415147781 Val Loss: tensor(54.7397, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7698 Loss: 0.8137506544589996 Val Loss: tensor(54.5383, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7699 Loss: 0.9041585922241211 Val Loss: tensor(55.0575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7700 Loss: 1.0025455355644226 Val Loss: tensor(54.5278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7701 Loss: 1.14201420545578 Val Loss: tensor(55.4714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7702 Loss: 1.3056346476078033 Val Loss: tensor(54.6430, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7703 Loss: 1.4866380393505096 Val Loss: tensor(55.9795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7704 Loss: 1.6943206787109375 Val Loss: tensor(54.9059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7705 Loss: 1.8114911317825317 Val Loss: tensor(56.3336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7706 Loss: 1.9142712950706482 Val Loss: tensor(55.1606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7707 Loss: 1.77893528342247 Val Loss: tensor(56.0486, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7708 Loss: 1.6031939089298248 Val Loss: tensor(55.1945, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7709 Loss: 1.3228618800640106 Val Loss: tensor(55.3623, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7710 Loss: 1.08163520693779 Val Loss: tensor(55.0027, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7711 Loss: 0.9058521389961243 Val Loss: tensor(54.9349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7712 Loss: 0.7920544147491455 Val Loss: tensor(54.7407, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7713 Loss: 0.7217371612787247 Val Loss: tensor(54.7799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7714 Loss: 0.7002047300338745 Val Loss: tensor(54.6101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7715 Loss: 0.6910011619329453 Val Loss: tensor(54.7796, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7716 Loss: 0.7002834528684616 Val Loss: tensor(54.6393, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7717 Loss: 0.7007243782281876 Val Loss: tensor(54.7329, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7718 Loss: 0.6870634257793427 Val Loss: tensor(54.6944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7719 Loss: 0.669690802693367 Val Loss: tensor(54.6098, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7720 Loss: 0.6338297128677368 Val Loss: tensor(54.6757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7721 Loss: 0.6072299778461456 Val Loss: tensor(54.5245, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7722 Loss: 0.5755743086338043 Val Loss: tensor(54.6332, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7723 Loss: 0.5544026046991348 Val Loss: tensor(54.4845, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7724 Loss: 0.535638153553009 Val Loss: tensor(54.5680, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7725 Loss: 0.523740753531456 Val Loss: tensor(54.4771, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7726 Loss: 0.5142341703176498 Val Loss: tensor(54.5166, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7727 Loss: 0.5082956105470657 Val Loss: tensor(54.4799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7728 Loss: 0.5030810236930847 Val Loss: tensor(54.4852, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7729 Loss: 0.49922969937324524 Val Loss: tensor(54.4771, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7730 Loss: 0.4960910826921463 Val Loss: tensor(54.4649, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7731 Loss: 0.4934238791465759 Val Loss: tensor(54.4671, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7732 Loss: 0.49201221764087677 Val Loss: tensor(54.4518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7733 Loss: 0.4904365539550781 Val Loss: tensor(54.4573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7734 Loss: 0.4900721162557602 Val Loss: tensor(54.4375, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7735 Loss: 0.48912760615348816 Val Loss: tensor(54.4523, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7736 Loss: 0.4889955371618271 Val Loss: tensor(54.4232, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7737 Loss: 0.4884159117937088 Val Loss: tensor(54.4508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7738 Loss: 0.4881485253572464 Val Loss: tensor(54.4119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7739 Loss: 0.48779158294200897 Val Loss: tensor(54.4500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7740 Loss: 0.48737117648124695 Val Loss: tensor(54.4065, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7741 Loss: 0.48714402318000793 Val Loss: tensor(54.4453, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7742 Loss: 0.4865710288286209 Val Loss: tensor(54.4058, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7743 Loss: 0.4864247143268585 Val Loss: tensor(54.4358, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7744 Loss: 0.48575910925865173 Val Loss: tensor(54.4073, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7745 Loss: 0.485652893781662 Val Loss: tensor(54.4248, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7746 Loss: 0.4849868714809418 Val Loss: tensor(54.4100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7747 Loss: 0.48494087159633636 Val Loss: tensor(54.4135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7748 Loss: 0.4843403100967407 Val Loss: tensor(54.4136, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7749 Loss: 0.48433569073677063 Val Loss: tensor(54.4025, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7750 Loss: 0.4838534891605377 Val Loss: tensor(54.4183, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7751 Loss: 0.4838729500770569 Val Loss: tensor(54.3912, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7752 Loss: 0.48356208205223083 Val Loss: tensor(54.4239, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7753 Loss: 0.48359647393226624 Val Loss: tensor(54.3795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7754 Loss: 0.4835185706615448 Val Loss: tensor(54.4301, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7755 Loss: 0.48354634642601013 Val Loss: tensor(54.3679, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7756 Loss: 0.483722060918808 Val Loss: tensor(54.4361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7757 Loss: 0.48368947207927704 Val Loss: tensor(54.3572, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7758 Loss: 0.4841274619102478 Val Loss: tensor(54.4409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7759 Loss: 0.4839836061000824 Val Loss: tensor(54.3482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7760 Loss: 0.48465675115585327 Val Loss: tensor(54.4434, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7761 Loss: 0.4843141436576843 Val Loss: tensor(54.3420, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7762 Loss: 0.4851905554533005 Val Loss: tensor(54.4426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7763 Loss: 0.4845675528049469 Val Loss: tensor(54.3394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7764 Loss: 0.4855794310569763 Val Loss: tensor(54.4379, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7765 Loss: 0.4846370667219162 Val Loss: tensor(54.3415, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7766 Loss: 0.4857866019010544 Val Loss: tensor(54.4287, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7767 Loss: 0.4845597892999649 Val Loss: tensor(54.3491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7768 Loss: 0.48592154681682587 Val Loss: tensor(54.4149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7769 Loss: 0.4846487492322922 Val Loss: tensor(54.3627, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7770 Loss: 0.48641209304332733 Val Loss: tensor(54.3970, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7771 Loss: 0.4855438768863678 Val Loss: tensor(54.3828, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7772 Loss: 0.488072469830513 Val Loss: tensor(54.3759, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7773 Loss: 0.48833250999450684 Val Loss: tensor(54.4093, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7774 Loss: 0.4921545088291168 Val Loss: tensor(54.3539, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7775 Loss: 0.49446040391921997 Val Loss: tensor(54.4419, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7776 Loss: 0.5002124011516571 Val Loss: tensor(54.3336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7777 Loss: 0.5055374354124069 Val Loss: tensor(54.4798, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7778 Loss: 0.5138249546289444 Val Loss: tensor(54.3185, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7779 Loss: 0.5228885859251022 Val Loss: tensor(54.5216, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7780 Loss: 0.533987283706665 Val Loss: tensor(54.3127, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7781 Loss: 0.5469545871019363 Val Loss: tensor(54.5654, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7782 Loss: 0.560503363609314 Val Loss: tensor(54.3195, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7783 Loss: 0.5765254348516464 Val Loss: tensor(54.6077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7784 Loss: 0.5911416560411453 Val Loss: tensor(54.3414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7785 Loss: 0.6081908643245697 Val Loss: tensor(54.6434, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7786 Loss: 0.6213338375091553 Val Loss: tensor(54.3781, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7787 Loss: 0.6363544762134552 Val Loss: tensor(54.6657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7788 Loss: 0.6446941792964935 Val Loss: tensor(54.4253, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7789 Loss: 0.6545259952545166 Val Loss: tensor(54.6685, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7790 Loss: 0.6551649123430252 Val Loss: tensor(54.4754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7791 Loss: 0.6580245494842529 Val Loss: tensor(54.6490, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7792 Loss: 0.6501211822032928 Val Loss: tensor(54.5195, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7793 Loss: 0.6466883569955826 Val Loss: tensor(54.6101, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7794 Loss: 0.6324247717857361 Val Loss: tensor(54.5522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7795 Loss: 0.6256280243396759 Val Loss: tensor(54.5593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7796 Loss: 0.6094115674495697 Val Loss: tensor(54.5738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7797 Loss: 0.6030854284763336 Val Loss: tensor(54.5057, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7798 Loss: 0.5894091427326202 Val Loss: tensor(54.5886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7799 Loss: 0.5868295282125473 Val Loss: tensor(54.4559, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7800 Loss: 0.5785044580698013 Val Loss: tensor(54.6025, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7801 Loss: 0.5816197395324707 Val Loss: tensor(54.4143, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7802 Loss: 0.5792383998632431 Val Loss: tensor(54.6182, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7803 Loss: 0.5884936898946762 Val Loss: tensor(54.3834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7804 Loss: 0.590695932507515 Val Loss: tensor(54.6341, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7805 Loss: 0.6049376577138901 Val Loss: tensor(54.3652, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7806 Loss: 0.609008178114891 Val Loss: tensor(54.6437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7807 Loss: 0.6252001374959946 Val Loss: tensor(54.3612, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7808 Loss: 0.6277641654014587 Val Loss: tensor(54.6374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7809 Loss: 0.6410980522632599 Val Loss: tensor(54.3720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7810 Loss: 0.6395155787467957 Val Loss: tensor(54.6063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7811 Loss: 0.6449587792158127 Val Loss: tensor(54.3966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7812 Loss: 0.6393342912197113 Val Loss: tensor(54.5487, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7813 Loss: 0.6352174580097198 Val Loss: tensor(54.4339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7814 Loss: 0.6296359002590179 Val Loss: tensor(54.4747, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7815 Loss: 0.6209040731191635 Val Loss: tensor(54.4859, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7816 Loss: 0.6224910616874695 Val Loss: tensor(54.4035, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7817 Loss: 0.620386004447937 Val Loss: tensor(54.5611, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7818 Loss: 0.6368786245584488 Val Loss: tensor(54.3552, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7819 Loss: 0.6548364013433456 Val Loss: tensor(54.6735, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7820 Loss: 0.6923975497484207 Val Loss: tensor(54.3436, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7821 Loss: 0.7406640499830246 Val Loss: tensor(54.8365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7822 Loss: 0.8007346987724304 Val Loss: tensor(54.3733, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7823 Loss: 0.8796330839395523 Val Loss: tensor(55.0479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7824 Loss: 0.9534614533185959 Val Loss: tensor(54.4359, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7825 Loss: 1.0448466688394547 Val Loss: tensor(55.2701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7826 Loss: 1.1080850660800934 Val Loss: tensor(54.5067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7827 Loss: 1.1715555638074875 Val Loss: tensor(55.4174, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7828 Loss: 1.1878298372030258 Val Loss: tensor(54.5546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7829 Loss: 1.1792672276496887 Val Loss: tensor(55.3890, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7830 Loss: 1.1249629855155945 Val Loss: tensor(54.5663, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7831 Loss: 1.0439250320196152 Val Loss: tensor(55.1736, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7832 Loss: 0.944660559296608 Val Loss: tensor(54.5511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7833 Loss: 0.8499497771263123 Val Loss: tensor(54.8995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7834 Loss: 0.7687857896089554 Val Loss: tensor(54.5340, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7835 Loss: 0.716996431350708 Val Loss: tensor(54.7082, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7836 Loss: 0.6890553534030914 Val Loss: tensor(54.5542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7837 Loss: 0.6894416660070419 Val Loss: tensor(54.6433, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7838 Loss: 0.6998583823442459 Val Loss: tensor(54.6274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7839 Loss: 0.7266474664211273 Val Loss: tensor(54.6612, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7840 Loss: 0.7406655251979828 Val Loss: tensor(54.7206, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7841 Loss: 0.7609983831644058 Val Loss: tensor(54.6829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7842 Loss: 0.75434410572052 Val Loss: tensor(54.7834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7843 Loss: 0.74839186668396 Val Loss: tensor(54.6587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7844 Loss: 0.7202982753515244 Val Loss: tensor(54.7993, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7845 Loss: 0.6938890516757965 Val Loss: tensor(54.5990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7846 Loss: 0.661332756280899 Val Loss: tensor(54.7826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7847 Loss: 0.6340077519416809 Val Loss: tensor(54.5418, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7848 Loss: 0.6096537560224533 Val Loss: tensor(54.7440, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7849 Loss: 0.590312048792839 Val Loss: tensor(54.5072, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7850 Loss: 0.5743739455938339 Val Loss: tensor(54.6898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7851 Loss: 0.5608595460653305 Val Loss: tensor(54.4929, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7852 Loss: 0.5500597953796387 Val Loss: tensor(54.6279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7853 Loss: 0.5397810339927673 Val Loss: tensor(54.4904, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7854 Loss: 0.5327516347169876 Val Loss: tensor(54.5673, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7855 Loss: 0.5250620692968369 Val Loss: tensor(54.4933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7856 Loss: 0.5214435011148453 Val Loss: tensor(54.5130, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7857 Loss: 0.5160022377967834 Val Loss: tensor(54.4995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7858 Loss: 0.5151554495096207 Val Loss: tensor(54.4655, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7859 Loss: 0.5116286426782608 Val Loss: tensor(54.5089, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7860 Loss: 0.5124845206737518 Val Loss: tensor(54.4252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7861 Loss: 0.5106092095375061 Val Loss: tensor(54.5211, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7862 Loss: 0.5120963305234909 Val Loss: tensor(54.3931, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7863 Loss: 0.5115206688642502 Val Loss: tensor(54.5342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7864 Loss: 0.5129435807466507 Val Loss: tensor(54.3695, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7865 Loss: 0.5133038014173508 Val Loss: tensor(54.5452, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7866 Loss: 0.5142954587936401 Val Loss: tensor(54.3527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7867 Loss: 0.515224888920784 Val Loss: tensor(54.5520, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7868 Loss: 0.5155456215143204 Val Loss: tensor(54.3412, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7869 Loss: 0.51678766310215 Val Loss: tensor(54.5533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7870 Loss: 0.5162527114152908 Val Loss: tensor(54.3347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7871 Loss: 0.5175700932741165 Val Loss: tensor(54.5488, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7872 Loss: 0.5161243677139282 Val Loss: tensor(54.3343, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7873 Loss: 0.517338290810585 Val Loss: tensor(54.5387, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7874 Loss: 0.5150764882564545 Val Loss: tensor(54.3407, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7875 Loss: 0.5161315351724625 Val Loss: tensor(54.5239, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7876 Loss: 0.5133485645055771 Val Loss: tensor(54.3542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7877 Loss: 0.5143076330423355 Val Loss: tensor(54.5049, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7878 Loss: 0.511493518948555 Val Loss: tensor(54.3747, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7879 Loss: 0.5126342624425888 Val Loss: tensor(54.4829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7880 Loss: 0.5103287845849991 Val Loss: tensor(54.4015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7881 Loss: 0.5120560973882675 Val Loss: tensor(54.4589, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7882 Loss: 0.510832667350769 Val Loss: tensor(54.4342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7883 Loss: 0.5136087834835052 Val Loss: tensor(54.4341, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7884 Loss: 0.5139196366071701 Val Loss: tensor(54.4715, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7885 Loss: 0.5181995183229446 Val Loss: tensor(54.4095, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7886 Loss: 0.5203027129173279 Val Loss: tensor(54.5118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7887 Loss: 0.526448205113411 Val Loss: tensor(54.3865, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7888 Loss: 0.5302724242210388 Val Loss: tensor(54.5529, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7889 Loss: 0.5383948236703873 Val Loss: tensor(54.3662, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7890 Loss: 0.5434199720621109 Val Loss: tensor(54.5918, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7891 Loss: 0.5531642436981201 Val Loss: tensor(54.3503, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7892 Loss: 0.5583776384592056 Val Loss: tensor(54.6249, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7893 Loss: 0.5687161982059479 Val Loss: tensor(54.3398, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7894 Loss: 0.5726549923419952 Val Loss: tensor(54.6478, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7895 Loss: 0.5817740857601166 Val Loss: tensor(54.3361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7896 Loss: 0.582938551902771 Val Loss: tensor(54.6565, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7897 Loss: 0.5885483771562576 Val Loss: tensor(54.3398, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7898 Loss: 0.5860457122325897 Val Loss: tensor(54.6479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7899 Loss: 0.5863279849290848 Val Loss: tensor(54.3510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7900 Loss: 0.5805705636739731 Val Loss: tensor(54.6222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7901 Loss: 0.5754655748605728 Val Loss: tensor(54.3698, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7902 Loss: 0.5684421509504318 Val Loss: tensor(54.5837, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7903 Loss: 0.5602977424860001 Val Loss: tensor(54.3967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7904 Loss: 0.5550732910633087 Val Loss: tensor(54.5401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7905 Loss: 0.5480905324220657 Val Loss: tensor(54.4343, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7906 Loss: 0.5478408485651016 Val Loss: tensor(54.4995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7907 Loss: 0.5466750711202621 Val Loss: tensor(54.4868, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7908 Loss: 0.5539929121732712 Val Loss: tensor(54.4685, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7909 Loss: 0.5623994022607803 Val Loss: tensor(54.5589, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7910 Loss: 0.5789324939250946 Val Loss: tensor(54.4515, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7911 Loss: 0.598800852894783 Val Loss: tensor(54.6532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7912 Loss: 0.6249383389949799 Val Loss: tensor(54.4507, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7913 Loss: 0.6555049121379852 Val Loss: tensor(54.7681, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7914 Loss: 0.6896549463272095 Val Loss: tensor(54.4658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7915 Loss: 0.7264963984489441 Val Loss: tensor(54.8946, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7916 Loss: 0.7639530748128891 Val Loss: tensor(54.4933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7917 Loss: 0.7985285371541977 Val Loss: tensor(55.0153, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7918 Loss: 0.8307483047246933 Val Loss: tensor(54.5262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7919 Loss: 0.8517788499593735 Val Loss: tensor(55.1042, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7920 Loss: 0.8679535388946533 Val Loss: tensor(54.5548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7921 Loss: 0.8665488213300705 Val Loss: tensor(55.1360, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7922 Loss: 0.8595997542142868 Val Loss: tensor(54.5687, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7923 Loss: 0.8366988748311996 Val Loss: tensor(55.1032, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7924 Loss: 0.81064273416996 Val Loss: tensor(54.5616, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7925 Loss: 0.7783161997795105 Val Loss: tensor(55.0254, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7926 Loss: 0.747187614440918 Val Loss: tensor(54.5384, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7927 Loss: 0.719668835401535 Val Loss: tensor(54.9335, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7928 Loss: 0.6959415078163147 Val Loss: tensor(54.5163, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7929 Loss: 0.6801634430885315 Val Loss: tensor(54.8479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7930 Loss: 0.6668956726789474 Val Loss: tensor(54.5109, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7931 Loss: 0.6620811969041824 Val Loss: tensor(54.7762, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7932 Loss: 0.6553463637828827 Val Loss: tensor(54.5241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7933 Loss: 0.6571008712053299 Val Loss: tensor(54.7194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7934 Loss: 0.6516961306333542 Val Loss: tensor(54.5463, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7935 Loss: 0.6550137847661972 Val Loss: tensor(54.6751, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7936 Loss: 0.6479041427373886 Val Loss: tensor(54.5657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7937 Loss: 0.6488573551177979 Val Loss: tensor(54.6394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7938 Loss: 0.639929473400116 Val Loss: tensor(54.5761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7939 Loss: 0.6372440904378891 Val Loss: tensor(54.6099, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7940 Loss: 0.6282666772603989 Val Loss: tensor(54.5766, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7941 Loss: 0.6232033520936966 Val Loss: tensor(54.5866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7942 Loss: 0.6161802113056183 Val Loss: tensor(54.5696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7943 Loss: 0.610772967338562 Val Loss: tensor(54.5721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7944 Loss: 0.6067687422037125 Val Loss: tensor(54.5566, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7945 Loss: 0.6023242622613907 Val Loss: tensor(54.5685, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7946 Loss: 0.6014849692583084 Val Loss: tensor(54.5381, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7947 Loss: 0.598510131239891 Val Loss: tensor(54.5768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7948 Loss: 0.6004441380500793 Val Loss: tensor(54.5144, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7949 Loss: 0.5990894287824631 Val Loss: tensor(54.5959, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7950 Loss: 0.60320183634758 Val Loss: tensor(54.4861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7951 Loss: 0.6035401523113251 Val Loss: tensor(54.6238, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7952 Loss: 0.6091563105583191 Val Loss: tensor(54.4547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7953 Loss: 0.6111878007650375 Val Loss: tensor(54.6571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7954 Loss: 0.6175889074802399 Val Loss: tensor(54.4222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7955 Loss: 0.6210203766822815 Val Loss: tensor(54.6917, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7956 Loss: 0.6274533420801163 Val Loss: tensor(54.3916, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7957 Loss: 0.6315968483686447 Val Loss: tensor(54.7230, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7958 Loss: 0.6371960341930389 Val Loss: tensor(54.3652, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7959 Loss: 0.6409703195095062 Val Loss: tensor(54.7457, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7960 Loss: 0.6447115689516068 Val Loss: tensor(54.3447, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7961 Loss: 0.6468835771083832 Val Loss: tensor(54.7556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7962 Loss: 0.6477511823177338 Val Loss: tensor(54.3310, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7963 Loss: 0.6473206728696823 Val Loss: tensor(54.7502, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7964 Loss: 0.6445719301700592 Val Loss: tensor(54.3238, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7965 Loss: 0.6411301046609879 Val Loss: tensor(54.7297, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7966 Loss: 0.6346558183431625 Val Loss: tensor(54.3222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7967 Loss: 0.628674179315567 Val Loss: tensor(54.6974, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7968 Loss: 0.6192140132188797 Val Loss: tensor(54.3257, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7969 Loss: 0.6118584722280502 Val Loss: tensor(54.6585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7970 Loss: 0.600762352347374 Val Loss: tensor(54.3337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7971 Loss: 0.5933937281370163 Val Loss: tensor(54.6190, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7972 Loss: 0.5821696072816849 Val Loss: tensor(54.3461, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7973 Loss: 0.5759180337190628 Val Loss: tensor(54.5831, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7974 Loss: 0.5657330006361008 Val Loss: tensor(54.3630, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7975 Loss: 0.56118343770504 Val Loss: tensor(54.5533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7976 Loss: 0.5526376962661743 Val Loss: tensor(54.3843, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7977 Loss: 0.5497953593730927 Val Loss: tensor(54.5299, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7978 Loss: 0.5429874956607819 Val Loss: tensor(54.4090, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7979 Loss: 0.5414458215236664 Val Loss: tensor(54.5115, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7980 Loss: 0.5361276268959045 Val Loss: tensor(54.4359, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7981 Loss: 0.5353343188762665 Val Loss: tensor(54.4961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7982 Loss: 0.531156063079834 Val Loss: tensor(54.4634, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7983 Loss: 0.5306292772293091 Val Loss: tensor(54.4816, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7984 Loss: 0.5272710621356964 Val Loss: tensor(54.4902, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7985 Loss: 0.5267399847507477 Val Loss: tensor(54.4668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7986 Loss: 0.5239999294281006 Val Loss: tensor(54.5149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7987 Loss: 0.5234801024198532 Val Loss: tensor(54.4508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7988 Loss: 0.5212503373622894 Val Loss: tensor(54.5366, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7989 Loss: 0.5208505988121033 Val Loss: tensor(54.4339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7990 Loss: 0.5190533101558685 Val Loss: tensor(54.5544, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7991 Loss: 0.5189125686883926 Val Loss: tensor(54.4168, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7992 Loss: 0.5174859762191772 Val Loss: tensor(54.5678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7993 Loss: 0.5175912082195282 Val Loss: tensor(54.4008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7994 Loss: 0.5164131820201874 Val Loss: tensor(54.5761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7995 Loss: 0.5165678262710571 Val Loss: tensor(54.3871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7996 Loss: 0.5155529379844666 Val Loss: tensor(54.5791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7997 Loss: 0.5154265612363815 Val Loss: tensor(54.3768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7998 Loss: 0.514485627412796 Val Loss: tensor(54.5763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7999 Loss: 0.5137683004140854 Val Loss: tensor(54.3709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8000 Loss: 0.5129116028547287 Val Loss: tensor(54.5679, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8001 Loss: 0.5114448368549347 Val Loss: tensor(54.3706, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8002 Loss: 0.5108116865158081 Val Loss: tensor(54.5543, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8003 Loss: 0.5087385922670364 Val Loss: tensor(54.3765, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8004 Loss: 0.5086135268211365 Val Loss: tensor(54.5363, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8005 Loss: 0.506426990032196 Val Loss: tensor(54.3896, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8006 Loss: 0.5072392821311951 Val Loss: tensor(54.5153, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8007 Loss: 0.5058401972055435 Val Loss: tensor(54.4107, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8008 Loss: 0.5081973969936371 Val Loss: tensor(54.4933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8009 Loss: 0.5088279098272324 Val Loss: tensor(54.4410, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8010 Loss: 0.5134769529104233 Val Loss: tensor(54.4724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8011 Loss: 0.517579197883606 Val Loss: tensor(54.4818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8012 Loss: 0.5254542678594589 Val Loss: tensor(54.4547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8013 Loss: 0.5344865173101425 Val Loss: tensor(54.5348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8014 Loss: 0.5466907471418381 Val Loss: tensor(54.4422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8015 Loss: 0.5618585646152496 Val Loss: tensor(54.6015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8016 Loss: 0.5795455425977707 Val Loss: tensor(54.4367, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8017 Loss: 0.6013505458831787 Val Loss: tensor(54.6827, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8018 Loss: 0.6254480630159378 Val Loss: tensor(54.4403, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8019 Loss: 0.6529753804206848 Val Loss: tensor(54.7772, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8020 Loss: 0.683483213186264 Val Loss: tensor(54.4548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8021 Loss: 0.7135288268327713 Val Loss: tensor(54.8792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8022 Loss: 0.7481995820999146 Val Loss: tensor(54.4813, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8023 Loss: 0.7745164930820465 Val Loss: tensor(54.9746, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8024 Loss: 0.8073654770851135 Val Loss: tensor(54.5189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8025 Loss: 0.8212305903434753 Val Loss: tensor(55.0402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8026 Loss: 0.8424762338399887 Val Loss: tensor(54.5610, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8027 Loss: 0.8365311771631241 Val Loss: tensor(55.0523, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8028 Loss: 0.8370723724365234 Val Loss: tensor(54.5949, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8029 Loss: 0.8118370622396469 Val Loss: tensor(55.0062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8030 Loss: 0.7913771569728851 Val Loss: tensor(54.6071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8031 Loss: 0.757032960653305 Val Loss: tensor(54.9256, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8032 Loss: 0.7262328118085861 Val Loss: tensor(54.5943, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8033 Loss: 0.6951659470796585 Val Loss: tensor(54.8449, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8034 Loss: 0.6667922288179398 Val Loss: tensor(54.5692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8035 Loss: 0.6457822173833847 Val Loss: tensor(54.7832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8036 Loss: 0.6261741518974304 Val Loss: tensor(54.5518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8037 Loss: 0.615673691034317 Val Loss: tensor(54.7404, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8038 Loss: 0.6044439822435379 Val Loss: tensor(54.5552, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8039 Loss: 0.6015673130750656 Val Loss: tensor(54.7075, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8040 Loss: 0.5953076481819153 Val Loss: tensor(54.5794, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8041 Loss: 0.5962057262659073 Val Loss: tensor(54.6750, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8042 Loss: 0.5914697200059891 Val Loss: tensor(54.6161, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8043 Loss: 0.5927855968475342 Val Loss: tensor(54.6382, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8044 Loss: 0.5876402407884598 Val Loss: tensor(54.6556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8045 Loss: 0.5876038819551468 Val Loss: tensor(54.5969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8046 Loss: 0.5819906741380692 Val Loss: tensor(54.6912, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8047 Loss: 0.5809079855680466 Val Loss: tensor(54.5553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8048 Loss: 0.5759599953889847 Val Loss: tensor(54.7197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8049 Loss: 0.575155183672905 Val Loss: tensor(54.5197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8050 Loss: 0.5720288306474686 Val Loss: tensor(54.7396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8051 Loss: 0.5724208056926727 Val Loss: tensor(54.4955, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8052 Loss: 0.5717082321643829 Val Loss: tensor(54.7500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8053 Loss: 0.5733596086502075 Val Loss: tensor(54.4861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8054 Loss: 0.5752218663692474 Val Loss: tensor(54.7507, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8055 Loss: 0.5779734998941422 Val Loss: tensor(54.4935, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8056 Loss: 0.5825866758823395 Val Loss: tensor(54.7427, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8057 Loss: 0.5868190824985504 Val Loss: tensor(54.5188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8058 Loss: 0.5945145189762115 Val Loss: tensor(54.7275, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8059 Loss: 0.6014367043972015 Val Loss: tensor(54.5631, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8060 Loss: 0.6124956458806992 Val Loss: tensor(54.7071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8061 Loss: 0.6239388287067413 Val Loss: tensor(54.6274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8062 Loss: 0.6383744180202484 Val Loss: tensor(54.6840, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8063 Loss: 0.6561222523450851 Val Loss: tensor(54.7123, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8064 Loss: 0.6735572963953018 Val Loss: tensor(54.6604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8065 Loss: 0.6983630061149597 Val Loss: tensor(54.8164, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8066 Loss: 0.7179016619920731 Val Loss: tensor(54.6379, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8067 Loss: 0.7484474033117294 Val Loss: tensor(54.9344, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8068 Loss: 0.7684309184551239 Val Loss: tensor(54.6163, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8069 Loss: 0.8004985302686691 Val Loss: tensor(55.0535, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8070 Loss: 0.8182192295789719 Val Loss: tensor(54.5954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8071 Loss: 0.8445072025060654 Val Loss: tensor(55.1508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8072 Loss: 0.8556723892688751 Val Loss: tensor(54.5761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8073 Loss: 0.867162436246872 Val Loss: tensor(55.1963, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8074 Loss: 0.8659924268722534 Val Loss: tensor(54.5598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8075 Loss: 0.8559920191764832 Val Loss: tensor(55.1664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8076 Loss: 0.8379597812891006 Val Loss: tensor(54.5444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8077 Loss: 0.8080195933580399 Val Loss: tensor(55.0658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8078 Loss: 0.7753692269325256 Val Loss: tensor(54.5247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8079 Loss: 0.7363196462392807 Val Loss: tensor(54.9313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8080 Loss: 0.6991386860609055 Val Loss: tensor(54.4997, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8081 Loss: 0.6632878333330154 Val Loss: tensor(54.8057, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8082 Loss: 0.6317175030708313 Val Loss: tensor(54.4757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8083 Loss: 0.6050016731023788 Val Loss: tensor(54.7110, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8084 Loss: 0.5826771408319473 Val Loss: tensor(54.4601, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8085 Loss: 0.5647094547748566 Val Loss: tensor(54.6467, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8086 Loss: 0.5500195622444153 Val Loss: tensor(54.4557, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8087 Loss: 0.5382136702537537 Val Loss: tensor(54.6022, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8088 Loss: 0.5283616632223129 Val Loss: tensor(54.4607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8089 Loss: 0.5204509049654007 Val Loss: tensor(54.5684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8090 Loss: 0.5134302973747253 Val Loss: tensor(54.4720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8091 Loss: 0.5079333484172821 Val Loss: tensor(54.5399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8092 Loss: 0.5027599781751633 Val Loss: tensor(54.4869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8093 Loss: 0.4989067614078522 Val Loss: tensor(54.5143, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8094 Loss: 0.49516811966896057 Val Loss: tensor(54.5031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8095 Loss: 0.4926408678293228 Val Loss: tensor(54.4910, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8096 Loss: 0.4901742488145828 Val Loss: tensor(54.5190, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8097 Loss: 0.48879463970661163 Val Loss: tensor(54.4703, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8098 Loss: 0.4874492734670639 Val Loss: tensor(54.5332, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8099 Loss: 0.48698219656944275 Val Loss: tensor(54.4521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8100 Loss: 0.4865744858980179 Val Loss: tensor(54.5447, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8101 Loss: 0.48674166202545166 Val Loss: tensor(54.4363, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8102 Loss: 0.4870673716068268 Val Loss: tensor(54.5535, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8103 Loss: 0.4875448942184448 Val Loss: tensor(54.4228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8104 Loss: 0.48834970593452454 Val Loss: tensor(54.5593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8105 Loss: 0.48888954520225525 Val Loss: tensor(54.4118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8106 Loss: 0.4899598956108093 Val Loss: tensor(54.5622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8107 Loss: 0.490339919924736 Val Loss: tensor(54.4035, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8108 Loss: 0.49152955412864685 Val Loss: tensor(54.5619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8109 Loss: 0.49158546328544617 Val Loss: tensor(54.3984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8110 Loss: 0.4928033649921417 Val Loss: tensor(54.5581, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8111 Loss: 0.4924289137125015 Val Loss: tensor(54.3968, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8112 Loss: 0.49368591606616974 Val Loss: tensor(54.5507, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8113 Loss: 0.49291421473026276 Val Loss: tensor(54.3995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8114 Loss: 0.49428533017635345 Val Loss: tensor(54.5398, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8115 Loss: 0.4933096468448639 Val Loss: tensor(54.4069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8116 Loss: 0.49499499797821045 Val Loss: tensor(54.5257, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8117 Loss: 0.494226410984993 Val Loss: tensor(54.4201, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8118 Loss: 0.4965304285287857 Val Loss: tensor(54.5092, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8119 Loss: 0.49660617113113403 Val Loss: tensor(54.4401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8120 Loss: 0.4999663084745407 Val Loss: tensor(54.4908, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8121 Loss: 0.5017974823713303 Val Loss: tensor(54.4683, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8122 Loss: 0.5068390369415283 Val Loss: tensor(54.4718, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8123 Loss: 0.5115978121757507 Val Loss: tensor(54.5063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8124 Loss: 0.5191235393285751 Val Loss: tensor(54.4536, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8125 Loss: 0.5281213372945786 Val Loss: tensor(54.5561, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8126 Loss: 0.5391543656587601 Val Loss: tensor(54.4381, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8127 Loss: 0.5536982119083405 Val Loss: tensor(54.6196, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8128 Loss: 0.5694246739149094 Val Loss: tensor(54.4273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8129 Loss: 0.5904495716094971 Val Loss: tensor(54.6984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8130 Loss: 0.6119852811098099 Val Loss: tensor(54.4237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8131 Loss: 0.6394185870885849 Val Loss: tensor(54.7920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8132 Loss: 0.6672584414482117 Val Loss: tensor(54.4304, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8133 Loss: 0.6990449875593185 Val Loss: tensor(54.8958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8134 Loss: 0.7319821268320084 Val Loss: tensor(54.4505, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8135 Loss: 0.7629223018884659 Val Loss: tensor(54.9970, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8136 Loss: 0.7963540256023407 Val Loss: tensor(54.4852, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8137 Loss: 0.8176658153533936 Val Loss: tensor(55.0717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8138 Loss: 0.8425204902887344 Val Loss: tensor(54.5309, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8139 Loss: 0.8444308042526245 Val Loss: tensor(55.0911, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8140 Loss: 0.8497855514287949 Val Loss: tensor(54.5758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8141 Loss: 0.8287018090486526 Val Loss: tensor(55.0429, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8142 Loss: 0.81035515666008 Val Loss: tensor(54.6033, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8143 Loss: 0.7746671885251999 Val Loss: tensor(54.9501, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8144 Loss: 0.7414843142032623 Val Loss: tensor(54.6036, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8145 Loss: 0.705856204032898 Val Loss: tensor(54.8569, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8146 Loss: 0.6727284342050552 Val Loss: tensor(54.5828, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8147 Loss: 0.6471923738718033 Val Loss: tensor(54.7934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8148 Loss: 0.6232167035341263 Val Loss: tensor(54.5598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8149 Loss: 0.6099613159894943 Val Loss: tensor(54.7627, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8150 Loss: 0.5962334126234055 Val Loss: tensor(54.5530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8151 Loss: 0.5928010195493698 Val Loss: tensor(54.7512, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8152 Loss: 0.5863939225673676 Val Loss: tensor(54.5701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8153 Loss: 0.5886890441179276 Val Loss: tensor(54.7432, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8154 Loss: 0.5859650820493698 Val Loss: tensor(54.6079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8155 Loss: 0.5895396620035172 Val Loss: tensor(54.7268, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8156 Loss: 0.5875501781702042 Val Loss: tensor(54.6584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8157 Loss: 0.5890019536018372 Val Loss: tensor(54.6953, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8158 Loss: 0.5862383395433426 Val Loss: tensor(54.7144, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8159 Loss: 0.5849651098251343 Val Loss: tensor(54.6489, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8160 Loss: 0.5817845612764359 Val Loss: tensor(54.7715, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8161 Loss: 0.5802494138479233 Val Loss: tensor(54.5961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8162 Loss: 0.5781598538160324 Val Loss: tensor(54.8265, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8163 Loss: 0.5793764740228653 Val Loss: tensor(54.5483, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8164 Loss: 0.5792296975851059 Val Loss: tensor(54.8742, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8165 Loss: 0.5840611308813095 Val Loss: tensor(54.5146, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8166 Loss: 0.5851093977689743 Val Loss: tensor(54.9077, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8167 Loss: 0.5918851494789124 Val Loss: tensor(54.4980, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8168 Loss: 0.5924736708402634 Val Loss: tensor(54.9222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8169 Loss: 0.598662793636322 Val Loss: tensor(54.4968, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8170 Loss: 0.5978188514709473 Val Loss: tensor(54.9173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8171 Loss: 0.6016772240400314 Val Loss: tensor(54.5071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8172 Loss: 0.5999176204204559 Val Loss: tensor(54.8970, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8173 Loss: 0.6013125777244568 Val Loss: tensor(54.5254, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8174 Loss: 0.6001758873462677 Val Loss: tensor(54.8680, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8175 Loss: 0.600459098815918 Val Loss: tensor(54.5503, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8176 Loss: 0.6015820503234863 Val Loss: tensor(54.8376, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8177 Loss: 0.6028762757778168 Val Loss: tensor(54.5825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8178 Loss: 0.6072193831205368 Val Loss: tensor(54.8117, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8179 Loss: 0.611541822552681 Val Loss: tensor(54.6241, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8180 Loss: 0.6191319525241852 Val Loss: tensor(54.7931, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8181 Loss: 0.6275913566350937 Val Loss: tensor(54.6770, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8182 Loss: 0.6375907063484192 Val Loss: tensor(54.7805, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8183 Loss: 0.6502560079097748 Val Loss: tensor(54.7415, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8184 Loss: 0.6612834483385086 Val Loss: tensor(54.7696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8185 Loss: 0.6772945523262024 Val Loss: tensor(54.8157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8186 Loss: 0.6876798272132874 Val Loss: tensor(54.7549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8187 Loss: 0.7053525894880295 Val Loss: tensor(54.8939, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8188 Loss: 0.7134534120559692 Val Loss: tensor(54.7325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8189 Loss: 0.7303347736597061 Val Loss: tensor(54.9665, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8190 Loss: 0.7346905916929245 Val Loss: tensor(54.7020, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8191 Loss: 0.7477596551179886 Val Loss: tensor(55.0203, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8192 Loss: 0.7471116036176682 Val Loss: tensor(54.6664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8193 Loss: 0.7533117532730103 Val Loss: tensor(55.0421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8194 Loss: 0.7466352134943008 Val Loss: tensor(54.6295, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8195 Loss: 0.7440954595804214 Val Loss: tensor(55.0246, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8196 Loss: 0.7311472743749619 Val Loss: tensor(54.5946, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8197 Loss: 0.7203786969184875 Val Loss: tensor(54.9702, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8198 Loss: 0.7023890465497971 Val Loss: tensor(54.5632, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8199 Loss: 0.6863319128751755 Val Loss: tensor(54.8913, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8200 Loss: 0.665906235575676 Val Loss: tensor(54.5367, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8201 Loss: 0.6484907269477844 Val Loss: tensor(54.8041, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8202 Loss: 0.6286649405956268 Val Loss: tensor(54.5168, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8203 Loss: 0.6129584163427353 Val Loss: tensor(54.7224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8204 Loss: 0.595828652381897 Val Loss: tensor(54.5048, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8205 Loss: 0.5833363980054855 Val Loss: tensor(54.6528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8206 Loss: 0.5697355270385742 Val Loss: tensor(54.5010, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8207 Loss: 0.5607014298439026 Val Loss: tensor(54.5968, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8208 Loss: 0.5505517274141312 Val Loss: tensor(54.5044, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8209 Loss: 0.5446835309267044 Val Loss: tensor(54.5528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8210 Loss: 0.5375439822673798 Val Loss: tensor(54.5136, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8211 Loss: 0.5343903005123138 Val Loss: tensor(54.5181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8212 Loss: 0.5298178791999817 Val Loss: tensor(54.5274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8213 Loss: 0.528985932469368 Val Loss: tensor(54.4908, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8214 Loss: 0.5265888124704361 Val Loss: tensor(54.5438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8215 Loss: 0.5277888476848602 Val Loss: tensor(54.4691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8216 Loss: 0.5272464901208878 Val Loss: tensor(54.5612, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8217 Loss: 0.5301768183708191 Val Loss: tensor(54.4524, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8218 Loss: 0.5311628878116608 Val Loss: tensor(54.5779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8219 Loss: 0.5355193763971329 Val Loss: tensor(54.4402, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8220 Loss: 0.5376475602388382 Val Loss: tensor(54.5925, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8221 Loss: 0.5430634468793869 Val Loss: tensor(54.4320, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8222 Loss: 0.545912891626358 Val Loss: tensor(54.6039, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8223 Loss: 0.551932767033577 Val Loss: tensor(54.4278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8224 Loss: 0.5550599098205566 Val Loss: tensor(54.6111, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8225 Loss: 0.5611315667629242 Val Loss: tensor(54.4272, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8226 Loss: 0.5641369074583054 Val Loss: tensor(54.6131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8227 Loss: 0.5696218311786652 Val Loss: tensor(54.4298, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8228 Loss: 0.5721953064203262 Val Loss: tensor(54.6089, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8229 Loss: 0.5764315277338028 Val Loss: tensor(54.4353, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8230 Loss: 0.5784212350845337 Val Loss: tensor(54.5980, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8231 Loss: 0.580858513712883 Val Loss: tensor(54.4433, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8232 Loss: 0.5823292583227158 Val Loss: tensor(54.5802, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8233 Loss: 0.5827211737632751 Val Loss: tensor(54.4539, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8234 Loss: 0.5839301496744156 Val Loss: tensor(54.5563, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8235 Loss: 0.5825555771589279 Val Loss: tensor(54.4676, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8236 Loss: 0.5839808583259583 Val Loss: tensor(54.5273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8237 Loss: 0.5817287266254425 Val Loss: tensor(54.4860, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8238 Loss: 0.5840010493993759 Val Loss: tensor(54.4950, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8239 Loss: 0.5823987573385239 Val Loss: tensor(54.5118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8240 Loss: 0.5862544775009155 Val Loss: tensor(54.4608, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8241 Loss: 0.5873464047908783 Val Loss: tensor(54.5486, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8242 Loss: 0.5936647206544876 Val Loss: tensor(54.4264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8243 Loss: 0.5997800081968307 Val Loss: tensor(54.6000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8244 Loss: 0.6095454543828964 Val Loss: tensor(54.3936, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8245 Loss: 0.6229759156703949 Val Loss: tensor(54.6693, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8246 Loss: 0.637287437915802 Val Loss: tensor(54.3647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8247 Loss: 0.6597342193126678 Val Loss: tensor(54.7575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8248 Loss: 0.6797382533550262 Val Loss: tensor(54.3437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8249 Loss: 0.7111557275056839 Val Loss: tensor(54.8618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8250 Loss: 0.7374388873577118 Val Loss: tensor(54.3357, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8251 Loss: 0.7743386477231979 Val Loss: tensor(54.9712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8252 Loss: 0.8053457587957382 Val Loss: tensor(54.3472, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8253 Loss: 0.838960200548172 Val Loss: tensor(55.0635, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8254 Loss: 0.8686094880104065 Val Loss: tensor(54.3812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8255 Loss: 0.8856550008058548 Val Loss: tensor(55.1067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8256 Loss: 0.9024638831615448 Val Loss: tensor(54.4322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8257 Loss: 0.8924710303544998 Val Loss: tensor(55.0779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8258 Loss: 0.8857966363430023 Val Loss: tensor(54.4843, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8259 Loss: 0.8521464020013809 Val Loss: tensor(54.9901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8260 Loss: 0.8238924592733383 Val Loss: tensor(54.5189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8261 Loss: 0.7835399210453033 Val Loss: tensor(54.8890, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8262 Loss: 0.7482400238513947 Val Loss: tensor(54.5293, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8263 Loss: 0.715773731470108 Val Loss: tensor(54.8099, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8264 Loss: 0.6854948848485947 Val Loss: tensor(54.5230, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8265 Loss: 0.6637181490659714 Val Loss: tensor(54.7546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8266 Loss: 0.640731930732727 Val Loss: tensor(54.5128, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8267 Loss: 0.6259054243564606 Val Loss: tensor(54.7100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8268 Loss: 0.6079215705394745 Val Loss: tensor(54.5055, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8269 Loss: 0.5966339260339737 Val Loss: tensor(54.6690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8270 Loss: 0.5814763307571411 Val Loss: tensor(54.5012, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8271 Loss: 0.572152704000473 Val Loss: tensor(54.6324, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8272 Loss: 0.5587692260742188 Val Loss: tensor(54.4980, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8273 Loss: 0.5507844984531403 Val Loss: tensor(54.6014, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8274 Loss: 0.5388860106468201 Val Loss: tensor(54.4956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8275 Loss: 0.5320923626422882 Val Loss: tensor(54.5753, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8276 Loss: 0.5218127816915512 Val Loss: tensor(54.4948, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8277 Loss: 0.5163587182760239 Val Loss: tensor(54.5531, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8278 Loss: 0.5079439729452133 Val Loss: tensor(54.4964, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8279 Loss: 0.504026398062706 Val Loss: tensor(54.5346, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8280 Loss: 0.4975622296333313 Val Loss: tensor(54.5005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8281 Loss: 0.49514245986938477 Val Loss: tensor(54.5189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8282 Loss: 0.49049054086208344 Val Loss: tensor(54.5067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8283 Loss: 0.4893503040075302 Val Loss: tensor(54.5051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8284 Loss: 0.48625892400741577 Val Loss: tensor(54.5149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8285 Loss: 0.48615163564682007 Val Loss: tensor(54.4926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8286 Loss: 0.48438872396945953 Val Loss: tensor(54.5248, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8287 Loss: 0.4851287454366684 Val Loss: tensor(54.4812, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8288 Loss: 0.48456384241580963 Val Loss: tensor(54.5363, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8289 Loss: 0.486087441444397 Val Loss: tensor(54.4704, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8290 Loss: 0.4867178648710251 Val Loss: tensor(54.5500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8291 Loss: 0.4890804588794708 Val Loss: tensor(54.4601, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8292 Loss: 0.4910208135843277 Val Loss: tensor(54.5667, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8293 Loss: 0.4943956732749939 Val Loss: tensor(54.4502, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8294 Loss: 0.49786512553691864 Val Loss: tensor(54.5872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8295 Loss: 0.5025403797626495 Val Loss: tensor(54.4408, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8296 Loss: 0.5078114867210388 Val Loss: tensor(54.6126, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8297 Loss: 0.5141807794570923 Val Loss: tensor(54.4322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8298 Loss: 0.5215410441160202 Val Loss: tensor(54.6437, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8299 Loss: 0.5300833433866501 Val Loss: tensor(54.4253, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8300 Loss: 0.539743110537529 Val Loss: tensor(54.6810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8301 Loss: 0.5509155243635178 Val Loss: tensor(54.4213, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8302 Loss: 0.5628348886966705 Val Loss: tensor(54.7244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8303 Loss: 0.5769638121128082 Val Loss: tensor(54.4214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8304 Loss: 0.5907002836465836 Val Loss: tensor(54.7727, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8305 Loss: 0.6076452881097794 Val Loss: tensor(54.4268, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8306 Loss: 0.6220531016588211 Val Loss: tensor(54.8228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8307 Loss: 0.6409097164869308 Val Loss: tensor(54.4383, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8308 Loss: 0.6540921926498413 Val Loss: tensor(54.8696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8309 Loss: 0.6727997809648514 Val Loss: tensor(54.4553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8310 Loss: 0.6822881996631622 Val Loss: tensor(54.9066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8311 Loss: 0.6977468580007553 Val Loss: tensor(54.4757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8312 Loss: 0.7011716365814209 Val Loss: tensor(54.9268, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8313 Loss: 0.7100340873003006 Val Loss: tensor(54.4949, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8314 Loss: 0.7062423080205917 Val Loss: tensor(54.9272, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8315 Loss: 0.7065065950155258 Val Loss: tensor(54.5081, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8316 Loss: 0.6962551325559616 Val Loss: tensor(54.9086, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8317 Loss: 0.6883742213249207 Val Loss: tensor(54.5120, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8318 Loss: 0.6741527318954468 Val Loss: tensor(54.8767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8319 Loss: 0.6608493328094482 Val Loss: tensor(54.5066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8320 Loss: 0.6455953270196915 Val Loss: tensor(54.8380, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8321 Loss: 0.6303434520959854 Val Loss: tensor(54.4957, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8322 Loss: 0.6164011657238007 Val Loss: tensor(54.7976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8323 Loss: 0.6018972098827362 Val Loss: tensor(54.4849, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8324 Loss: 0.5905844420194626 Val Loss: tensor(54.7587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8325 Loss: 0.5782305598258972 Val Loss: tensor(54.4791, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8326 Loss: 0.5701119750738144 Val Loss: tensor(54.7230, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8327 Loss: 0.5603916049003601 Val Loss: tensor(54.4815, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8328 Loss: 0.5556478947401047 Val Loss: tensor(54.6917, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8329 Loss: 0.5486745536327362 Val Loss: tensor(54.4933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8330 Loss: 0.5473207086324692 Val Loss: tensor(54.6657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8331 Loss: 0.5432519614696503 Val Loss: tensor(54.5149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8332 Loss: 0.5452741980552673 Val Loss: tensor(54.6456, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8333 Loss: 0.5443809032440186 Val Loss: tensor(54.5464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8334 Loss: 0.5497163832187653 Val Loss: tensor(54.6313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8335 Loss: 0.5522681325674057 Val Loss: tensor(54.5878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8336 Loss: 0.5608595311641693 Val Loss: tensor(54.6220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8337 Loss: 0.5668426752090454 Val Loss: tensor(54.6389, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8338 Loss: 0.5786366760730743 Val Loss: tensor(54.6160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8339 Loss: 0.587592825293541 Val Loss: tensor(54.6989, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8340 Loss: 0.6025818437337875 Val Loss: tensor(54.6111, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8341 Loss: 0.6133828312158585 Val Loss: tensor(54.7659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8342 Loss: 0.6315426230430603 Val Loss: tensor(54.6055, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8343 Loss: 0.6424066424369812 Val Loss: tensor(54.8356, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8344 Loss: 0.6633174866437912 Val Loss: tensor(54.5984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8345 Loss: 0.671804204583168 Val Loss: tensor(54.9014, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8346 Loss: 0.6937341243028641 Val Loss: tensor(54.5907, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8347 Loss: 0.6968935877084732 Val Loss: tensor(54.9532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8348 Loss: 0.7160731703042984 Val Loss: tensor(54.5823, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8349 Loss: 0.7111233621835709 Val Loss: tensor(54.9811, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8350 Loss: 0.7225810885429382 Val Loss: tensor(54.5719, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8351 Loss: 0.708452582359314 Val Loss: tensor(54.9794, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8352 Loss: 0.7088907957077026 Val Loss: tensor(54.5583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8353 Loss: 0.6877143085002899 Val Loss: tensor(54.9508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8354 Loss: 0.6782345473766327 Val Loss: tensor(54.5413, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8355 Loss: 0.654632642865181 Val Loss: tensor(54.9046, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8356 Loss: 0.6400838941335678 Val Loss: tensor(54.5228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8357 Loss: 0.6184958964586258 Val Loss: tensor(54.8511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8358 Loss: 0.6039599478244781 Val Loss: tensor(54.5054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8359 Loss: 0.5866303592920303 Val Loss: tensor(54.7976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8360 Loss: 0.5750360488891602 Val Loss: tensor(54.4909, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8361 Loss: 0.562272921204567 Val Loss: tensor(54.7486, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8362 Loss: 0.5544170290231705 Val Loss: tensor(54.4808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8363 Loss: 0.5457571893930435 Val Loss: tensor(54.7071, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8364 Loss: 0.5413594543933868 Val Loss: tensor(54.4764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8365 Loss: 0.5362475514411926 Val Loss: tensor(54.6746, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8366 Loss: 0.5347650051116943 Val Loss: tensor(54.4782, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8367 Loss: 0.5327911376953125 Val Loss: tensor(54.6515, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8368 Loss: 0.5337631702423096 Val Loss: tensor(54.4863, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8369 Loss: 0.5346152037382126 Val Loss: tensor(54.6374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8370 Loss: 0.5377990901470184 Val Loss: tensor(54.5001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8371 Loss: 0.5413129180669785 Val Loss: tensor(54.6316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8372 Loss: 0.546639546751976 Val Loss: tensor(54.5185, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8373 Loss: 0.5528636276721954 Val Loss: tensor(54.6342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8374 Loss: 0.5604053139686584 Val Loss: tensor(54.5403, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8375 Loss: 0.569597989320755 Val Loss: tensor(54.6461, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8376 Loss: 0.5794952362775803 Val Loss: tensor(54.5649, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8377 Loss: 0.5921237021684647 Val Loss: tensor(54.6695, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8378 Loss: 0.6045439392328262 Val Loss: tensor(54.5910, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8379 Loss: 0.6211367696523666 Val Loss: tensor(54.7068, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8380 Loss: 0.6361062824726105 Val Loss: tensor(54.6171, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8381 Loss: 0.6570307016372681 Val Loss: tensor(54.7602, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8382 Loss: 0.674289658665657 Val Loss: tensor(54.6400, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8383 Loss: 0.6992796361446381 Val Loss: tensor(54.8305, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8384 Loss: 0.7179083824157715 Val Loss: tensor(54.6553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8385 Loss: 0.7455507814884186 Val Loss: tensor(54.9154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8386 Loss: 0.7639413923025131 Val Loss: tensor(54.6573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8387 Loss: 0.7911668121814728 Val Loss: tensor(55.0073, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8388 Loss: 0.8071028143167496 Val Loss: tensor(54.6420, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8389 Loss: 0.8293732702732086 Val Loss: tensor(55.0922, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8390 Loss: 0.8405246436595917 Val Loss: tensor(54.6102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8391 Loss: 0.8525895178318024 Val Loss: tensor(55.1504, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8392 Loss: 0.8566724210977554 Val Loss: tensor(54.5684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8393 Loss: 0.8539117127656937 Val Loss: tensor(55.1605, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8394 Loss: 0.8484872132539749 Val Loss: tensor(54.5262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8395 Loss: 0.828788161277771 Val Loss: tensor(55.1097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8396 Loss: 0.8119737356901169 Val Loss: tensor(54.4897, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8397 Loss: 0.7780092358589172 Val Loss: tensor(55.0044, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8398 Loss: 0.750996932387352 Val Loss: tensor(54.4600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8399 Loss: 0.7105973809957504 Val Loss: tensor(54.8706, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8400 Loss: 0.6790632754564285 Val Loss: tensor(54.4368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8401 Loss: 0.6413493901491165 Val Loss: tensor(54.7406, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8402 Loss: 0.6125602126121521 Val Loss: tensor(54.4221, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8403 Loss: 0.583249643445015 Val Loss: tensor(54.6365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8404 Loss: 0.5616851449012756 Val Loss: tensor(54.4182, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8405 Loss: 0.5420152544975281 Val Loss: tensor(54.5646, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8406 Loss: 0.5283041447401047 Val Loss: tensor(54.4255, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8407 Loss: 0.5169002711772919 Val Loss: tensor(54.5201, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8408 Loss: 0.5094550848007202 Val Loss: tensor(54.4419, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8409 Loss: 0.5041345208883286 Val Loss: tensor(54.4942, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8410 Loss: 0.5009186118841171 Val Loss: tensor(54.4648, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8411 Loss: 0.49966563284397125 Val Loss: tensor(54.4788, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8412 Loss: 0.49906498193740845 Val Loss: tensor(54.4920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8413 Loss: 0.5004159510135651 Val Loss: tensor(54.4685, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8414 Loss: 0.5014489442110062 Val Loss: tensor(54.5220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8415 Loss: 0.5045137405395508 Val Loss: tensor(54.4600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8416 Loss: 0.5066849440336227 Val Loss: tensor(54.5540, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8417 Loss: 0.5110517144203186 Val Loss: tensor(54.4517, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8418 Loss: 0.5141675174236298 Val Loss: tensor(54.5872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8419 Loss: 0.5197143256664276 Val Loss: tensor(54.4426, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8420 Loss: 0.5236572027206421 Val Loss: tensor(54.6215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8421 Loss: 0.5304256081581116 Val Loss: tensor(54.4323, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8422 Loss: 0.5350381880998611 Val Loss: tensor(54.6566, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8423 Loss: 0.5430445969104767 Val Loss: tensor(54.4209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8424 Loss: 0.548027902841568 Val Loss: tensor(54.6918, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8425 Loss: 0.5571290552616119 Val Loss: tensor(54.4091, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8426 Loss: 0.561926081776619 Val Loss: tensor(54.7252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8427 Loss: 0.5716699063777924 Val Loss: tensor(54.3980, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8428 Loss: 0.5755164623260498 Val Loss: tensor(54.7542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8429 Loss: 0.5850052386522293 Val Loss: tensor(54.3884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8430 Loss: 0.5869593173265457 Val Loss: tensor(54.7750, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8431 Loss: 0.5948345363140106 Val Loss: tensor(54.3813, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8432 Loss: 0.5940239578485489 Val Loss: tensor(54.7843, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8433 Loss: 0.598797470331192 Val Loss: tensor(54.3769, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8434 Loss: 0.5947462618350983 Val Loss: tensor(54.7797, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8435 Loss: 0.59523905813694 Val Loss: tensor(54.3753, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8436 Loss: 0.5882367789745331 Val Loss: tensor(54.7613, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8437 Loss: 0.5842238515615463 Val Loss: tensor(54.3760, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8438 Loss: 0.5752775818109512 Val Loss: tensor(54.7318, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8439 Loss: 0.5677650719881058 Val Loss: tensor(54.3793, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8440 Loss: 0.5583884716033936 Val Loss: tensor(54.6961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8441 Loss: 0.549296036362648 Val Loss: tensor(54.3864, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8442 Loss: 0.541072741150856 Val Loss: tensor(54.6592, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8443 Loss: 0.5326174050569534 Val Loss: tensor(54.3993, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8444 Loss: 0.5268854796886444 Val Loss: tensor(54.6252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8445 Loss: 0.521091490983963 Val Loss: tensor(54.4212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8446 Loss: 0.5189753621816635 Val Loss: tensor(54.5969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8447 Loss: 0.5174422860145569 Val Loss: tensor(54.4553, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8448 Loss: 0.5198814123868942 Val Loss: tensor(54.5754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8449 Loss: 0.5238289088010788 Val Loss: tensor(54.5049, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8450 Loss: 0.5317608714103699 Val Loss: tensor(54.5607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8451 Loss: 0.5419647544622421 Val Loss: tensor(54.5720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8452 Loss: 0.5562659651041031 Val Loss: tensor(54.5517, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8453 Loss: 0.5728422701358795 Val Loss: tensor(54.6570, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8454 Loss: 0.5942704379558563 Val Loss: tensor(54.5468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8455 Loss: 0.6161934286355972 Val Loss: tensor(54.7575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8456 Loss: 0.6449489444494247 Val Loss: tensor(54.5450, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8457 Loss: 0.6693331748247147 Val Loss: tensor(54.8668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8458 Loss: 0.7043005377054214 Val Loss: tensor(54.5459, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8459 Loss: 0.72593554854393 Val Loss: tensor(54.9720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8460 Loss: 0.7631544023752213 Val Loss: tensor(54.5499, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8461 Loss: 0.774715468287468 Val Loss: tensor(55.0533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8462 Loss: 0.8059947788715363 Val Loss: tensor(54.5558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8463 Loss: 0.8002536445856094 Val Loss: tensor(55.0882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8464 Loss: 0.8148226290941238 Val Loss: tensor(54.5588, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8465 Loss: 0.789494976401329 Val Loss: tensor(55.0636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8466 Loss: 0.7812166810035706 Val Loss: tensor(54.5525, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8467 Loss: 0.7428568452596664 Val Loss: tensor(54.9884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8468 Loss: 0.717089056968689 Val Loss: tensor(54.5334, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8469 Loss: 0.6775113791227341 Val Loss: tensor(54.8892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8470 Loss: 0.6472935825586319 Val Loss: tensor(54.5062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8471 Loss: 0.6152838170528412 Val Loss: tensor(54.7929, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8472 Loss: 0.5909223556518555 Val Loss: tensor(54.4811, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8473 Loss: 0.5689954310655594 Val Loss: tensor(54.7133, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8474 Loss: 0.5537162274122238 Val Loss: tensor(54.4678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8475 Loss: 0.5405602008104324 Val Loss: tensor(54.6528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8476 Loss: 0.5329686105251312 Val Loss: tensor(54.4704, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8477 Loss: 0.5261818170547485 Val Loss: tensor(54.6080, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8478 Loss: 0.5234060138463974 Val Loss: tensor(54.4871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8479 Loss: 0.52082858979702 Val Loss: tensor(54.5743, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8480 Loss: 0.5204276591539383 Val Loss: tensor(54.5125, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8481 Loss: 0.5203167200088501 Val Loss: tensor(54.5476, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8482 Loss: 0.5207709074020386 Val Loss: tensor(54.5403, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8483 Loss: 0.5218490809202194 Val Loss: tensor(54.5255, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8484 Loss: 0.5223684161901474 Val Loss: tensor(54.5658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8485 Loss: 0.5237758010625839 Val Loss: tensor(54.5066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8486 Loss: 0.5239666998386383 Val Loss: tensor(54.5866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8487 Loss: 0.5251617282629013 Val Loss: tensor(54.4894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8488 Loss: 0.5248790681362152 Val Loss: tensor(54.6017, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8489 Loss: 0.5255042314529419 Val Loss: tensor(54.4729, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8490 Loss: 0.5247333496809006 Val Loss: tensor(54.6115, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8491 Loss: 0.5245847851037979 Val Loss: tensor(54.4573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8492 Loss: 0.523463562130928 Val Loss: tensor(54.6166, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8493 Loss: 0.5225224196910858 Val Loss: tensor(54.4431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8494 Loss: 0.521315410733223 Val Loss: tensor(54.6180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8495 Loss: 0.5198170989751816 Val Loss: tensor(54.4316, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8496 Loss: 0.5189803689718246 Val Loss: tensor(54.6167, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8497 Loss: 0.5174649506807327 Val Loss: tensor(54.4245, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8498 Loss: 0.5176272392272949 Val Loss: tensor(54.6140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8499 Loss: 0.5169902443885803 Val Loss: tensor(54.4245, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8500 Loss: 0.5189617872238159 Val Loss: tensor(54.6114, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8501 Loss: 0.5204509645700455 Val Loss: tensor(54.4350, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8502 Loss: 0.5251781791448593 Val Loss: tensor(54.6095, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8503 Loss: 0.5302777886390686 Val Loss: tensor(54.4605, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8504 Loss: 0.5387466251850128 Val Loss: tensor(54.6085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8505 Loss: 0.5490484833717346 Val Loss: tensor(54.5057, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8506 Loss: 0.5621820986270905 Val Loss: tensor(54.6063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8507 Loss: 0.579020157456398 Val Loss: tensor(54.5749, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8508 Loss: 0.5975759923458099 Val Loss: tensor(54.6004, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8509 Loss: 0.621707409620285 Val Loss: tensor(54.6705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8510 Loss: 0.6463881880044937 Val Loss: tensor(54.5887, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8511 Loss: 0.6776843369007111 Val Loss: tensor(54.7918, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8512 Loss: 0.7092686891555786 Val Loss: tensor(54.5741, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8513 Loss: 0.7464476078748703 Val Loss: tensor(54.9342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8514 Loss: 0.7852364778518677 Val Loss: tensor(54.5643, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8515 Loss: 0.8248602151870728 Val Loss: tensor(55.0859, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8516 Loss: 0.868714302778244 Val Loss: tensor(54.5697, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8517 Loss: 0.9033809155225754 Val Loss: tensor(55.2226, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8518 Loss: 0.9444697797298431 Val Loss: tensor(54.5941, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8519 Loss: 0.9621899425983429 Val Loss: tensor(55.3046, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8520 Loss: 0.9861752539873123 Val Loss: tensor(54.6276, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8521 Loss: 0.9751939326524734 Val Loss: tensor(55.2914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8522 Loss: 0.9684846848249435 Val Loss: tensor(54.6472, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8523 Loss: 0.9282606840133667 Val Loss: tensor(55.1768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8524 Loss: 0.8919593244791031 Val Loss: tensor(54.6340, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8525 Loss: 0.8370456546545029 Val Loss: tensor(55.0054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8526 Loss: 0.7894229590892792 Val Loss: tensor(54.5914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8527 Loss: 0.7374193072319031 Val Loss: tensor(54.8378, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8528 Loss: 0.6973237991333008 Val Loss: tensor(54.5416, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8529 Loss: 0.657284751534462 Val Loss: tensor(54.7087, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8530 Loss: 0.6313793808221817 Val Loss: tensor(54.5063, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8531 Loss: 0.6041126251220703 Val Loss: tensor(54.6223, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8532 Loss: 0.5895007401704788 Val Loss: tensor(54.4938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8533 Loss: 0.5722049325704575 Val Loss: tensor(54.5679, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8534 Loss: 0.5637457221746445 Val Loss: tensor(54.4990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8535 Loss: 0.5530360341072083 Val Loss: tensor(54.5325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8536 Loss: 0.5471303761005402 Val Loss: tensor(54.5113, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8537 Loss: 0.5402661710977554 Val Loss: tensor(54.5066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8538 Loss: 0.5351957529783249 Val Loss: tensor(54.5226, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8539 Loss: 0.5304313600063324 Val Loss: tensor(54.4848, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8540 Loss: 0.5256164968013763 Val Loss: tensor(54.5306, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8541 Loss: 0.5220304876565933 Val Loss: tensor(54.4654, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8542 Loss: 0.5174008160829544 Val Loss: tensor(54.5356, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8543 Loss: 0.5145480036735535 Val Loss: tensor(54.4483, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8544 Loss: 0.5102336406707764 Val Loss: tensor(54.5380, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8545 Loss: 0.5078305453062057 Val Loss: tensor(54.4336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8546 Loss: 0.5039090216159821 Val Loss: tensor(54.5384, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8547 Loss: 0.5017992854118347 Val Loss: tensor(54.4208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8548 Loss: 0.498317688703537 Val Loss: tensor(54.5374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8549 Loss: 0.49633392691612244 Val Loss: tensor(54.4094, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8550 Loss: 0.49329976737499237 Val Loss: tensor(54.5351, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8551 Loss: 0.49135708808898926 Val Loss: tensor(54.3996, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8552 Loss: 0.48877376317977905 Val Loss: tensor(54.5313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8553 Loss: 0.48683227598667145 Val Loss: tensor(54.3919, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8554 Loss: 0.4847428947687149 Val Loss: tensor(54.5257, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8555 Loss: 0.48283275961875916 Val Loss: tensor(54.3871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8556 Loss: 0.4812864661216736 Val Loss: tensor(54.5178, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8557 Loss: 0.4795083999633789 Val Loss: tensor(54.3858, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8558 Loss: 0.4785635769367218 Val Loss: tensor(54.5079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8559 Loss: 0.47705209255218506 Val Loss: tensor(54.3887, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8560 Loss: 0.4768197238445282 Val Loss: tensor(54.4960, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8561 Loss: 0.4757843464612961 Val Loss: tensor(54.3966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8562 Loss: 0.4763633608818054 Val Loss: tensor(54.4823, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8563 Loss: 0.47604329884052277 Val Loss: tensor(54.4102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8564 Loss: 0.47760535776615143 Val Loss: tensor(54.4667, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8565 Loss: 0.4782972037792206 Val Loss: tensor(54.4306, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8566 Loss: 0.4810510128736496 Val Loss: tensor(54.4487, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8567 Loss: 0.483099564909935 Val Loss: tensor(54.4585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8568 Loss: 0.4873327761888504 Val Loss: tensor(54.4281, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8569 Loss: 0.49113376438617706 Val Loss: tensor(54.4945, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8570 Loss: 0.4972674250602722 Val Loss: tensor(54.4048, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8571 Loss: 0.5032568424940109 Val Loss: tensor(54.5392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8572 Loss: 0.5118580460548401 Val Loss: tensor(54.3792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8573 Loss: 0.5204659402370453 Val Loss: tensor(54.5923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8574 Loss: 0.5322151482105255 Val Loss: tensor(54.3527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8575 Loss: 0.5437435954809189 Val Loss: tensor(54.6531, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8576 Loss: 0.5592785328626633 Val Loss: tensor(54.3282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8577 Loss: 0.5736236423254013 Val Loss: tensor(54.7192, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8578 Loss: 0.5930582880973816 Val Loss: tensor(54.3105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8579 Loss: 0.6092197448015213 Val Loss: tensor(54.7851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8580 Loss: 0.6313123852014542 Val Loss: tensor(54.3062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8581 Loss: 0.6467988193035126 Val Loss: tensor(54.8404, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8582 Loss: 0.6680586934089661 Val Loss: tensor(54.3217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8583 Loss: 0.6788288950920105 Val Loss: tensor(54.8698, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8584 Loss: 0.6934370547533035 Val Loss: tensor(54.3597, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8585 Loss: 0.695228099822998 Val Loss: tensor(54.8578, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8586 Loss: 0.6974316835403442 Val Loss: tensor(54.4156, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8587 Loss: 0.6888999789953232 Val Loss: tensor(54.7998, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8588 Loss: 0.6775732487440109 Val Loss: tensor(54.4778, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8589 Loss: 0.6625731587409973 Val Loss: tensor(54.7103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8590 Loss: 0.6436613202095032 Val Loss: tensor(54.5354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8591 Loss: 0.6294646859169006 Val Loss: tensor(54.6171, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8592 Loss: 0.6122277677059174 Val Loss: tensor(54.5859, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8593 Loss: 0.6054598391056061 Val Loss: tensor(54.5458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8594 Loss: 0.5963956415653229 Val Loss: tensor(54.6343, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8595 Loss: 0.6009136438369751 Val Loss: tensor(54.5081, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8596 Loss: 0.6013413816690445 Val Loss: tensor(54.6865, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8597 Loss: 0.6180553287267685 Val Loss: tensor(54.5023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8598 Loss: 0.6253233850002289 Val Loss: tensor(54.7428, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8599 Loss: 0.6522098630666733 Val Loss: tensor(54.5179, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8600 Loss: 0.6612459123134613 Val Loss: tensor(54.7962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8601 Loss: 0.6928172260522842 Val Loss: tensor(54.5414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8602 Loss: 0.6970185190439224 Val Loss: tensor(54.8335, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8603 Loss: 0.724303126335144 Val Loss: tensor(54.5583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8604 Loss: 0.7171391993761063 Val Loss: tensor(54.8411, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8605 Loss: 0.7301250249147415 Val Loss: tensor(54.5568, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8606 Loss: 0.7087271958589554 Val Loss: tensor(54.8135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8607 Loss: 0.7021542936563492 Val Loss: tensor(54.5334, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8608 Loss: 0.6703357845544815 Val Loss: tensor(54.7582, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8609 Loss: 0.6485322713851929 Val Loss: tensor(54.4951, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8610 Loss: 0.6153004318475723 Val Loss: tensor(54.6924, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8611 Loss: 0.5892438739538193 Val Loss: tensor(54.4554, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8612 Loss: 0.5629333108663559 Val Loss: tensor(54.6331, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8613 Loss: 0.5424099117517471 Val Loss: tensor(54.4272, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8614 Loss: 0.5267474204301834 Val Loss: tensor(54.5890, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8615 Loss: 0.5160784721374512 Val Loss: tensor(54.4188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8616 Loss: 0.5107849836349487 Val Loss: tensor(54.5612, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8617 Loss: 0.5098922252655029 Val Loss: tensor(54.4333, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8618 Loss: 0.5131190121173859 Val Loss: tensor(54.5462, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8619 Loss: 0.5199019312858582 Val Loss: tensor(54.4703, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8620 Loss: 0.5297448486089706 Val Loss: tensor(54.5388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8621 Loss: 0.5416379570960999 Val Loss: tensor(54.5260, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8622 Loss: 0.5563383400440216 Val Loss: tensor(54.5339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8623 Loss: 0.5707462579011917 Val Loss: tensor(54.5939, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8624 Loss: 0.5885848701000214 Val Loss: tensor(54.5265, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8625 Loss: 0.6028396934270859 Val Loss: tensor(54.6658, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8626 Loss: 0.6220934689044952 Val Loss: tensor(54.5138, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8627 Loss: 0.6336014419794083 Val Loss: tensor(54.7337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8628 Loss: 0.6526407897472382 Val Loss: tensor(54.4967, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8629 Loss: 0.659482553601265 Val Loss: tensor(54.7913, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8630 Loss: 0.6767382770776749 Val Loss: tensor(54.4798, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8631 Loss: 0.6782985776662827 Val Loss: tensor(54.8342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8632 Loss: 0.6922421157360077 Val Loss: tensor(54.4700, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8633 Loss: 0.689300000667572 Val Loss: tensor(54.8598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8634 Loss: 0.6987936496734619 Val Loss: tensor(54.4723, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8635 Loss: 0.6931748241186142 Val Loss: tensor(54.8671, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8636 Loss: 0.6979360580444336 Val Loss: tensor(54.4871, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8637 Loss: 0.6918453276157379 Val Loss: tensor(54.8578, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8638 Loss: 0.6927160769701004 Val Loss: tensor(54.5097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8639 Loss: 0.6880518794059753 Val Loss: tensor(54.8376, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8640 Loss: 0.6865753084421158 Val Loss: tensor(54.5330, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8641 Loss: 0.6844333559274673 Val Loss: tensor(54.8148, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8642 Loss: 0.6821916848421097 Val Loss: tensor(54.5511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8643 Loss: 0.6825520396232605 Val Loss: tensor(54.7969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8644 Loss: 0.6805752068758011 Val Loss: tensor(54.5609, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8645 Loss: 0.6824108809232712 Val Loss: tensor(54.7870, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8646 Loss: 0.6808922588825226 Val Loss: tensor(54.5618, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8647 Loss: 0.682403564453125 Val Loss: tensor(54.7825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8648 Loss: 0.6807478070259094 Val Loss: tensor(54.5545, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8649 Loss: 0.6800168454647064 Val Loss: tensor(54.7776, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8650 Loss: 0.6773087829351425 Val Loss: tensor(54.5399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8651 Loss: 0.6731693744659424 Val Loss: tensor(54.7677, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8652 Loss: 0.6687925308942795 Val Loss: tensor(54.5190, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8653 Loss: 0.6612599194049835 Val Loss: tensor(54.7514, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8654 Loss: 0.6552421599626541 Val Loss: tensor(54.4934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8655 Loss: 0.6453582793474197 Val Loss: tensor(54.7307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8656 Loss: 0.6383177936077118 Val Loss: tensor(54.4654, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8657 Loss: 0.6274548023939133 Val Loss: tensor(54.7078, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8658 Loss: 0.6201086193323135 Val Loss: tensor(54.4380, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8659 Loss: 0.6093004792928696 Val Loss: tensor(54.6839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8660 Loss: 0.6020607501268387 Val Loss: tensor(54.4135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8661 Loss: 0.5918008238077164 Val Loss: tensor(54.6587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8662 Loss: 0.5847489386796951 Val Loss: tensor(54.3937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8663 Loss: 0.5751859694719315 Val Loss: tensor(54.6322, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8664 Loss: 0.568258136510849 Val Loss: tensor(54.3788, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8665 Loss: 0.5594567060470581 Val Loss: tensor(54.6041, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8666 Loss: 0.5526243150234222 Val Loss: tensor(54.3689, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8667 Loss: 0.5447190552949905 Val Loss: tensor(54.5754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8668 Loss: 0.5380943864583969 Val Loss: tensor(54.3637, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8669 Loss: 0.5313127189874649 Val Loss: tensor(54.5467, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8670 Loss: 0.5251557976007462 Val Loss: tensor(54.3630, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8671 Loss: 0.5197581052780151 Val Loss: tensor(54.5194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8672 Loss: 0.5144199132919312 Val Loss: tensor(54.3670, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8673 Loss: 0.5106880068778992 Val Loss: tensor(54.4940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8674 Loss: 0.5066052377223969 Val Loss: tensor(54.3762, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8675 Loss: 0.5047835260629654 Val Loss: tensor(54.4712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8676 Loss: 0.5024247169494629 Val Loss: tensor(54.3913, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8677 Loss: 0.5027951598167419 Val Loss: tensor(54.4511, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8678 Loss: 0.502618134021759 Val Loss: tensor(54.4135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8679 Loss: 0.5055215507745743 Val Loss: tensor(54.4337, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8680 Loss: 0.507934495806694 Val Loss: tensor(54.4440, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8681 Loss: 0.5137702524662018 Val Loss: tensor(54.4186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8682 Loss: 0.5190709978342056 Val Loss: tensor(54.4842, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8683 Loss: 0.5283042043447495 Val Loss: tensor(54.4053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8684 Loss: 0.536598339676857 Val Loss: tensor(54.5349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8685 Loss: 0.5497061163187027 Val Loss: tensor(54.3929, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8686 Loss: 0.5607381463050842 Val Loss: tensor(54.5958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8687 Loss: 0.5780071914196014 Val Loss: tensor(54.3811, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8688 Loss: 0.5910388976335526 Val Loss: tensor(54.6647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8689 Loss: 0.6123229563236237 Val Loss: tensor(54.3702, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8690 Loss: 0.6258991360664368 Val Loss: tensor(54.7370, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8691 Loss: 0.6500962674617767 Val Loss: tensor(54.3622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8692 Loss: 0.6618873625993729 Val Loss: tensor(54.8049, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8693 Loss: 0.6863022893667221 Val Loss: tensor(54.3604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8694 Loss: 0.693152517080307 Val Loss: tensor(54.8575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8695 Loss: 0.7132502496242523 Val Loss: tensor(54.3678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8696 Loss: 0.7119915932416916 Val Loss: tensor(54.8823, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8697 Loss: 0.722455233335495 Val Loss: tensor(54.3844, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8698 Loss: 0.7115270495414734 Val Loss: tensor(54.8704, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8699 Loss: 0.7090802192687988 Val Loss: tensor(54.4058, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8700 Loss: 0.6901994049549103 Val Loss: tensor(54.8226, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8701 Loss: 0.676233246922493 Val Loss: tensor(54.4258, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8702 Loss: 0.6540500521659851 Val Loss: tensor(54.7510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8703 Loss: 0.6343804597854614 Val Loss: tensor(54.4407, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8704 Loss: 0.6139331012964249 Val Loss: tensor(54.6737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8705 Loss: 0.5952414125204086 Val Loss: tensor(54.4528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8706 Loss: 0.5798606127500534 Val Loss: tensor(54.6054, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8707 Loss: 0.5664069950580597 Val Loss: tensor(54.4678, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8708 Loss: 0.5573658794164658 Val Loss: tensor(54.5534, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8709 Loss: 0.5502582639455795 Val Loss: tensor(54.4902, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8710 Loss: 0.5474289953708649 Val Loss: tensor(54.5181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8711 Loss: 0.5456422120332718 Val Loss: tensor(54.5207, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8712 Loss: 0.5481751561164856 Val Loss: tensor(54.4959, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8713 Loss: 0.5498564690351486 Val Loss: tensor(54.5566, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8714 Loss: 0.5564386993646622 Val Loss: tensor(54.4820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8715 Loss: 0.5596742480993271 Val Loss: tensor(54.5938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8716 Loss: 0.5688000321388245 Val Loss: tensor(54.4720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8717 Loss: 0.5718881338834763 Val Loss: tensor(54.6286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8718 Loss: 0.5819127857685089 Val Loss: tensor(54.4626, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8719 Loss: 0.5833439975976944 Val Loss: tensor(54.6579, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8720 Loss: 0.5926579982042313 Val Loss: tensor(54.4512, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8721 Loss: 0.5912543684244156 Val Loss: tensor(54.6795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8722 Loss: 0.5984193831682205 Val Loss: tensor(54.4365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8723 Loss: 0.5935723036527634 Val Loss: tensor(54.6916, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8724 Loss: 0.5975707322359085 Val Loss: tensor(54.4192, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8725 Loss: 0.5895597338676453 Val Loss: tensor(54.6937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8726 Loss: 0.5900162011384964 Val Loss: tensor(54.4018, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8727 Loss: 0.5799724459648132 Val Loss: tensor(54.6865, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8728 Loss: 0.5772334486246109 Val Loss: tensor(54.3882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8729 Loss: 0.5667809247970581 Val Loss: tensor(54.6710, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8730 Loss: 0.5618240833282471 Val Loss: tensor(54.3825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8731 Loss: 0.5525258183479309 Val Loss: tensor(54.6488, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8732 Loss: 0.5467994511127472 Val Loss: tensor(54.3879, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8733 Loss: 0.5398528724908829 Val Loss: tensor(54.6215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8734 Loss: 0.5349423289299011 Val Loss: tensor(54.4067, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8735 Loss: 0.5311435461044312 Val Loss: tensor(54.5909, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8736 Loss: 0.5285133421421051 Val Loss: tensor(54.4394, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8737 Loss: 0.5283942520618439 Val Loss: tensor(54.5591, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8738 Loss: 0.5290752053260803 Val Loss: tensor(54.4851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8739 Loss: 0.5330299437046051 Val Loss: tensor(54.5284, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8740 Loss: 0.5373522341251373 Val Loss: tensor(54.5412, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8741 Loss: 0.5457061529159546 Val Loss: tensor(54.5002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8742 Loss: 0.5531216561794281 Val Loss: tensor(54.6039, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8743 Loss: 0.5659656822681427 Val Loss: tensor(54.4754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8744 Loss: 0.5751066505908966 Val Loss: tensor(54.6681, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8745 Loss: 0.5920837968587875 Val Loss: tensor(54.4538, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8746 Loss: 0.6009025275707245 Val Loss: tensor(54.7285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8747 Loss: 0.6208068877458572 Val Loss: tensor(54.4357, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8748 Loss: 0.626878559589386 Val Loss: tensor(54.7796, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8749 Loss: 0.6472348719835281 Val Loss: tensor(54.4226, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8750 Loss: 0.6482710391283035 Val Loss: tensor(54.8156, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8751 Loss: 0.6653316020965576 Val Loss: tensor(54.4184, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8752 Loss: 0.6598400324583054 Val Loss: tensor(54.8305, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8753 Loss: 0.6695885062217712 Val Loss: tensor(54.4264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8754 Loss: 0.6578530222177505 Val Loss: tensor(54.8205, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8755 Loss: 0.658077284693718 Val Loss: tensor(54.4478, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8756 Loss: 0.6424850523471832 Val Loss: tensor(54.7865, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8757 Loss: 0.634448230266571 Val Loss: tensor(54.4789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8758 Loss: 0.6186629384756088 Val Loss: tensor(54.7356, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8759 Loss: 0.6065383851528168 Val Loss: tensor(54.5139, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8760 Loss: 0.5937447994947433 Val Loss: tensor(54.6787, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8761 Loss: 0.5820100903511047 Val Loss: tensor(54.5480, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8762 Loss: 0.573841854929924 Val Loss: tensor(54.6260, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8763 Loss: 0.5652054697275162 Val Loss: tensor(54.5795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8764 Loss: 0.5616886615753174 Val Loss: tensor(54.5831, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8765 Loss: 0.556563675403595 Val Loss: tensor(54.6078, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8766 Loss: 0.5565612614154816 Val Loss: tensor(54.5498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8767 Loss: 0.5537003427743912 Val Loss: tensor(54.6314, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8768 Loss: 0.5553955584764481 Val Loss: tensor(54.5222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8769 Loss: 0.5528787076473236 Val Loss: tensor(54.6479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8770 Loss: 0.554360568523407 Val Loss: tensor(54.4957, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8771 Loss: 0.5505451411008835 Val Loss: tensor(54.6550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8772 Loss: 0.5503553450107574 Val Loss: tensor(54.4682, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8773 Loss: 0.5445389598608017 Val Loss: tensor(54.6517, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8774 Loss: 0.542039543390274 Val Loss: tensor(54.4414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8775 Loss: 0.5346769839525223 Val Loss: tensor(54.6391, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8776 Loss: 0.5301838368177414 Val Loss: tensor(54.4193, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8777 Loss: 0.5226354002952576 Val Loss: tensor(54.6197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8778 Loss: 0.5173172652721405 Val Loss: tensor(54.4065, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8779 Loss: 0.5113685876131058 Val Loss: tensor(54.5972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8780 Loss: 0.5071356743574142 Val Loss: tensor(54.4076, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8781 Loss: 0.504688948392868 Val Loss: tensor(54.5754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8782 Loss: 0.5038342475891113 Val Loss: tensor(54.4266, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8783 Loss: 0.5067410171031952 Val Loss: tensor(54.5574, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8784 Loss: 0.511750802397728 Val Loss: tensor(54.4676, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8785 Loss: 0.5218907743692398 Val Loss: tensor(54.5454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8786 Loss: 0.5351218432188034 Val Loss: tensor(54.5340, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8787 Loss: 0.554456427693367 Val Loss: tensor(54.5403, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8788 Loss: 0.577744796872139 Val Loss: tensor(54.6282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8789 Loss: 0.6082670837640762 Val Loss: tensor(54.5421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8790 Loss: 0.6424201279878616 Val Loss: tensor(54.7498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8791 Loss: 0.6857064813375473 Val Loss: tensor(54.5532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8792 Loss: 0.7297722697257996 Val Loss: tensor(54.8958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8793 Loss: 0.7857251316308975 Val Loss: tensor(54.5807, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8794 Loss: 0.8357759118080139 Val Loss: tensor(55.0590, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8795 Loss: 0.9001433253288269 Val Loss: tensor(54.6356, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8796 Loss: 0.948264092206955 Val Loss: tensor(55.2253, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8797 Loss: 1.0098847150802612 Val Loss: tensor(54.7239, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8798 Loss: 1.045522689819336 Val Loss: tensor(55.3700, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8799 Loss: 1.0875297337770462 Val Loss: tensor(54.8324, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8800 Loss: 1.1026478111743927 Val Loss: tensor(55.4621, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8801 Loss: 1.111496478319168 Val Loss: tensor(54.9244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8802 Loss: 1.1047060936689377 Val Loss: tensor(55.4756, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8803 Loss: 1.0805944055318832 Val Loss: tensor(54.9606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8804 Loss: 1.052871972322464 Val Loss: tensor(55.4028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8805 Loss: 1.0076946914196014 Val Loss: tensor(54.9307, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8806 Loss: 0.9581112861633301 Val Loss: tensor(55.2594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8807 Loss: 0.9042944759130478 Val Loss: tensor(54.8564, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8808 Loss: 0.8376587331295013 Val Loss: tensor(55.0841, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8809 Loss: 0.7843859195709229 Val Loss: tensor(54.7676, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8810 Loss: 0.716280534863472 Val Loss: tensor(54.9281, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8811 Loss: 0.6719619333744049 Val Loss: tensor(54.6863, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8812 Loss: 0.6185791492462158 Val Loss: tensor(54.8282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8813 Loss: 0.5886111706495285 Val Loss: tensor(54.6274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8814 Loss: 0.5551885068416595 Val Loss: tensor(54.7807, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8815 Loss: 0.5386399775743484 Val Loss: tensor(54.5956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8816 Loss: 0.5205144882202148 Val Loss: tensor(54.7546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8817 Loss: 0.5127056390047073 Val Loss: tensor(54.5833, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8818 Loss: 0.503234788775444 Val Loss: tensor(54.7268, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8819 Loss: 0.5001022219657898 Val Loss: tensor(54.5784, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8820 Loss: 0.4949217587709427 Val Loss: tensor(54.6959, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8821 Loss: 0.49424275755882263 Val Loss: tensor(54.5720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8822 Loss: 0.4911080449819565 Val Loss: tensor(54.6703, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8823 Loss: 0.49170663952827454 Val Loss: tensor(54.5611, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8824 Loss: 0.48946070671081543 Val Loss: tensor(54.6547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8825 Loss: 0.49058690667152405 Val Loss: tensor(54.5482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8826 Loss: 0.48869363963603973 Val Loss: tensor(54.6482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8827 Loss: 0.4899367243051529 Val Loss: tensor(54.5358, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8828 Loss: 0.48822782933712006 Val Loss: tensor(54.6469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8829 Loss: 0.4894411563873291 Val Loss: tensor(54.5242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8830 Loss: 0.4879266917705536 Val Loss: tensor(54.6474, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8831 Loss: 0.4890952408313751 Val Loss: tensor(54.5125, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8832 Loss: 0.4878055155277252 Val Loss: tensor(54.6482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8833 Loss: 0.48893798887729645 Val Loss: tensor(54.5008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8834 Loss: 0.4878716319799423 Val Loss: tensor(54.6489, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8835 Loss: 0.4889427423477173 Val Loss: tensor(54.4895, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8836 Loss: 0.4880676716566086 Val Loss: tensor(54.6494, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8837 Loss: 0.489040270447731 Val Loss: tensor(54.4790, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8838 Loss: 0.4883127510547638 Val Loss: tensor(54.6496, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8839 Loss: 0.48915354907512665 Val Loss: tensor(54.4696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8840 Loss: 0.48855678737163544 Val Loss: tensor(54.6493, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8841 Loss: 0.4892209470272064 Val Loss: tensor(54.4616, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8842 Loss: 0.4887564331293106 Val Loss: tensor(54.6484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8843 Loss: 0.48922809958457947 Val Loss: tensor(54.4550, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8844 Loss: 0.48889313638210297 Val Loss: tensor(54.6468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8845 Loss: 0.489165797829628 Val Loss: tensor(54.4503, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8846 Loss: 0.4889694005250931 Val Loss: tensor(54.6439, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8847 Loss: 0.4890366494655609 Val Loss: tensor(54.4479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8848 Loss: 0.4889908730983734 Val Loss: tensor(54.6397, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8849 Loss: 0.48887941241264343 Val Loss: tensor(54.4483, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8850 Loss: 0.48898209631443024 Val Loss: tensor(54.6336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8851 Loss: 0.4887188524007797 Val Loss: tensor(54.4517, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8852 Loss: 0.48895518481731415 Val Loss: tensor(54.6252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8853 Loss: 0.48859627544879913 Val Loss: tensor(54.4584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8854 Loss: 0.4889546036720276 Val Loss: tensor(54.6144, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8855 Loss: 0.4885406494140625 Val Loss: tensor(54.4686, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8856 Loss: 0.48903264105319977 Val Loss: tensor(54.6007, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8857 Loss: 0.4886527508497238 Val Loss: tensor(54.4821, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8858 Loss: 0.48927445709705353 Val Loss: tensor(54.5843, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8859 Loss: 0.4889875203371048 Val Loss: tensor(54.4987, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8860 Loss: 0.48975756764411926 Val Loss: tensor(54.5650, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8861 Loss: 0.4896809160709381 Val Loss: tensor(54.5177, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8862 Loss: 0.49062293767929077 Val Loss: tensor(54.5431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8863 Loss: 0.49085602164268494 Val Loss: tensor(54.5384, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8864 Loss: 0.49201980233192444 Val Loss: tensor(54.5186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8865 Loss: 0.4926648437976837 Val Loss: tensor(54.5602, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8866 Loss: 0.4941265136003494 Val Loss: tensor(54.4921, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8867 Loss: 0.4952779859304428 Val Loss: tensor(54.5825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8868 Loss: 0.49713748693466187 Val Loss: tensor(54.4640, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8869 Loss: 0.4988982379436493 Val Loss: tensor(54.6049, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8870 Loss: 0.5012544393539429 Val Loss: tensor(54.4356, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8871 Loss: 0.5036989003419876 Val Loss: tensor(54.6270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8872 Loss: 0.5066832602024078 Val Loss: tensor(54.4085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8873 Loss: 0.509848952293396 Val Loss: tensor(54.6485, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8874 Loss: 0.5135576725006104 Val Loss: tensor(54.3862, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8875 Loss: 0.5174171030521393 Val Loss: tensor(54.6680, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8876 Loss: 0.5218701213598251 Val Loss: tensor(54.3737, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8877 Loss: 0.5263063311576843 Val Loss: tensor(54.6828, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8878 Loss: 0.5314067155122757 Val Loss: tensor(54.3779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8879 Loss: 0.5361990481615067 Val Loss: tensor(54.6877, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8880 Loss: 0.5416907966136932 Val Loss: tensor(54.4060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8881 Loss: 0.5467301309108734 Val Loss: tensor(54.6758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8882 Loss: 0.5524498373270035 Val Loss: tensor(54.4636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8883 Loss: 0.5584039241075516 Val Loss: tensor(54.6412, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8884 Loss: 0.5650714337825775 Val Loss: tensor(54.5520, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8885 Loss: 0.5745556056499481 Val Loss: tensor(54.5854, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8886 Loss: 0.5852799266576767 Val Loss: tensor(54.6696, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8887 Loss: 0.6040174663066864 Val Loss: tensor(54.5221, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8888 Loss: 0.6253554821014404 Val Loss: tensor(54.8159, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8889 Loss: 0.662220224738121 Val Loss: tensor(54.4770, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8890 Loss: 0.7028236240148544 Val Loss: tensor(54.9958, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8891 Loss: 0.7679815590381622 Val Loss: tensor(54.4815, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8892 Loss: 0.8328626602888107 Val Loss: tensor(55.2130, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8893 Loss: 0.931225061416626 Val Loss: tensor(54.5623, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8894 Loss: 1.0109340101480484 Val Loss: tensor(55.4458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8895 Loss: 1.1261413246393204 Val Loss: tensor(54.7212, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8896 Loss: 1.1853090077638626 Val Loss: tensor(55.6149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8897 Loss: 1.2625891268253326 Val Loss: tensor(54.9051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8898 Loss: 1.2512279003858566 Val Loss: tensor(55.6068, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8899 Loss: 1.2260747849941254 Val Loss: tensor(55.0127, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8900 Loss: 1.1379929333925247 Val Loss: tensor(55.4047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8901 Loss: 1.0251078009605408 Val Loss: tensor(54.9789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8902 Loss: 0.9173252582550049 Val Loss: tensor(55.1358, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8903 Loss: 0.8018959164619446 Val Loss: tensor(54.8411, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8904 Loss: 0.7193014621734619 Val Loss: tensor(54.9184, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8905 Loss: 0.6475570648908615 Val Loss: tensor(54.6901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8906 Loss: 0.5972413718700409 Val Loss: tensor(54.7843, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8907 Loss: 0.5656173527240753 Val Loss: tensor(54.5908, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8908 Loss: 0.5389188528060913 Val Loss: tensor(54.7141, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8909 Loss: 0.5294909030199051 Val Loss: tensor(54.5583, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8910 Loss: 0.5161646455526352 Val Loss: tensor(54.6768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8911 Loss: 0.5142855197191238 Val Loss: tensor(54.5713, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8912 Loss: 0.5084837526082993 Val Loss: tensor(54.6528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8913 Loss: 0.508183553814888 Val Loss: tensor(54.5984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8914 Loss: 0.5072569847106934 Val Loss: tensor(54.6281, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8915 Loss: 0.5075820535421371 Val Loss: tensor(54.6217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8916 Loss: 0.509685829281807 Val Loss: tensor(54.5987, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8917 Loss: 0.5105761736631393 Val Loss: tensor(54.6430, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8918 Loss: 0.5148025900125504 Val Loss: tensor(54.5694, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8919 Loss: 0.5159067511558533 Val Loss: tensor(54.6686, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8920 Loss: 0.5218470692634583 Val Loss: tensor(54.5422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8921 Loss: 0.5227337181568146 Val Loss: tensor(54.6975, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8922 Loss: 0.5297304540872574 Val Loss: tensor(54.5177, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8923 Loss: 0.5300506502389908 Val Loss: tensor(54.7259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8924 Loss: 0.5370073318481445 Val Loss: tensor(54.4954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8925 Loss: 0.5363627970218658 Val Loss: tensor(54.7492, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8926 Loss: 0.5419773459434509 Val Loss: tensor(54.4740, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8927 Loss: 0.5399567186832428 Val Loss: tensor(54.7619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8928 Loss: 0.5431295335292816 Val Loss: tensor(54.4548, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8929 Loss: 0.5395118445158005 Val Loss: tensor(54.7611, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8930 Loss: 0.5396384745836258 Val Loss: tensor(54.4413, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8931 Loss: 0.534655287861824 Val Loss: tensor(54.7466, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8932 Loss: 0.5318997502326965 Val Loss: tensor(54.4378, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8933 Loss: 0.5262924879789352 Val Loss: tensor(54.7198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8934 Loss: 0.5216615498065948 Val Loss: tensor(54.4473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8935 Loss: 0.5165546089410782 Val Loss: tensor(54.6832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8936 Loss: 0.5116924941539764 Val Loss: tensor(54.4717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8937 Loss: 0.5083434283733368 Val Loss: tensor(54.6409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8938 Loss: 0.5049243718385696 Val Loss: tensor(54.5116, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8939 Loss: 0.504483699798584 Val Loss: tensor(54.5966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8940 Loss: 0.503755122423172 Val Loss: tensor(54.5657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8941 Loss: 0.5071491301059723 Val Loss: tensor(54.5534, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8942 Loss: 0.5097444355487823 Val Loss: tensor(54.6306, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8943 Loss: 0.5176248401403427 Val Loss: tensor(54.5135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8944 Loss: 0.5235332101583481 Val Loss: tensor(54.7005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8945 Loss: 0.5360678136348724 Val Loss: tensor(54.4787, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8946 Loss: 0.5446325689554214 Val Loss: tensor(54.7684, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8947 Loss: 0.5611605048179626 Val Loss: tensor(54.4510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8948 Loss: 0.5708389729261398 Val Loss: tensor(54.8275, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8949 Loss: 0.5894592702388763 Val Loss: tensor(54.4321, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8950 Loss: 0.5977402329444885 Val Loss: tensor(54.8705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8951 Loss: 0.6150597780942917 Val Loss: tensor(54.4242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8952 Loss: 0.6188410520553589 Val Loss: tensor(54.8902, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8953 Loss: 0.6304519176483154 Val Loss: tensor(54.4297, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8954 Loss: 0.6273198872804642 Val Loss: tensor(54.8802, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8955 Loss: 0.6295738369226456 Val Loss: tensor(54.4502, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8956 Loss: 0.6193888485431671 Val Loss: tensor(54.8396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8957 Loss: 0.6119832992553711 Val Loss: tensor(54.4850, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8958 Loss: 0.5976575165987015 Val Loss: tensor(54.7755, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8959 Loss: 0.5845020711421967 Val Loss: tensor(54.5305, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8960 Loss: 0.5707342177629471 Val Loss: tensor(54.7017, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8961 Loss: 0.5576621145009995 Val Loss: tensor(54.5826, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8962 Loss: 0.5486245602369308 Val Loss: tensor(54.6330, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8963 Loss: 0.5401089042425156 Val Loss: tensor(54.6377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8964 Loss: 0.5381710380315781 Val Loss: tensor(54.5792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8965 Loss: 0.5358224213123322 Val Loss: tensor(54.6927, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8966 Loss: 0.5413350611925125 Val Loss: tensor(54.5429, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8967 Loss: 0.5442642420530319 Val Loss: tensor(54.7442, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8968 Loss: 0.5558232665061951 Val Loss: tensor(54.5208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8969 Loss: 0.5615247189998627 Val Loss: tensor(54.7878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8970 Loss: 0.576316311955452 Val Loss: tensor(54.5070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8971 Loss: 0.5813945829868317 Val Loss: tensor(54.8189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8972 Loss: 0.5956820249557495 Val Loss: tensor(54.4969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8973 Loss: 0.5968703329563141 Val Loss: tensor(54.8327, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8974 Loss: 0.606593981385231 Val Loss: tensor(54.4898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8975 Loss: 0.6018954515457153 Val Loss: tensor(54.8259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8976 Loss: 0.6040664464235306 Val Loss: tensor(54.4875, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8977 Loss: 0.5937303900718689 Val Loss: tensor(54.7987, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8978 Loss: 0.5879816114902496 Val Loss: tensor(54.4927, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8979 Loss: 0.5744625777006149 Val Loss: tensor(54.7557, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8980 Loss: 0.5634941309690475 Val Loss: tensor(54.5065, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8981 Loss: 0.550163060426712 Val Loss: tensor(54.7048, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8982 Loss: 0.5381543338298798 Val Loss: tensor(54.5285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8983 Loss: 0.5277630239725113 Val Loss: tensor(54.6541, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8984 Loss: 0.5181394964456558 Val Loss: tensor(54.5577, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8985 Loss: 0.5121091902256012 Val Loss: tensor(54.6090, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8986 Loss: 0.5065705925226212 Val Loss: tensor(54.5925, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8987 Loss: 0.5052574127912521 Val Loss: tensor(54.5712, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8988 Loss: 0.5039495974779129 Val Loss: tensor(54.6309, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8989 Loss: 0.5070057511329651 Val Loss: tensor(54.5398, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8990 Loss: 0.509275272488594 Val Loss: tensor(54.6703, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8991 Loss: 0.5159265846014023 Val Loss: tensor(54.5126, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8992 Loss: 0.5207279622554779 Val Loss: tensor(54.7074, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8993 Loss: 0.5298733860254288 Val Loss: tensor(54.4889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8994 Loss: 0.53596331179142 Val Loss: tensor(54.7392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8995 Loss: 0.5462051779031754 Val Loss: tensor(54.4697, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8996 Loss: 0.552178218960762 Val Loss: tensor(54.7624, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8997 Loss: 0.5618188828229904 Val Loss: tensor(54.4584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8998 Loss: 0.566290095448494 Val Loss: tensor(54.7745, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8999 Loss: 0.573517307639122 Val Loss: tensor(54.4596, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9000 Loss: 0.5754409730434418 Val Loss: tensor(54.7731, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9001 Loss: 0.5787965953350067 Val Loss: tensor(54.4767, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9002 Loss: 0.5778310745954514 Val Loss: tensor(54.7568, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9003 Loss: 0.5768465548753738 Val Loss: tensor(54.5102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9004 Loss: 0.5735972821712494 Val Loss: tensor(54.7264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9005 Loss: 0.5690520703792572 Val Loss: tensor(54.5561, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9006 Loss: 0.5649346262216568 Val Loss: tensor(54.6861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9007 Loss: 0.5585471838712692 Val Loss: tensor(54.6069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9008 Loss: 0.5552027523517609 Val Loss: tensor(54.6425, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9009 Loss: 0.548896536231041 Val Loss: tensor(54.6540, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9010 Loss: 0.5475705564022064 Val Loss: tensor(54.6028, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9011 Loss: 0.5429188311100006 Val Loss: tensor(54.6907, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9012 Loss: 0.5440700650215149 Val Loss: tensor(54.5718, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9013 Loss: 0.5420877188444138 Val Loss: tensor(54.7149, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9014 Loss: 0.545608788728714 Val Loss: tensor(54.5516, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9015 Loss: 0.5470026284456253 Val Loss: tensor(54.7291, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9016 Loss: 0.5529116243124008 Val Loss: tensor(54.5431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9017 Loss: 0.5585106164216995 Val Loss: tensor(54.7393, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9018 Loss: 0.567784309387207 Val Loss: tensor(54.5510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9019 Loss: 0.5794481784105301 Val Loss: tensor(54.7529, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9020 Loss: 0.5949667096138 Val Loss: tensor(54.5874, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9021 Loss: 0.61614128947258 Val Loss: tensor(54.7744, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9022 Loss: 0.6430114954710007 Val Loss: tensor(54.6722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9023 Loss: 0.678370863199234 Val Loss: tensor(54.8008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9024 Loss: 0.7227440774440765 Val Loss: tensor(54.8266, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9025 Loss: 0.7764630764722824 Val Loss: tensor(54.8194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9026 Loss: 0.8425276279449463 Val Loss: tensor(55.0588, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9027 Loss: 0.9166762232780457 Val Loss: tensor(54.8238, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9028 Loss: 1.0036579221487045 Val Loss: tensor(55.3540, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9029 Loss: 1.0966381132602692 Val Loss: tensor(54.8342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9030 Loss: 1.1927550435066223 Val Loss: tensor(55.6665, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9031 Loss: 1.2898910492658615 Val Loss: tensor(54.8806, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9032 Loss: 1.3563061356544495 Val Loss: tensor(55.8845, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9033 Loss: 1.4127494096755981 Val Loss: tensor(54.9498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9034 Loss: 1.3817609548568726 Val Loss: tensor(55.8311, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9035 Loss: 1.3363143056631088 Val Loss: tensor(54.9757, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9036 Loss: 1.187914714217186 Val Loss: tensor(55.4717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9037 Loss: 1.0516125559806824 Val Loss: tensor(54.9129, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9038 Loss: 0.8822648823261261 Val Loss: tensor(55.0509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9039 Loss: 0.7592234760522842 Val Loss: tensor(54.7968, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9040 Loss: 0.6523397117853165 Val Loss: tensor(54.7625, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9041 Loss: 0.594780296087265 Val Loss: tensor(54.6882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9042 Loss: 0.5441641211509705 Val Loss: tensor(54.6134, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9043 Loss: 0.5264651328325272 Val Loss: tensor(54.6197, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9044 Loss: 0.5041557401418686 Val Loss: tensor(54.5597, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9045 Loss: 0.4980831742286682 Val Loss: tensor(54.5980, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9046 Loss: 0.48884204030036926 Val Loss: tensor(54.5506, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9047 Loss: 0.4848406910896301 Val Loss: tensor(54.5948, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9048 Loss: 0.480786457657814 Val Loss: tensor(54.5509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9049 Loss: 0.4782659411430359 Val Loss: tensor(54.5872, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9050 Loss: 0.4765637218952179 Val Loss: tensor(54.5469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9051 Loss: 0.47665242850780487 Val Loss: tensor(54.5763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9052 Loss: 0.4770505577325821 Val Loss: tensor(54.5416, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9053 Loss: 0.47983990609645844 Val Loss: tensor(54.5664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9054 Loss: 0.4820483773946762 Val Loss: tensor(54.5469, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9055 Loss: 0.4868481159210205 Val Loss: tensor(54.5591, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9056 Loss: 0.49032193422317505 Val Loss: tensor(54.5663, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9057 Loss: 0.49664056301116943 Val Loss: tensor(54.5528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9058 Loss: 0.5009031593799591 Val Loss: tensor(54.5932, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9059 Loss: 0.5084640681743622 Val Loss: tensor(54.5482, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9060 Loss: 0.5131651908159256 Val Loss: tensor(54.6221, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9061 Loss: 0.5218344479799271 Val Loss: tensor(54.5465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9062 Loss: 0.5266818255186081 Val Loss: tensor(54.6513, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9063 Loss: 0.5362620502710342 Val Loss: tensor(54.5467, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9064 Loss: 0.5409364998340607 Val Loss: tensor(54.6815, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9065 Loss: 0.5509366691112518 Val Loss: tensor(54.5467, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9066 Loss: 0.554869756102562 Val Loss: tensor(54.7119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9067 Loss: 0.5645740330219269 Val Loss: tensor(54.5454, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9068 Loss: 0.5669381320476532 Val Loss: tensor(54.7396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9069 Loss: 0.5754877030849457 Val Loss: tensor(54.5421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9070 Loss: 0.5754981637001038 Val Loss: tensor(54.7606, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9071 Loss: 0.5819610804319382 Val Loss: tensor(54.5366, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9072 Loss: 0.5791765749454498 Val Loss: tensor(54.7725, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9073 Loss: 0.582766056060791 Val Loss: tensor(54.5282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9074 Loss: 0.5772014707326889 Val Loss: tensor(54.7744, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9075 Loss: 0.5776174515485764 Val Loss: tensor(54.5167, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9076 Loss: 0.569766953587532 Val Loss: tensor(54.7672, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9077 Loss: 0.5672526061534882 Val Loss: tensor(54.5020, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9078 Loss: 0.5580060184001923 Val Loss: tensor(54.7527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9079 Loss: 0.5532951653003693 Val Loss: tensor(54.4856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9080 Loss: 0.5436854958534241 Val Loss: tensor(54.7333, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9081 Loss: 0.5377279222011566 Val Loss: tensor(54.4691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9082 Loss: 0.5287300944328308 Val Loss: tensor(54.7114, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9083 Loss: 0.5225199162960052 Val Loss: tensor(54.4549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9084 Loss: 0.5148912370204926 Val Loss: tensor(54.6891, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9085 Loss: 0.5092214047908783 Val Loss: tensor(54.4455, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9086 Loss: 0.5034973919391632 Val Loss: tensor(54.6679, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9087 Loss: 0.4989895671606064 Val Loss: tensor(54.4431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9088 Loss: 0.49550212919712067 Val Loss: tensor(54.6486, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9089 Loss: 0.4926465004682541 Val Loss: tensor(54.4498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9090 Loss: 0.49162182211875916 Val Loss: tensor(54.6312, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9091 Loss: 0.4907982796430588 Val Loss: tensor(54.4670, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9092 Loss: 0.49245186150074005 Val Loss: tensor(54.6154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9093 Loss: 0.4939678758382797 Val Loss: tensor(54.4960, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9094 Loss: 0.4985527843236923 Val Loss: tensor(54.6009, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9095 Loss: 0.5026558041572571 Val Loss: tensor(54.5369, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9096 Loss: 0.5104180723428726 Val Loss: tensor(54.5873, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9097 Loss: 0.5172191113233566 Val Loss: tensor(54.5890, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9098 Loss: 0.5284695625305176 Val Loss: tensor(54.5749, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9099 Loss: 0.537788137793541 Val Loss: tensor(54.6502, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9100 Loss: 0.552770346403122 Val Loss: tensor(54.5636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9101 Loss: 0.5639451295137405 Val Loss: tensor(54.7170, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9102 Loss: 0.582605704665184 Val Loss: tensor(54.5540, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9103 Loss: 0.5942518264055252 Val Loss: tensor(54.7844, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9104 Loss: 0.6158003658056259 Val Loss: tensor(54.5461, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9105 Loss: 0.6255526393651962 Val Loss: tensor(54.8455, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9106 Loss: 0.6478798389434814 Val Loss: tensor(54.5396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9107 Loss: 0.6525071114301682 Val Loss: tensor(54.8920, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9108 Loss: 0.6718252748250961 Val Loss: tensor(54.5338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9109 Loss: 0.6682783961296082 Val Loss: tensor(54.9157, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9110 Loss: 0.6798538416624069 Val Loss: tensor(54.5281, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9111 Loss: 0.6670425683259964 Val Loss: tensor(54.9112, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9112 Loss: 0.6675988882780075 Val Loss: tensor(54.5224, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9113 Loss: 0.6479423940181732 Val Loss: tensor(54.8803, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9114 Loss: 0.6384028345346451 Val Loss: tensor(54.5174, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9115 Loss: 0.6171005219221115 Val Loss: tensor(54.8326, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9116 Loss: 0.6025911271572113 Val Loss: tensor(54.5154, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9117 Loss: 0.5848356783390045 Val Loss: tensor(54.7808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9118 Loss: 0.5715137124061584 Val Loss: tensor(54.5201, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9119 Loss: 0.5603517889976501 Val Loss: tensor(54.7361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9120 Loss: 0.5523505806922913 Val Loss: tensor(54.5361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9121 Loss: 0.5486580282449722 Val Loss: tensor(54.7042, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9122 Loss: 0.5472793281078339 Val Loss: tensor(54.5664, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9123 Loss: 0.5506193339824677 Val Loss: tensor(54.6860, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9124 Loss: 0.5549674183130264 Val Loss: tensor(54.6104, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9125 Loss: 0.5641726553440094 Val Loss: tensor(54.6784, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9126 Loss: 0.5719779878854752 Val Loss: tensor(54.6631, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9127 Loss: 0.5852240473031998 Val Loss: tensor(54.6769, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9128 Loss: 0.5935197025537491 Val Loss: tensor(54.7160, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9129 Loss: 0.6082890778779984 Val Loss: tensor(54.6759, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9130 Loss: 0.6138390153646469 Val Loss: tensor(54.7600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9131 Loss: 0.627063974738121 Val Loss: tensor(54.6702, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9132 Loss: 0.6269741356372833 Val Loss: tensor(54.7879, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9133 Loss: 0.635498583316803 Val Loss: tensor(54.6552, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9134 Loss: 0.6280556172132492 Val Loss: tensor(54.7962, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9135 Loss: 0.6296485215425491 Val Loss: tensor(54.6283, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9136 Loss: 0.615267425775528 Val Loss: tensor(54.7861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9137 Loss: 0.6098369508981705 Val Loss: tensor(54.5916, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9138 Loss: 0.5912699103355408 Val Loss: tensor(54.7632, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9139 Loss: 0.5811886489391327 Val Loss: tensor(54.5507, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9140 Loss: 0.5624439120292664 Val Loss: tensor(54.7349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9141 Loss: 0.5513778626918793 Val Loss: tensor(54.5132, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9142 Loss: 0.5359054207801819 Val Loss: tensor(54.7083, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9143 Loss: 0.5270096063613892 Val Loss: tensor(54.4851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9144 Loss: 0.5165970176458359 Val Loss: tensor(54.6875, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9145 Loss: 0.5114814043045044 Val Loss: tensor(54.4706, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9146 Loss: 0.5063197463750839 Val Loss: tensor(54.6738, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9147 Loss: 0.50513656437397 Val Loss: tensor(54.4717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9148 Loss: 0.5044439882040024 Val Loss: tensor(54.6656, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9149 Loss: 0.506441280245781 Val Loss: tensor(54.4883, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9150 Loss: 0.5090875923633575 Val Loss: tensor(54.6595, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9151 Loss: 0.5131011307239532 Val Loss: tensor(54.5186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9152 Loss: 0.5178102999925613 Val Loss: tensor(54.6515, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9153 Loss: 0.5225406438112259 Val Loss: tensor(54.5580, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9154 Loss: 0.5281526148319244 Val Loss: tensor(54.6385, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9155 Loss: 0.5323698222637177 Val Loss: tensor(54.6009, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9156 Loss: 0.5379277169704437 Val Loss: tensor(54.6182, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9157 Loss: 0.5406815111637115 Val Loss: tensor(54.6412, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9158 Loss: 0.5455676317214966 Val Loss: tensor(54.5903, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9159 Loss: 0.5463390946388245 Val Loss: tensor(54.6749, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9160 Loss: 0.5503324717283249 Val Loss: tensor(54.5558, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9161 Loss: 0.5490574985742569 Val Loss: tensor(54.7006, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9162 Loss: 0.5522895008325577 Val Loss: tensor(54.5173, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9163 Loss: 0.5494073927402496 Val Loss: tensor(54.7191, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9164 Loss: 0.5522072017192841 Val Loss: tensor(54.4792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9165 Loss: 0.5486520528793335 Val Loss: tensor(54.7325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9166 Loss: 0.5514883249998093 Val Loss: tensor(54.4478, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9167 Loss: 0.5485114902257919 Val Loss: tensor(54.7424, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9168 Loss: 0.5518741458654404 Val Loss: tensor(54.4304, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9169 Loss: 0.5506879240274429 Val Loss: tensor(54.7495, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9170 Loss: 0.5549111515283585 Val Loss: tensor(54.4342, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9171 Loss: 0.5562607496976852 Val Loss: tensor(54.7523, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9172 Loss: 0.5612841844558716 Val Loss: tensor(54.4629, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9173 Loss: 0.5650415569543839 Val Loss: tensor(54.7491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9174 Loss: 0.5702332109212875 Val Loss: tensor(54.5146, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9175 Loss: 0.5754586607217789 Val Loss: tensor(54.7379, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9176 Loss: 0.5797556489706039 Val Loss: tensor(54.5799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9177 Loss: 0.5852000713348389 Val Loss: tensor(54.7189, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9178 Loss: 0.5875631868839264 Val Loss: tensor(54.6452, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9179 Loss: 0.5922862440347672 Val Loss: tensor(54.6939, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9180 Loss: 0.5922117829322815 Val Loss: tensor(54.6972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9181 Loss: 0.5958924740552902 Val Loss: tensor(54.6649, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9182 Loss: 0.5933608114719391 Val Loss: tensor(54.7298, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9183 Loss: 0.596069261431694 Val Loss: tensor(54.6315, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9184 Loss: 0.5911089777946472 Val Loss: tensor(54.7443, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9185 Loss: 0.5927764326334 Val Loss: tensor(54.5926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9186 Loss: 0.5852116942405701 Val Loss: tensor(54.7458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9187 Loss: 0.5853609293699265 Val Loss: tensor(54.5484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9188 Loss: 0.5753956437110901 Val Loss: tensor(54.7393, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9189 Loss: 0.5736037194728851 Val Loss: tensor(54.5043, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9190 Loss: 0.562487855553627 Val Loss: tensor(54.7276, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9191 Loss: 0.5590940713882446 Val Loss: tensor(54.4690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9192 Loss: 0.5490473955869675 Val Loss: tensor(54.7130, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9193 Loss: 0.5453140139579773 Val Loss: tensor(54.4508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9194 Loss: 0.5385797470808029 Val Loss: tensor(54.6975, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9195 Loss: 0.5360149294137955 Val Loss: tensor(54.4540, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9196 Loss: 0.5337012708187103 Val Loss: tensor(54.6833, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9197 Loss: 0.5332054644823074 Val Loss: tensor(54.4781, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9198 Loss: 0.5350709855556488 Val Loss: tensor(54.6716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9199 Loss: 0.5367464870214462 Val Loss: tensor(54.5188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9200 Loss: 0.541843056678772 Val Loss: tensor(54.6626, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9201 Loss: 0.5454524606466293 Val Loss: tensor(54.5693, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9202 Loss: 0.5531348139047623 Val Loss: tensor(54.6546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9203 Loss: 0.5586513429880142 Val Loss: tensor(54.6228, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9204 Loss: 0.5692334473133087 Val Loss: tensor(54.6460, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9205 Loss: 0.5772554278373718 Val Loss: tensor(54.6758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9206 Loss: 0.5922186523675919 Val Loss: tensor(54.6363, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9207 Loss: 0.6038826704025269 Val Loss: tensor(54.7289, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9208 Loss: 0.6254861503839493 Val Loss: tensor(54.6297, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9209 Loss: 0.6421487182378769 Val Loss: tensor(54.7867, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9210 Loss: 0.6722276508808136 Val Loss: tensor(54.6354, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9211 Loss: 0.6948523968458176 Val Loss: tensor(54.8545, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9212 Loss: 0.7331203371286392 Val Loss: tensor(54.6666, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9213 Loss: 0.7616258859634399 Val Loss: tensor(54.9348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9214 Loss: 0.8039193749427795 Val Loss: tensor(54.7330, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9215 Loss: 0.8369481712579727 Val Loss: tensor(55.0273, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9216 Loss: 0.8756285607814789 Val Loss: tensor(54.8312, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9217 Loss: 0.9116499871015549 Val Loss: tensor(55.1329, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9218 Loss: 0.9393546432256699 Val Loss: tensor(54.9370, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9219 Loss: 0.9784187823534012 Val Loss: tensor(55.2570, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9220 Loss: 0.9929756820201874 Val Loss: tensor(55.0095, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9221 Loss: 1.0345293581485748 Val Loss: tensor(55.3969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9222 Loss: 1.0383484661579132 Val Loss: tensor(55.0122, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9223 Loss: 1.0729112923145294 Val Loss: tensor(55.5115, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9224 Loss: 1.065228909254074 Val Loss: tensor(54.9407, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9225 Loss: 1.0695707648992538 Val Loss: tensor(55.5143, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9226 Loss: 1.0401932895183563 Val Loss: tensor(54.8309, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9227 Loss: 0.9937364757061005 Val Loss: tensor(55.3535, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9228 Loss: 0.9330635219812393 Val Loss: tensor(54.7247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9229 Loss: 0.85069440305233 Val Loss: tensor(55.1037, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9230 Loss: 0.775032252073288 Val Loss: tensor(54.6385, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9231 Loss: 0.7008474320173264 Val Loss: tensor(54.8976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9232 Loss: 0.6442993134260178 Val Loss: tensor(54.5840, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9233 Loss: 0.6045453399419785 Val Loss: tensor(54.7948, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9234 Loss: 0.5810785889625549 Val Loss: tensor(54.5813, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9235 Loss: 0.5716902911663055 Val Loss: tensor(54.7651, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9236 Loss: 0.5719876885414124 Val Loss: tensor(54.6347, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9237 Loss: 0.5789986103773117 Val Loss: tensor(54.7563, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9238 Loss: 0.5884968638420105 Val Loss: tensor(54.7208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9239 Loss: 0.6008900701999664 Val Loss: tensor(54.7465, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9240 Loss: 0.6085132956504822 Val Loss: tensor(54.8051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9241 Loss: 0.6190868765115738 Val Loss: tensor(54.7361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9242 Loss: 0.6191436201334 Val Loss: tensor(54.8666, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9243 Loss: 0.6237121373414993 Val Loss: tensor(54.7247, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9244 Loss: 0.6155878901481628 Val Loss: tensor(54.9018, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9245 Loss: 0.6131693869829178 Val Loss: tensor(54.7088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9246 Loss: 0.5993609577417374 Val Loss: tensor(54.9130, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9247 Loss: 0.5916513800621033 Val Loss: tensor(54.6878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9248 Loss: 0.5754659026861191 Val Loss: tensor(54.9008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9249 Loss: 0.565273717045784 Val Loss: tensor(54.6628, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9250 Loss: 0.5495098829269409 Val Loss: tensor(54.8683, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9251 Loss: 0.5392705947160721 Val Loss: tensor(54.6360, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9252 Loss: 0.5257682055234909 Val Loss: tensor(54.8244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9253 Loss: 0.5169971883296967 Val Loss: tensor(54.6094, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9254 Loss: 0.5065048485994339 Val Loss: tensor(54.7796, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9255 Loss: 0.499831423163414 Val Loss: tensor(54.5852, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9256 Loss: 0.49225279688835144 Val Loss: tensor(54.7405, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9257 Loss: 0.48762278258800507 Val Loss: tensor(54.5659, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9258 Loss: 0.4824771583080292 Val Loss: tensor(54.7088, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9259 Loss: 0.47941240668296814 Val Loss: tensor(54.5533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9260 Loss: 0.4761403948068619 Val Loss: tensor(54.6835, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9261 Loss: 0.4741455316543579 Val Loss: tensor(54.5472, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9262 Loss: 0.47227928042411804 Val Loss: tensor(54.6630, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9263 Loss: 0.4709381014108658 Val Loss: tensor(54.5468, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9264 Loss: 0.47013217210769653 Val Loss: tensor(54.6451, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9265 Loss: 0.4691782295703888 Val Loss: tensor(54.5508, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9266 Loss: 0.4691404849290848 Val Loss: tensor(54.6284, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9267 Loss: 0.4684441387653351 Val Loss: tensor(54.5584, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9268 Loss: 0.46900245547294617 Val Loss: tensor(54.6118, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9269 Loss: 0.4685201495885849 Val Loss: tensor(54.5691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9270 Loss: 0.4695740044116974 Val Loss: tensor(54.5946, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9271 Loss: 0.46930545568466187 Val Loss: tensor(54.5821, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9272 Loss: 0.47079335153102875 Val Loss: tensor(54.5766, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9273 Loss: 0.4708383083343506 Val Loss: tensor(54.5973, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9274 Loss: 0.47273895144462585 Val Loss: tensor(54.5576, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9275 Loss: 0.4731852412223816 Val Loss: tensor(54.6143, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9276 Loss: 0.4755338132381439 Val Loss: tensor(54.5380, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9277 Loss: 0.47654642164707184 Val Loss: tensor(54.6331, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9278 Loss: 0.47938287258148193 Val Loss: tensor(54.5180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9279 Loss: 0.4811510443687439 Val Loss: tensor(54.6535, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9280 Loss: 0.48457153141498566 Val Loss: tensor(54.4981, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9281 Loss: 0.48728130757808685 Val Loss: tensor(54.6755, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9282 Loss: 0.4913925528526306 Val Loss: tensor(54.4795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9283 Loss: 0.4951990395784378 Val Loss: tensor(54.6986, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9284 Loss: 0.5000693649053574 Val Loss: tensor(54.4637, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9285 Loss: 0.5050012916326523 Val Loss: tensor(54.7223, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9286 Loss: 0.510581836104393 Val Loss: tensor(54.4530, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9287 Loss: 0.5164923369884491 Val Loss: tensor(54.7451, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9288 Loss: 0.5224995613098145 Val Loss: tensor(54.4496, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9289 Loss: 0.5289391428232193 Val Loss: tensor(54.7648, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9290 Loss: 0.5347916930913925 Val Loss: tensor(54.4557, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9291 Loss: 0.5409888029098511 Val Loss: tensor(54.7780, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9292 Loss: 0.545793280005455 Val Loss: tensor(54.4728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9293 Loss: 0.5507567524909973 Val Loss: tensor(54.7810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9294 Loss: 0.5534713566303253 Val Loss: tensor(54.5004, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9295 Loss: 0.5563840270042419 Val Loss: tensor(54.7708, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9296 Loss: 0.5562466681003571 Val Loss: tensor(54.5363, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9297 Loss: 0.5568356513977051 Val Loss: tensor(54.7467, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9298 Loss: 0.5538973808288574 Val Loss: tensor(54.5772, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9299 Loss: 0.5527669936418533 Val Loss: tensor(54.7109, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9300 Loss: 0.5481110513210297 Val Loss: tensor(54.6198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9301 Loss: 0.5467228591442108 Val Loss: tensor(54.6677, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9302 Loss: 0.5422061830759048 Val Loss: tensor(54.6626, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9303 Loss: 0.5425610840320587 Val Loss: tensor(54.6223, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9304 Loss: 0.5402411967515945 Val Loss: tensor(54.7060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9305 Loss: 0.5445706397294998 Val Loss: tensor(54.5786, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9306 Loss: 0.5460529029369354 Val Loss: tensor(54.7518, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9307 Loss: 0.556520089507103 Val Loss: tensor(54.5401, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9308 Loss: 0.5624941438436508 Val Loss: tensor(54.8020, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9309 Loss: 0.5808462351560593 Val Loss: tensor(54.5104, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9310 Loss: 0.5906749367713928 Val Loss: tensor(54.8562, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9311 Loss: 0.6172708868980408 Val Loss: tensor(54.4944, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9312 Loss: 0.6284862011671066 Val Loss: tensor(54.9091, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9313 Loss: 0.6606756448745728 Val Loss: tensor(54.4978, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9314 Loss: 0.668835386633873 Val Loss: tensor(54.9493, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9315 Loss: 0.6994412690401077 Val Loss: tensor(54.5243, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9316 Loss: 0.6993869096040726 Val Loss: tensor(54.9620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9317 Loss: 0.7183601409196854 Val Loss: tensor(54.5710, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9318 Loss: 0.7075430601835251 Val Loss: tensor(54.9386, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9319 Loss: 0.7080508172512054 Val Loss: tensor(54.6259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9320 Loss: 0.6890095174312592 Val Loss: tensor(54.8853, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9321 Loss: 0.6733033061027527 Val Loss: tensor(54.6717, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9322 Loss: 0.6519303917884827 Val Loss: tensor(54.8215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9323 Loss: 0.629309818148613 Val Loss: tensor(54.6966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9324 Loss: 0.6108556538820267 Val Loss: tensor(54.7661, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9325 Loss: 0.5906433761119843 Val Loss: tensor(54.7022, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9326 Loss: 0.5780741423368454 Val Loss: tensor(54.7293, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9327 Loss: 0.5652052164077759 Val Loss: tensor(54.7006, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9328 Loss: 0.5595660656690598 Val Loss: tensor(54.7099, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9329 Loss: 0.5545194745063782 Val Loss: tensor(54.7046, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9330 Loss: 0.555424302816391 Val Loss: tensor(54.7015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9331 Loss: 0.5560886114835739 Val Loss: tensor(54.7202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9332 Loss: 0.5622486770153046 Val Loss: tensor(54.6982, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9333 Loss: 0.5660577565431595 Val Loss: tensor(54.7457, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9334 Loss: 0.5758785456418991 Val Loss: tensor(54.6976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9335 Loss: 0.5809579342603683 Val Loss: tensor(54.7754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9336 Loss: 0.5928757637739182 Val Loss: tensor(54.6990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9337 Loss: 0.5980548113584518 Val Loss: tensor(54.8042, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9338 Loss: 0.6105992197990417 Val Loss: tensor(54.7006, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9339 Loss: 0.6149760335683823 Val Loss: tensor(54.8297, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9340 Loss: 0.6267953366041183 Val Loss: tensor(54.6991, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9341 Loss: 0.6295152008533478 Val Loss: tensor(54.8521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9342 Loss: 0.6395125985145569 Val Loss: tensor(54.6915, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9343 Loss: 0.6401167809963226 Val Loss: tensor(54.8722, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9344 Loss: 0.6478359699249268 Val Loss: tensor(54.6778, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9345 Loss: 0.6466699093580246 Val Loss: tensor(54.8913, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9346 Loss: 0.6525425761938095 Val Loss: tensor(54.6622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9347 Loss: 0.6509149372577667 Val Loss: tensor(54.9103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9348 Loss: 0.6559918373823166 Val Loss: tensor(54.6515, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9349 Loss: 0.6555561870336533 Val Loss: tensor(54.9301, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9350 Loss: 0.6606827676296234 Val Loss: tensor(54.6509, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9351 Loss: 0.6624657809734344 Val Loss: tensor(54.9499, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9352 Loss: 0.6676484495401382 Val Loss: tensor(54.6610, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9353 Loss: 0.6713485419750214 Val Loss: tensor(54.9674, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9354 Loss: 0.6756808906793594 Val Loss: tensor(54.6775, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9355 Loss: 0.6797188818454742 Val Loss: tensor(54.9781, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9356 Loss: 0.6817122846841812 Val Loss: tensor(54.6941, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9357 Loss: 0.683992326259613 Val Loss: tensor(54.9766, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9358 Loss: 0.6822015345096588 Val Loss: tensor(54.7062, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9359 Loss: 0.6809743940830231 Val Loss: tensor(54.9587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9360 Loss: 0.6746639013290405 Val Loss: tensor(54.7127, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9361 Loss: 0.6692191064357758 Val Loss: tensor(54.9237, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9362 Loss: 0.6586978882551193 Val Loss: tensor(54.7155, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9363 Loss: 0.6497117131948471 Val Loss: tensor(54.8764, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9364 Loss: 0.6365348845720291 Val Loss: tensor(54.7178, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9365 Loss: 0.6260465979576111 Val Loss: tensor(54.8262, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9366 Loss: 0.612761378288269 Val Loss: tensor(54.7227, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9367 Loss: 0.603523001074791 Val Loss: tensor(54.7829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9368 Loss: 0.5928435176610947 Val Loss: tensor(54.7330, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9369 Loss: 0.5874830186367035 Val Loss: tensor(54.7525, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9370 Loss: 0.5813575834035873 Val Loss: tensor(54.7514, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9371 Loss: 0.581564724445343 Val Loss: tensor(54.7355, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9372 Loss: 0.580682784318924 Val Loss: tensor(54.7795, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9373 Loss: 0.5869776159524918 Val Loss: tensor(54.7282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9374 Loss: 0.5910301953554153 Val Loss: tensor(54.8181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9375 Loss: 0.6028535217046738 Val Loss: tensor(54.7263, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9376 Loss: 0.6109871119260788 Val Loss: tensor(54.8665, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9377 Loss: 0.626898318529129 Val Loss: tensor(54.7265, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9378 Loss: 0.6380186080932617 Val Loss: tensor(54.9232, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9379 Loss: 0.6558584272861481 Val Loss: tensor(54.7274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9380 Loss: 0.6685724705457687 Val Loss: tensor(54.9842, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9381 Loss: 0.6855434030294418 Val Loss: tensor(54.7277, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9382 Loss: 0.6977240890264511 Val Loss: tensor(55.0422, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9383 Loss: 0.7106326967477798 Val Loss: tensor(54.7253, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9384 Loss: 0.7192883193492889 Val Loss: tensor(55.0861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9385 Loss: 0.7251463830471039 Val Loss: tensor(54.7172, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9386 Loss: 0.7268539667129517 Val Loss: tensor(55.1033, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9387 Loss: 0.7237199693918228 Val Loss: tensor(54.7013, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9388 Loss: 0.7161717861890793 Val Loss: tensor(55.0855, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9389 Loss: 0.7040929049253464 Val Loss: tensor(54.6784, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9390 Loss: 0.6876645237207413 Val Loss: tensor(55.0344, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9391 Loss: 0.6691217720508575 Val Loss: tensor(54.6526, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9392 Loss: 0.6473832428455353 Val Loss: tensor(54.9630, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9393 Loss: 0.6267634481191635 Val Loss: tensor(54.6302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9394 Loss: 0.6047756224870682 Val Loss: tensor(54.8890, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9395 Loss: 0.5865768194198608 Val Loss: tensor(54.6169, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9396 Loss: 0.5686341673135757 Val Loss: tensor(54.8271, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9397 Loss: 0.5557777136564255 Val Loss: tensor(54.6165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9398 Loss: 0.5441355854272842 Val Loss: tensor(54.7834, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9399 Loss: 0.5373861789703369 Val Loss: tensor(54.6302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9400 Loss: 0.5324435532093048 Val Loss: tensor(54.7563, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9401 Loss: 0.5310358256101608 Val Loss: tensor(54.6569, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9402 Loss: 0.5320400893688202 Val Loss: tensor(54.7403, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9403 Loss: 0.5346499383449554 Val Loss: tensor(54.6940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9404 Loss: 0.5402493476867676 Val Loss: tensor(54.7299, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9405 Loss: 0.5455207079648972 Val Loss: tensor(54.7388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9406 Loss: 0.5541836023330688 Val Loss: tensor(54.7200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9407 Loss: 0.560988262295723 Val Loss: tensor(54.7888, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9408 Loss: 0.5712745189666748 Val Loss: tensor(54.7079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9409 Loss: 0.5786754637956619 Val Loss: tensor(54.8412, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9410 Loss: 0.5893146991729736 Val Loss: tensor(54.6923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9411 Loss: 0.5965386778116226 Val Loss: tensor(54.8923, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9412 Loss: 0.6064749211072922 Val Loss: tensor(54.6734, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9413 Loss: 0.6126997768878937 Val Loss: tensor(54.9377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9414 Loss: 0.620915099978447 Val Loss: tensor(54.6522, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9415 Loss: 0.6251827776432037 Val Loss: tensor(54.9716, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9416 Loss: 0.6306044012308121 Val Loss: tensor(54.6300, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9417 Loss: 0.6318240463733673 Val Loss: tensor(54.9889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9418 Loss: 0.6334648430347443 Val Loss: tensor(54.6083, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9419 Loss: 0.630708634853363 Val Loss: tensor(54.9859, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9420 Loss: 0.6280123889446259 Val Loss: tensor(54.5886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9421 Loss: 0.6210578978061676 Val Loss: tensor(54.9627, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9422 Loss: 0.614276111125946 Val Loss: tensor(54.5725, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9423 Loss: 0.6039862483739853 Val Loss: tensor(54.9234, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9424 Loss: 0.5944023132324219 Val Loss: tensor(54.5620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9425 Loss: 0.5825648009777069 Val Loss: tensor(54.8752, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9426 Loss: 0.5721660405397415 Val Loss: tensor(54.5594, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9427 Loss: 0.5609818696975708 Val Loss: tensor(54.8263, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9428 Loss: 0.5518338680267334 Val Loss: tensor(54.5667, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9429 Loss: 0.5432144552469254 Val Loss: tensor(54.7832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9430 Loss: 0.536864623427391 Val Loss: tensor(54.5851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9431 Loss: 0.5320612043142319 Val Loss: tensor(54.7495, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9432 Loss: 0.5292458981275558 Val Loss: tensor(54.6147, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9433 Loss: 0.528766855597496 Val Loss: tensor(54.7259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9434 Loss: 0.5295703709125519 Val Loss: tensor(54.6546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9435 Loss: 0.5332239419221878 Val Loss: tensor(54.7107, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9436 Loss: 0.5372552871704102 Val Loss: tensor(54.7032, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9437 Loss: 0.5443744957447052 Val Loss: tensor(54.7007, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9438 Loss: 0.5509384125471115 Val Loss: tensor(54.7585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9439 Loss: 0.5605365484952927 Val Loss: tensor(54.6924, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9440 Loss: 0.5687725096940994 Val Loss: tensor(54.8179, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9441 Loss: 0.5797670036554337 Val Loss: tensor(54.6825, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9442 Loss: 0.5887180864810944 Val Loss: tensor(54.8785, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9443 Loss: 0.6001222133636475 Val Loss: tensor(54.6691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9444 Loss: 0.6087898761034012 Val Loss: tensor(54.9361, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9445 Loss: 0.6196925044059753 Val Loss: tensor(54.6524, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9446 Loss: 0.6269488036632538 Val Loss: tensor(54.9856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9447 Loss: 0.6363032907247543 Val Loss: tensor(54.6339, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9448 Loss: 0.640748530626297 Val Loss: tensor(55.0198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9449 Loss: 0.6471785306930542 Val Loss: tensor(54.6161, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9450 Loss: 0.6473079025745392 Val Loss: tensor(55.0323, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9451 Loss: 0.6492534428834915 Val Loss: tensor(54.6008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9452 Loss: 0.6439938694238663 Val Loss: tensor(55.0193, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9453 Loss: 0.6404765099287033 Val Loss: tensor(54.5888, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9454 Loss: 0.630031630396843 Val Loss: tensor(54.9824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9455 Loss: 0.6215215623378754 Val Loss: tensor(54.5808, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9456 Loss: 0.6077781915664673 Val Loss: tensor(54.9296, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9457 Loss: 0.596381887793541 Val Loss: tensor(54.5781, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9458 Loss: 0.5822197496891022 Val Loss: tensor(54.8721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9459 Loss: 0.570854127407074 Val Loss: tensor(54.5827, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9460 Loss: 0.5590650737285614 Val Loss: tensor(54.8200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9461 Loss: 0.5501972138881683 Val Loss: tensor(54.5969, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9462 Loss: 0.5425855368375778 Val Loss: tensor(54.7800, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9463 Loss: 0.5375072211027145 Val Loss: tensor(54.6217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9464 Loss: 0.534670352935791 Val Loss: tensor(54.7539, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9465 Loss: 0.5335414111614227 Val Loss: tensor(54.6568, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9466 Loss: 0.5351090580224991 Val Loss: tensor(54.7396, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9467 Loss: 0.5373242944478989 Val Loss: tensor(54.7005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9468 Loss: 0.5422244817018509 Val Loss: tensor(54.7328, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9469 Loss: 0.5467881113290787 Val Loss: tensor(54.7503, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9470 Loss: 0.5536272823810577 Val Loss: tensor(54.7280, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9471 Loss: 0.5594542175531387 Val Loss: tensor(54.8038, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9472 Loss: 0.5669671893119812 Val Loss: tensor(54.7200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9473 Loss: 0.5730836242437363 Val Loss: tensor(54.8581, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9474 Loss: 0.5805135816335678 Val Loss: tensor(54.7061, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9475 Loss: 0.5862175673246384 Val Loss: tensor(54.9105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9476 Loss: 0.5933791100978851 Val Loss: tensor(54.6861, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9477 Loss: 0.5982352942228317 Val Loss: tensor(54.9573, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9478 Loss: 0.6051915436983109 Val Loss: tensor(54.6627, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9479 Loss: 0.6086942255496979 Val Loss: tensor(54.9940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9480 Loss: 0.6151147335767746 Val Loss: tensor(54.6399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9481 Loss: 0.6164264529943466 Val Loss: tensor(55.0156, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9482 Loss: 0.6212553083896637 Val Loss: tensor(54.6210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9483 Loss: 0.6193379461765289 Val Loss: tensor(55.0182, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9484 Loss: 0.6211381703615189 Val Loss: tensor(54.6079, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9485 Loss: 0.6152797043323517 Val Loss: tensor(55.0004, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9486 Loss: 0.6130345314741135 Val Loss: tensor(54.6008, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9487 Loss: 0.6036207377910614 Val Loss: tensor(54.9654, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9488 Loss: 0.5975224524736404 Val Loss: tensor(54.5993, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9489 Loss: 0.5861258059740067 Val Loss: tensor(54.9199, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9490 Loss: 0.5777154862880707 Val Loss: tensor(54.6033, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9491 Loss: 0.5665384083986282 Val Loss: tensor(54.8727, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9492 Loss: 0.5579278767108917 Val Loss: tensor(54.6133, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9493 Loss: 0.5488748699426651 Val Loss: tensor(54.8309, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9494 Loss: 0.5418459326028824 Val Loss: tensor(54.6300, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9495 Loss: 0.5360116064548492 Val Loss: tensor(54.7988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9496 Loss: 0.5314715057611465 Val Loss: tensor(54.6537, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9497 Loss: 0.5290872305631638 Val Loss: tensor(54.7772, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9498 Loss: 0.5271629989147186 Val Loss: tensor(54.6838, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9499 Loss: 0.5277834832668304 Val Loss: tensor(54.7641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9500 Loss: 0.5281289517879486 Val Loss: tensor(54.7194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9501 Loss: 0.5309188067913055 Val Loss: tensor(54.7556, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9502 Loss: 0.5329918414354324 Val Loss: tensor(54.7592, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9503 Loss: 0.5370840728282928 Val Loss: tensor(54.7476, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9504 Loss: 0.5403754711151123 Val Loss: tensor(54.8019, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9505 Loss: 0.5451589077711105 Val Loss: tensor(54.7365, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9506 Loss: 0.549291267991066 Val Loss: tensor(54.8463, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9507 Loss: 0.5546673983335495 Val Loss: tensor(54.7206, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9508 Loss: 0.5594733208417892 Val Loss: tensor(54.8914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9509 Loss: 0.5657462179660797 Val Loss: tensor(54.7005, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9510 Loss: 0.5711010843515396 Val Loss: tensor(54.9353, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9511 Loss: 0.5786922574043274 Val Loss: tensor(54.6785, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9512 Loss: 0.5842550247907639 Val Loss: tensor(54.9752, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9513 Loss: 0.593091145157814 Val Loss: tensor(54.6585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9514 Loss: 0.5979705154895782 Val Loss: tensor(55.0073, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9515 Loss: 0.6071076989173889 Val Loss: tensor(54.6433, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9516 Loss: 0.6098800897598267 Val Loss: tensor(55.0268, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9517 Loss: 0.6175014674663544 Val Loss: tensor(54.6353, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9518 Loss: 0.6166280955076218 Val Loss: tensor(55.0293, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9519 Loss: 0.6206604093313217 Val Loss: tensor(54.6344, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9520 Loss: 0.6153443604707718 Val Loss: tensor(55.0128, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9521 Loss: 0.6144595742225647 Val Loss: tensor(54.6392, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9522 Loss: 0.6053440123796463 Val Loss: tensor(54.9801, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9523 Loss: 0.5997990965843201 Val Loss: tensor(54.6478, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9524 Loss: 0.5888361930847168 Val Loss: tensor(54.9378, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9525 Loss: 0.5804608166217804 Val Loss: tensor(54.6590, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9526 Loss: 0.5700090974569321 Val Loss: tensor(54.8948, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9527 Loss: 0.5611877739429474 Val Loss: tensor(54.6728, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9528 Loss: 0.5530427247285843 Val Loss: tensor(54.8587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9529 Loss: 0.5456138551235199 Val Loss: tensor(54.6897, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9530 Loss: 0.5405908226966858 Val Loss: tensor(54.8329, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9531 Loss: 0.5353882312774658 Val Loss: tensor(54.7103, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9532 Loss: 0.5334093123674393 Val Loss: tensor(54.8174, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9533 Loss: 0.5304669737815857 Val Loss: tensor(54.7349, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9534 Loss: 0.5309220105409622 Val Loss: tensor(54.8097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9535 Loss: 0.5298540890216827 Val Loss: tensor(54.7629, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9536 Loss: 0.5318302512168884 Val Loss: tensor(54.8051, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9537 Loss: 0.5321738719940186 Val Loss: tensor(54.7938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9538 Loss: 0.5348462760448456 Val Loss: tensor(54.7996, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9539 Loss: 0.5361990183591843 Val Loss: tensor(54.8274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9540 Loss: 0.5390863120555878 Val Loss: tensor(54.7894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9541 Loss: 0.5412373393774033 Val Loss: tensor(54.8633, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9542 Loss: 0.5444141626358032 Val Loss: tensor(54.7730, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9543 Loss: 0.5473834276199341 Val Loss: tensor(54.9015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9544 Loss: 0.551522433757782 Val Loss: tensor(54.7506, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9545 Loss: 0.5554380267858505 Val Loss: tensor(54.9415, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9546 Loss: 0.5614166706800461 Val Loss: tensor(54.7248, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9547 Loss: 0.5662665218114853 Val Loss: tensor(54.9820, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9548 Loss: 0.5747259259223938 Val Loss: tensor(54.6995, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9549 Loss: 0.5800276845693588 Val Loss: tensor(55.0206, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9550 Loss: 0.5907401293516159 Val Loss: tensor(54.6789, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9551 Loss: 0.5953306555747986 Val Loss: tensor(55.0528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9552 Loss: 0.6069084852933884 Val Loss: tensor(54.6665, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9553 Loss: 0.6090817898511887 Val Loss: tensor(55.0730, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9554 Loss: 0.6191553473472595 Val Loss: tensor(54.6639, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9555 Loss: 0.6172789484262466 Val Loss: tensor(55.0758, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9556 Loss: 0.6232570111751556 Val Loss: tensor(54.6705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9557 Loss: 0.6167803853750229 Val Loss: tensor(55.0587, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9558 Loss: 0.6170748025178909 Val Loss: tensor(54.6832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9559 Loss: 0.607099786400795 Val Loss: tensor(55.0252, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9560 Loss: 0.6021497845649719 Val Loss: tensor(54.6986, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9561 Loss: 0.5910647958517075 Val Loss: tensor(54.9841, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9562 Loss: 0.583079606294632 Val Loss: tensor(54.7140, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9563 Loss: 0.57334104180336 Val Loss: tensor(54.9459, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9564 Loss: 0.5649372339248657 Val Loss: tensor(54.7286, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9565 Loss: 0.5580302774906158 Val Loss: tensor(54.9180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9566 Loss: 0.5509605407714844 Val Loss: tensor(54.7427, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9567 Loss: 0.5472491234540939 Val Loss: tensor(54.9033, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9568 Loss: 0.5421122014522552 Val Loss: tensor(54.7571, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9569 Loss: 0.5412246137857437 Val Loss: tensor(54.9003, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9570 Loss: 0.5378225445747375 Val Loss: tensor(54.7724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9571 Loss: 0.5389651954174042 Val Loss: tensor(54.9053, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9572 Loss: 0.5368086099624634 Val Loss: tensor(54.7886, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9573 Loss: 0.5389756858348846 Val Loss: tensor(54.9136, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9574 Loss: 0.5375309139490128 Val Loss: tensor(54.8060, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9575 Loss: 0.5396726429462433 Val Loss: tensor(54.9207, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9576 Loss: 0.5384745895862579 Val Loss: tensor(54.8244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9577 Loss: 0.5397334396839142 Val Loss: tensor(54.9226, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9578 Loss: 0.5384805798530579 Val Loss: tensor(54.8441, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9579 Loss: 0.5384028106927872 Val Loss: tensor(54.9165, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9580 Loss: 0.5369844436645508 Val Loss: tensor(54.8652, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9581 Loss: 0.5357173532247543 Val Loss: tensor(54.9009, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9582 Loss: 0.5342836827039719 Val Loss: tensor(54.8878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9583 Loss: 0.5325551480054855 Val Loss: tensor(54.8768, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9584 Loss: 0.5314684361219406 Val Loss: tensor(54.9120, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9585 Loss: 0.5303230881690979 Val Loss: tensor(54.8464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9586 Loss: 0.530017077922821 Val Loss: tensor(54.9374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9587 Loss: 0.5304368287324905 Val Loss: tensor(54.8135, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9588 Loss: 0.531190499663353 Val Loss: tensor(54.9634, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9589 Loss: 0.5338177531957626 Val Loss: tensor(54.7818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9590 Loss: 0.5356093198060989 Val Loss: tensor(54.9889, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9591 Loss: 0.5405392050743103 Val Loss: tensor(54.7549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9592 Loss: 0.5429617017507553 Val Loss: tensor(55.0119, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9593 Loss: 0.5497337281703949 Val Loss: tensor(54.7358, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9594 Loss: 0.5520281940698624 Val Loss: tensor(55.0294, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9595 Loss: 0.5596551150083542 Val Loss: tensor(54.7270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9596 Loss: 0.5609035193920135 Val Loss: tensor(55.0373, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9597 Loss: 0.5679792761802673 Val Loss: tensor(54.7299, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9598 Loss: 0.5673681497573853 Val Loss: tensor(55.0313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9599 Loss: 0.5724236220121384 Val Loss: tensor(54.7444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9600 Loss: 0.5697159022092819 Val Loss: tensor(55.0093, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9601 Loss: 0.5717759132385254 Val Loss: tensor(54.7692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9602 Loss: 0.5675148069858551 Val Loss: tensor(54.9729, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9603 Loss: 0.5665579438209534 Val Loss: tensor(54.8016, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9604 Loss: 0.5618782937526703 Val Loss: tensor(54.9274, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9605 Loss: 0.5588454455137253 Val Loss: tensor(54.8390, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9606 Loss: 0.5549854189157486 Val Loss: tensor(54.8792, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9607 Loss: 0.5512712895870209 Val Loss: tensor(54.8786, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9608 Loss: 0.5490543097257614 Val Loss: tensor(54.8344, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9609 Loss: 0.5458333343267441 Val Loss: tensor(54.9180, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9610 Loss: 0.5455950498580933 Val Loss: tensor(54.7965, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9611 Loss: 0.5434256345033646 Val Loss: tensor(54.9546, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9612 Loss: 0.5450703352689743 Val Loss: tensor(54.7671, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9613 Loss: 0.5438343584537506 Val Loss: tensor(54.9851, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9614 Loss: 0.5469576418399811 Val Loss: tensor(54.7471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9615 Loss: 0.5460124164819717 Val Loss: tensor(55.0059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9616 Loss: 0.5499321967363358 Val Loss: tensor(54.7367, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9617 Loss: 0.5483303964138031 Val Loss: tensor(55.0128, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9618 Loss: 0.5521491914987564 Val Loss: tensor(54.7359, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9619 Loss: 0.5490680932998657 Val Loss: tensor(55.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9620 Loss: 0.5519743114709854 Val Loss: tensor(54.7440, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9621 Loss: 0.5470698028802872 Val Loss: tensor(54.9763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9622 Loss: 0.5486202836036682 Val Loss: tensor(54.7599, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9623 Loss: 0.5422469228506088 Val Loss: tensor(54.9364, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9624 Loss: 0.542678639292717 Val Loss: tensor(54.7818, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9625 Loss: 0.5357340425252914 Val Loss: tensor(54.8898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9626 Loss: 0.5358078628778458 Val Loss: tensor(54.8083, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9627 Loss: 0.5293009281158447 Val Loss: tensor(54.8433, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9628 Loss: 0.5299790501594543 Val Loss: tensor(54.8381, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9629 Loss: 0.5246627777814865 Val Loss: tensor(54.8023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9630 Loss: 0.5266114920377731 Val Loss: tensor(54.8699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9631 Loss: 0.5228798091411591 Val Loss: tensor(54.7701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9632 Loss: 0.5263686627149582 Val Loss: tensor(54.9021, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9633 Loss: 0.5243656635284424 Val Loss: tensor(54.7485, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9634 Loss: 0.5291971266269684 Val Loss: tensor(54.9319, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9635 Loss: 0.5289137810468674 Val Loss: tensor(54.7390, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9636 Loss: 0.5344789773225784 Val Loss: tensor(54.9549, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9637 Loss: 0.535743460059166 Val Loss: tensor(54.7431, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9638 Loss: 0.5410726368427277 Val Loss: tensor(54.9647, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9639 Loss: 0.5434512794017792 Val Loss: tensor(54.7622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9640 Loss: 0.5475270450115204 Val Loss: tensor(54.9537, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9641 Loss: 0.5504143238067627 Val Loss: tensor(54.7970, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9642 Loss: 0.5528973788022995 Val Loss: tensor(54.9162, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9643 Loss: 0.5559048652648926 Val Loss: tensor(54.8473, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9644 Loss: 0.5580044090747833 Val Loss: tensor(54.8524, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9645 Loss: 0.5617252737283707 Val Loss: tensor(54.9132, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9646 Loss: 0.5666197687387466 Val Loss: tensor(54.7705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9647 Loss: 0.5729557275772095 Val Loss: tensor(54.9959, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9648 Loss: 0.585252121090889 Val Loss: tensor(54.6856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9649 Loss: 0.596869632601738 Val Loss: tensor(55.0968, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9650 Loss: 0.6215239614248276 Val Loss: tensor(54.6175, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9651 Loss: 0.6402787566184998 Val Loss: tensor(55.2147, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9652 Loss: 0.681500032544136 Val Loss: tensor(54.5887, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9653 Loss: 0.7055530250072479 Val Loss: tensor(55.3376, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9654 Loss: 0.7639127969741821 Val Loss: tensor(54.6235, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9655 Loss: 0.7834340929985046 Val Loss: tensor(55.4300, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9656 Loss: 0.8488486558198929 Val Loss: tensor(54.7348, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9657 Loss: 0.8458571285009384 Val Loss: tensor(55.4335, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9658 Loss: 0.8931331634521484 Val Loss: tensor(54.8988, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9659 Loss: 0.8582456558942795 Val Loss: tensor(55.3203, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9660 Loss: 0.8660259544849396 Val Loss: tensor(55.0544, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9661 Loss: 0.8176017105579376 Val Loss: tensor(55.1528, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9662 Loss: 0.7969270497560501 Val Loss: tensor(55.1557, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9663 Loss: 0.7607855200767517 Val Loss: tensor(55.0294, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9664 Loss: 0.7383304238319397 Val Loss: tensor(55.1961, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9665 Loss: 0.7201849073171616 Val Loss: tensor(54.9972, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9666 Loss: 0.7067451179027557 Val Loss: tensor(55.1845, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9667 Loss: 0.6997466087341309 Val Loss: tensor(55.0386, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9668 Loss: 0.6939187049865723 Val Loss: tensor(55.1409, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9669 Loss: 0.693348690867424 Val Loss: tensor(55.1055, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9670 Loss: 0.6940228193998337 Val Loss: tensor(55.0990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9671 Loss: 0.6999902576208115 Val Loss: tensor(55.1615, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9672 Loss: 0.7057766616344452 Val Loss: tensor(55.0841, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9673 Loss: 0.7177486717700958 Val Loss: tensor(55.1901, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9674 Loss: 0.7245080173015594 Val Loss: tensor(55.0983, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9675 Loss: 0.7391657382249832 Val Loss: tensor(55.1856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9676 Loss: 0.7424467653036118 Val Loss: tensor(55.1282, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9677 Loss: 0.755903035402298 Val Loss: tensor(55.1520, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9678 Loss: 0.7542193531990051 Val Loss: tensor(55.1598, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9679 Loss: 0.7648888379335403 Val Loss: tensor(55.1044, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9680 Loss: 0.7602743953466415 Val Loss: tensor(55.1876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9681 Loss: 0.7701830267906189 Val Loss: tensor(55.0640, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9682 Loss: 0.7667186260223389 Val Loss: tensor(55.2130, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9683 Loss: 0.779954269528389 Val Loss: tensor(55.0499, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9684 Loss: 0.7810826599597931 Val Loss: tensor(55.2374, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9685 Loss: 0.7998564690351486 Val Loss: tensor(55.0720, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9686 Loss: 0.8058786541223526 Val Loss: tensor(55.2560, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9687 Loss: 0.8276176750659943 Val Loss: tensor(55.1244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9688 Loss: 0.834690049290657 Val Loss: tensor(55.2577, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9689 Loss: 0.85195092856884 Val Loss: tensor(55.1850, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9690 Loss: 0.852399542927742 Val Loss: tensor(55.2290, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9691 Loss: 0.8556226938962936 Val Loss: tensor(55.2259, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9692 Loss: 0.8411089181900024 Val Loss: tensor(55.1616, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9693 Loss: 0.8247338682413101 Val Loss: tensor(55.2329, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9694 Loss: 0.7933321595191956 Val Loss: tensor(55.0641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9695 Loss: 0.7618423104286194 Val Loss: tensor(55.2151, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9696 Loss: 0.7222282737493515 Val Loss: tensor(54.9595, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9697 Loss: 0.6875595152378082 Val Loss: tensor(55.1906, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9698 Loss: 0.6524203419685364 Val Loss: tensor(54.8705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9699 Loss: 0.6241636723279953 Val Loss: tensor(55.1657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9700 Loss: 0.6000884920358658 Val Loss: tensor(54.8073, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9701 Loss: 0.5802794694900513 Val Loss: tensor(55.1353, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9702 Loss: 0.5658964663743973 Val Loss: tensor(54.7691, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9703 Loss: 0.5517991781234741 Val Loss: tensor(55.0956, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9704 Loss: 0.5429068207740784 Val Loss: tensor(54.7491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9705 Loss: 0.5316691696643829 Val Loss: tensor(55.0495, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9706 Loss: 0.5253080427646637 Val Loss: tensor(54.7388, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9707 Loss: 0.5157870799303055 Val Loss: tensor(55.0029, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9708 Loss: 0.510770782828331 Val Loss: tensor(54.7323, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9709 Loss: 0.5029371827840805 Val Loss: tensor(54.9597, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9710 Loss: 0.4988976716995239 Val Loss: tensor(54.7281, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9711 Loss: 0.49284689128398895 Val Loss: tensor(54.9207, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9712 Loss: 0.48960258066654205 Val Loss: tensor(54.7268, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9713 Loss: 0.48520056903362274 Val Loss: tensor(54.8858, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9714 Loss: 0.48261862993240356 Val Loss: tensor(54.7285, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9715 Loss: 0.47958889603614807 Val Loss: tensor(54.8542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9716 Loss: 0.4776473492383957 Val Loss: tensor(54.7325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9717 Loss: 0.4756852090358734 Val Loss: tensor(54.8251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9718 Loss: 0.4743538498878479 Val Loss: tensor(54.7380, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9719 Loss: 0.4732518196105957 Val Loss: tensor(54.7976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9720 Loss: 0.47253401577472687 Val Loss: tensor(54.7449, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9721 Loss: 0.47216202318668365 Val Loss: tensor(54.7709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9722 Loss: 0.47208425402641296 Val Loss: tensor(54.7537, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9723 Loss: 0.47241173684597015 Val Loss: tensor(54.7445, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9724 Loss: 0.47302867472171783 Val Loss: tensor(54.7649, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9725 Loss: 0.4740723967552185 Val Loss: tensor(54.7186, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9726 Loss: 0.47548842430114746 Val Loss: tensor(54.7786, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9727 Loss: 0.47731921076774597 Val Loss: tensor(54.6933, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9728 Loss: 0.4796234369277954 Val Loss: tensor(54.7952, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9729 Loss: 0.4823509454727173 Val Loss: tensor(54.6692, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9730 Loss: 0.48558442294597626 Val Loss: tensor(54.8143, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9731 Loss: 0.4893094003200531 Val Loss: tensor(54.6467, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9732 Loss: 0.4934706389904022 Val Loss: tensor(54.8357, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9733 Loss: 0.4982706159353256 Val Loss: tensor(54.6264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9734 Loss: 0.5032366812229156 Val Loss: tensor(54.8590, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9735 Loss: 0.5091341137886047 Val Loss: tensor(54.6091, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9736 Loss: 0.5146362334489822 Val Loss: tensor(54.8831, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9737 Loss: 0.5215198993682861 Val Loss: tensor(54.5959, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9738 Loss: 0.5271719843149185 Val Loss: tensor(54.9068, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9739 Loss: 0.5347441732883453 Val Loss: tensor(54.5877, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9740 Loss: 0.5399946868419647 Val Loss: tensor(54.9278, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9741 Loss: 0.5477180778980255 Val Loss: tensor(54.5856, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9742 Loss: 0.5519388616085052 Val Loss: tensor(54.9438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9743 Loss: 0.5590122044086456 Val Loss: tensor(54.5903, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9744 Loss: 0.5615901350975037 Val Loss: tensor(54.9521, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9745 Loss: 0.5671009719371796 Val Loss: tensor(54.6016, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9746 Loss: 0.5676056891679764 Val Loss: tensor(54.9506, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9747 Loss: 0.5707794725894928 Val Loss: tensor(54.6188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9748 Loss: 0.5691699981689453 Val Loss: tensor(54.9389, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9749 Loss: 0.5696477293968201 Val Loss: tensor(54.6404, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9750 Loss: 0.5663261413574219 Val Loss: tensor(54.9181, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9751 Loss: 0.5643985718488693 Val Loss: tensor(54.6651, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9752 Loss: 0.5601725429296494 Val Loss: tensor(54.8908, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9753 Loss: 0.5567227303981781 Val Loss: tensor(54.6918, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9754 Loss: 0.5526229441165924 Val Loss: tensor(54.8601, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9755 Loss: 0.5488627105951309 Val Loss: tensor(54.7210, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9756 Loss: 0.5459173917770386 Val Loss: tensor(54.8283, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9757 Loss: 0.543045163154602 Val Loss: tensor(54.7533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9758 Loss: 0.5422704070806503 Val Loss: tensor(54.7973, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9759 Loss: 0.541262224316597 Val Loss: tensor(54.7898, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9760 Loss: 0.5435656905174255 Val Loss: tensor(54.7681, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9761 Loss: 0.5450478494167328 Val Loss: tensor(54.8311, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9762 Loss: 0.5513053834438324 Val Loss: tensor(54.7416, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9763 Loss: 0.5553981512784958 Val Loss: tensor(54.8769, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9764 Loss: 0.5664108991622925 Val Loss: tensor(54.7187, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9765 Loss: 0.5726250559091568 Val Loss: tensor(54.9261, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9766 Loss: 0.5889524668455124 Val Loss: tensor(54.7006, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9767 Loss: 0.5959311127662659 Val Loss: tensor(54.9756, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9768 Loss: 0.6174735873937607 Val Loss: tensor(54.6878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9769 Loss: 0.6228160113096237 Val Loss: tensor(55.0204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9770 Loss: 0.6479901522397995 Val Loss: tensor(54.6802, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9771 Loss: 0.6483647227287292 Val Loss: tensor(55.0532, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9772 Loss: 0.673229917883873 Val Loss: tensor(54.6754, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9773 Loss: 0.6652816087007523 Val Loss: tensor(55.0668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9774 Loss: 0.683879017829895 Val Loss: tensor(54.6699, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9775 Loss: 0.6661691069602966 Val Loss: tensor(55.0575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9776 Loss: 0.673444464802742 Val Loss: tensor(54.6607, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9777 Loss: 0.6484769433736801 Val Loss: tensor(55.0308, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9778 Loss: 0.6445529460906982 Val Loss: tensor(54.6484, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9779 Loss: 0.6183736324310303 Val Loss: tensor(54.9991, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9780 Loss: 0.60920549929142 Val Loss: tensor(54.6373, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9781 Loss: 0.5880425870418549 Val Loss: tensor(54.9752, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9782 Loss: 0.5808276534080505 Val Loss: tensor(54.6324, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9783 Loss: 0.5679911822080612 Val Loss: tensor(54.9655, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9784 Loss: 0.5668104887008667 Val Loss: tensor(54.6372, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9785 Loss: 0.5624261498451233 Val Loss: tensor(54.9698, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9786 Loss: 0.5678854435682297 Val Loss: tensor(54.6533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9787 Loss: 0.570261225104332 Val Loss: tensor(54.9831, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9788 Loss: 0.5811685919761658 Val Loss: tensor(54.6810, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9789 Loss: 0.5877088457345963 Val Loss: tensor(54.9990, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9790 Loss: 0.6022056639194489 Val Loss: tensor(54.7202, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9791 Loss: 0.6098180860280991 Val Loss: tensor(55.0100, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9792 Loss: 0.6260943561792374 Val Loss: tensor(54.7706, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9793 Loss: 0.6321307569742203 Val Loss: tensor(55.0102, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9794 Loss: 0.6491198092699051 Val Loss: tensor(54.8336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9795 Loss: 0.6530968546867371 Val Loss: tensor(54.9989, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9796 Loss: 0.6715544909238815 Val Loss: tensor(54.9131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9797 Loss: 0.6761208921670914 Val Loss: tensor(54.9824, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9798 Loss: 0.6987926810979843 Val Loss: tensor(55.0131, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9799 Loss: 0.7082097232341766 Val Loss: tensor(54.9710, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9800 Loss: 0.7380646020174026 Val Loss: tensor(55.1338, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9801 Loss: 0.7549371868371964 Val Loss: tensor(54.9724, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9802 Loss: 0.7920399308204651 Val Loss: tensor(55.2663, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9803 Loss: 0.8147478550672531 Val Loss: tensor(54.9859, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9804 Loss: 0.8536182194948196 Val Loss: tensor(55.3879, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9805 Loss: 0.8753845542669296 Val Loss: tensor(54.9998, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9806 Loss: 0.9046603143215179 Val Loss: tensor(55.4624, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9807 Loss: 0.9146778434514999 Val Loss: tensor(54.9950, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9808 Loss: 0.9212071299552917 Val Loss: tensor(55.4542, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9809 Loss: 0.9099052846431732 Val Loss: tensor(54.9575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9810 Loss: 0.887693852186203 Val Loss: tensor(55.3576, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9811 Loss: 0.8548392653465271 Val Loss: tensor(54.8880, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9812 Loss: 0.8111631870269775 Val Loss: tensor(55.2105, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9813 Loss: 0.7677967846393585 Val Loss: tensor(54.8015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9814 Loss: 0.7180618494749069 Val Loss: tensor(55.0675, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9815 Loss: 0.6779046803712845 Val Loss: tensor(54.7166, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9816 Loss: 0.6350617408752441 Val Loss: tensor(54.9616, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9817 Loss: 0.6056952774524689 Val Loss: tensor(54.6499, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9818 Loss: 0.5749699026346207 Val Loss: tensor(54.8957, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9819 Loss: 0.5567727088928223 Val Loss: tensor(54.6097, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9820 Loss: 0.5370612889528275 Val Loss: tensor(54.8555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9821 Loss: 0.5267401337623596 Val Loss: tensor(54.5926, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9822 Loss: 0.514585331082344 Val Loss: tensor(54.8279, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9823 Loss: 0.5086701810359955 Val Loss: tensor(54.5890, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9824 Loss: 0.501015916466713 Val Loss: tensor(54.8069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9825 Loss: 0.4972613602876663 Val Loss: tensor(54.5900, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9826 Loss: 0.4922310709953308 Val Loss: tensor(54.7904, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9827 Loss: 0.4894675016403198 Val Loss: tensor(54.5916, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9828 Loss: 0.48603351414203644 Val Loss: tensor(54.7779, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9829 Loss: 0.483755961060524 Val Loss: tensor(54.5928, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9830 Loss: 0.4813927710056305 Val Loss: tensor(54.7682, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9831 Loss: 0.479393407702446 Val Loss: tensor(54.5938, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9832 Loss: 0.4777909219264984 Val Loss: tensor(54.7603, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9833 Loss: 0.4760027974843979 Val Loss: tensor(54.5941, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9834 Loss: 0.47497864067554474 Val Loss: tensor(54.7533, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9835 Loss: 0.47338496148586273 Val Loss: tensor(54.5934, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9836 Loss: 0.47280295193195343 Val Loss: tensor(54.7464, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9837 Loss: 0.4713788479566574 Val Loss: tensor(54.5917, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9838 Loss: 0.4711294174194336 Val Loss: tensor(54.7395, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9839 Loss: 0.4698704332113266 Val Loss: tensor(54.5894, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9840 Loss: 0.46986207365989685 Val Loss: tensor(54.7326, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9841 Loss: 0.4687424749135971 Val Loss: tensor(54.5876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9842 Loss: 0.46894124150276184 Val Loss: tensor(54.7256, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9843 Loss: 0.4679511934518814 Val Loss: tensor(54.5868, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9844 Loss: 0.46835486590862274 Val Loss: tensor(54.7184, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9845 Loss: 0.4675218164920807 Val Loss: tensor(54.5876, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9846 Loss: 0.4681388735771179 Val Loss: tensor(54.7107, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9847 Loss: 0.46752598881721497 Val Loss: tensor(54.5906, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9848 Loss: 0.4684017300605774 Val Loss: tensor(54.7024, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9849 Loss: 0.46814143657684326 Val Loss: tensor(54.5963, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9850 Loss: 0.4693839102983475 Val Loss: tensor(54.6931, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9851 Loss: 0.46969984471797943 Val Loss: tensor(54.6056, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9852 Loss: 0.4714670330286026 Val Loss: tensor(54.6830, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9853 Loss: 0.47268253564834595 Val Loss: tensor(54.6194, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9854 Loss: 0.4752781540155411 Val Loss: tensor(54.6721, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9855 Loss: 0.4778803437948227 Val Loss: tensor(54.6391, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9856 Loss: 0.4817282557487488 Val Loss: tensor(54.6604, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9857 Loss: 0.48642534017562866 Val Loss: tensor(54.6668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9858 Loss: 0.49220916628837585 Val Loss: tensor(54.6483, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9859 Loss: 0.4999445378780365 Val Loss: tensor(54.7047, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9860 Loss: 0.5086824595928192 Val Loss: tensor(54.6367, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9861 Loss: 0.5206818133592606 Val Loss: tensor(54.7561, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9862 Loss: 0.5337692201137543 Val Loss: tensor(54.6269, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9863 Loss: 0.5514199733734131 Val Loss: tensor(54.8242, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9864 Loss: 0.5706784576177597 Val Loss: tensor(54.6209, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9865 Loss: 0.5951852053403854 Val Loss: tensor(54.9120, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9866 Loss: 0.6225507110357285 Val Loss: tensor(54.6220, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9867 Loss: 0.6541198939085007 Val Loss: tensor(55.0204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9868 Loss: 0.6907584071159363 Val Loss: tensor(54.6336, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9869 Loss: 0.7270902693271637 Val Loss: tensor(55.1432, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9870 Loss: 0.7714202105998993 Val Loss: tensor(54.6592, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9871 Loss: 0.8057628273963928 Val Loss: tensor(55.2620, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9872 Loss: 0.850548729300499 Val Loss: tensor(54.6980, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9873 Loss: 0.8710511326789856 Val Loss: tensor(55.3428, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9874 Loss: 0.9022362232208252 Val Loss: tensor(54.7414, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9875 Loss: 0.8964724540710449 Val Loss: tensor(55.3491, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9876 Loss: 0.8993829488754272 Val Loss: tensor(54.7710, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9877 Loss: 0.8647239506244659 Val Loss: tensor(55.2725, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9878 Loss: 0.8378470242023468 Val Loss: tensor(54.7690, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9879 Loss: 0.7867113947868347 Val Loss: tensor(55.1471, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9880 Loss: 0.7456749230623245 Val Loss: tensor(54.7346, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9881 Loss: 0.6953137665987015 Val Loss: tensor(55.0214, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9882 Loss: 0.6581155210733414 Val Loss: tensor(54.6869, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9883 Loss: 0.618499293923378 Val Loss: tensor(54.9244, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9884 Loss: 0.5928622931241989 Val Loss: tensor(54.6480, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9885 Loss: 0.5657051652669907 Val Loss: tensor(54.8619, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9886 Loss: 0.5509361028671265 Val Loss: tensor(54.6289, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9887 Loss: 0.5342925488948822 Val Loss: tensor(54.8270, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9888 Loss: 0.5270089507102966 Val Loss: tensor(54.6296, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9889 Loss: 0.5181891769170761 Val Loss: tensor(54.8087, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9890 Loss: 0.5153742432594299 Val Loss: tensor(54.6445, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9891 Loss: 0.5117008090019226 Val Loss: tensor(54.7957, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9892 Loss: 0.5113969445228577 Val Loss: tensor(54.6679, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9893 Loss: 0.5105774700641632 Val Loss: tensor(54.7797, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9894 Loss: 0.5116819441318512 Val Loss: tensor(54.6973, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9895 Loss: 0.5123086720705032 Val Loss: tensor(54.7576, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9896 Loss: 0.5143109112977982 Val Loss: tensor(54.7318, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9897 Loss: 0.5159453600645065 Val Loss: tensor(54.7299, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9898 Loss: 0.5186431556940079 Val Loss: tensor(54.7714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9899 Loss: 0.521489605307579 Val Loss: tensor(54.6984, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9900 Loss: 0.5248405039310455 Val Loss: tensor(54.8147, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9901 Loss: 0.5292813777923584 Val Loss: tensor(54.6653, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9902 Loss: 0.5331467539072037 Val Loss: tensor(54.8600, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9903 Loss: 0.5394290536642075 Val Loss: tensor(54.6326, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9904 Loss: 0.5434066951274872 Val Loss: tensor(54.9044, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9905 Loss: 0.5513557195663452 Val Loss: tensor(54.6023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9906 Loss: 0.5547401309013367 Val Loss: tensor(54.9439, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9907 Loss: 0.5635982006788254 Val Loss: tensor(54.5765, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9908 Loss: 0.5654271990060806 Val Loss: tensor(54.9746, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9909 Loss: 0.5739501416683197 Val Loss: tensor(54.5569, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9910 Loss: 0.5732927024364471 Val Loss: tensor(54.9930, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9911 Loss: 0.5799143761396408 Val Loss: tensor(54.5440, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9912 Loss: 0.5761959701776505 Val Loss: tensor(54.9966, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9913 Loss: 0.5795628577470779 Val Loss: tensor(54.5379, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9914 Loss: 0.5728868544101715 Val Loss: tensor(54.9855, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9915 Loss: 0.5724144876003265 Val Loss: tensor(54.5377, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9916 Loss: 0.5636489987373352 Val Loss: tensor(54.9622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9917 Loss: 0.5598247349262238 Val Loss: tensor(54.5423, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9918 Loss: 0.5503311157226562 Val Loss: tensor(54.9314, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9919 Loss: 0.5445769131183624 Val Loss: tensor(54.5510, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9920 Loss: 0.5358032137155533 Val Loss: tensor(54.8982, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9921 Loss: 0.5299294888973236 Val Loss: tensor(54.5641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9922 Loss: 0.5230489671230316 Val Loss: tensor(54.8668, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9923 Loss: 0.5186734050512314 Val Loss: tensor(54.5829, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9924 Loss: 0.5144853442907333 Val Loss: tensor(54.8399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9925 Loss: 0.5128367096185684 Val Loss: tensor(54.6089, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9926 Loss: 0.5118139237165451 Val Loss: tensor(54.8184, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9927 Loss: 0.513714998960495 Val Loss: tensor(54.6441, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9928 Loss: 0.5161550641059875 Val Loss: tensor(54.8015, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9929 Loss: 0.5221086293458939 Val Loss: tensor(54.6903, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9930 Loss: 0.5282288044691086 Val Loss: tensor(54.7878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9931 Loss: 0.5384741127490997 Val Loss: tensor(54.7486, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9932 Loss: 0.5484351366758347 Val Loss: tensor(54.7755, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9933 Loss: 0.562827542424202 Val Loss: tensor(54.8198, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9934 Loss: 0.5768573135137558 Val Loss: tensor(54.7630, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9935 Loss: 0.5946816504001617 Val Loss: tensor(54.9031, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9936 Loss: 0.612941637635231 Val Loss: tensor(54.7500, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9937 Loss: 0.6326967030763626 Val Loss: tensor(54.9951, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9938 Loss: 0.6549219787120819 Val Loss: tensor(54.7381, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9939 Loss: 0.6740036755800247 Val Loss: tensor(55.0884, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9940 Loss: 0.6986816227436066 Val Loss: tensor(54.7302, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9941 Loss: 0.7132783383131027 Val Loss: tensor(55.1701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9942 Loss: 0.7366584986448288 Val Loss: tensor(54.7289, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9943 Loss: 0.7421868592500687 Val Loss: tensor(55.2229, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9944 Loss: 0.7584817707538605 Val Loss: tensor(54.7340, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9945 Loss: 0.7514844983816147 Val Loss: tensor(55.2314, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9946 Loss: 0.7553770244121552 Val Loss: tensor(54.7407, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9947 Loss: 0.7361466288566589 Val Loss: tensor(55.1914, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9948 Loss: 0.7267282158136368 Val Loss: tensor(54.7425, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9949 Loss: 0.7000419199466705 Val Loss: tensor(55.1145, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9950 Loss: 0.6821497827768326 Val Loss: tensor(54.7368, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9951 Loss: 0.6545609086751938 Val Loss: tensor(55.0222, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9952 Loss: 0.6352997720241547 Val Loss: tensor(54.7275, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9953 Loss: 0.6115663349628448 Val Loss: tensor(54.9333, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9954 Loss: 0.5959278643131256 Val Loss: tensor(54.7208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9955 Loss: 0.577896922826767 Val Loss: tensor(54.8586, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9956 Loss: 0.567439928650856 Val Loss: tensor(54.7215, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9957 Loss: 0.5550028532743454 Val Loss: tensor(54.8003, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9958 Loss: 0.5490404367446899 Val Loss: tensor(54.7304, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9959 Loss: 0.541324257850647 Val Loss: tensor(54.7555, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9960 Loss: 0.538561150431633 Val Loss: tensor(54.7461, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9961 Loss: 0.5345571041107178 Val Loss: tensor(54.7204, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9962 Loss: 0.5339676737785339 Val Loss: tensor(54.7657, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9963 Loss: 0.5327470749616623 Val Loss: tensor(54.6921, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9964 Loss: 0.533674493432045 Val Loss: tensor(54.7866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9965 Loss: 0.5345206409692764 Val Loss: tensor(54.6695, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9966 Loss: 0.5366556346416473 Val Loss: tensor(54.8069, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9967 Loss: 0.5390467494726181 Val Loss: tensor(54.6535, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9968 Loss: 0.5422430485486984 Val Loss: tensor(54.8251, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9969 Loss: 0.5458172261714935 Val Loss: tensor(54.6451, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9970 Loss: 0.550072431564331 Val Loss: tensor(54.8404, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9971 Loss: 0.5546538382768631 Val Loss: tensor(54.6455, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9972 Loss: 0.5600700825452805 Val Loss: tensor(54.8520, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9973 Loss: 0.5656262934207916 Val Loss: tensor(54.6559, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9974 Loss: 0.5723754912614822 Val Loss: tensor(54.8593, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9975 Loss: 0.5790097862482071 Val Loss: tensor(54.6774, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9976 Loss: 0.5872765779495239 Val Loss: tensor(54.8614, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9977 Loss: 0.5951820611953735 Val Loss: tensor(54.7111, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9978 Loss: 0.6050839275121689 Val Loss: tensor(54.8572, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9979 Loss: 0.6144056767225266 Val Loss: tensor(54.7575, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9980 Loss: 0.6259193271398544 Val Loss: tensor(54.8458, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9981 Loss: 0.6366538554430008 Val Loss: tensor(54.8159, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9982 Loss: 0.6495814919471741 Val Loss: tensor(54.8269, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9983 Loss: 0.6614506989717484 Val Loss: tensor(54.8838, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9984 Loss: 0.6753357797861099 Val Loss: tensor(54.8014, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9985 Loss: 0.68764428794384 Val Loss: tensor(54.9564, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9986 Loss: 0.7017131745815277 Val Loss: tensor(54.7711, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9987 Loss: 0.7131087929010391 Val Loss: tensor(55.0264, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9988 Loss: 0.7260448932647705 Val Loss: tensor(54.7387, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9989 Loss: 0.7342376708984375 Val Loss: tensor(55.0839, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9990 Loss: 0.7440139353275299 Val Loss: tensor(54.7066, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9991 Loss: 0.7459246516227722 Val Loss: tensor(55.1179, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9992 Loss: 0.7498468160629272 Val Loss: tensor(54.6761, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9993 Loss: 0.7425558716058731 Val Loss: tensor(55.1190, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9994 Loss: 0.7380483448505402 Val Loss: tensor(54.6479, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9995 Loss: 0.720576137304306 Val Loss: tensor(55.0845, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9996 Loss: 0.706798642873764 Val Loss: tensor(54.6217, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9997 Loss: 0.6814472526311874 Val Loss: tensor(55.0208, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9998 Loss: 0.6606544852256775 Val Loss: tensor(54.5975, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9999 Loss: 0.6324742883443832 Val Loss: tensor(54.9426, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# it works!\n",
    "sim_ae = SimpleAutoEncoder(16, 12)\n",
    "sim_ae.fit(data[0, :, :16], val=test_data[0, :, :16], epochs=10_000, lr=0.0005, VERBOSE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d88f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load(\"model_data/test.npy\")\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.mse_loss(sim_ae(test_data[0, :, :16]), test_data[0, :, :16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06baf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it works!\n",
    "ae = AutoEncoder(16, 10)\n",
    "ae.fit(data[:, :, :16], epochs=1000, lr=0.01, VERBOSE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948333a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = StackedAutoEncoder(5, 16, 10)\n",
    "sae.fit(data[0, :, :16], epochs=100, lr=0.01, VERBOSE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bcedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0, 0, :16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ae(data[0, 0, :16])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
