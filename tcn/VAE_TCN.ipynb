{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.utils as param\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('tick_data/train.npy')\n",
    "test_data = np.load('tick_data/test.npy')\n",
    "val_data = np.load('tick_data/val.npy')\n",
    "\n",
    "X_train = train_data[:,:,:-1].reshape(333,730,6,16).reshape(333*730,6,16)\n",
    "X_val = val_data[:,:,:-1].reshape(40,730,6,16).reshape(40*730,6,16)\n",
    "Y_train = train_data[:,:,-1].reshape(-1,1)\n",
    "Y_val = val_data[:,:,-1].reshape(-1,1)\n",
    "\n",
    "X_train = torch.tensor(X_train).to(device)\n",
    "Y_train = torch.tensor(Y_train.reshape(-1)).to(device)\n",
    "X_val = torch.tensor(X_val).to(device)\n",
    "Y_val = torch.tensor(Y_val.reshape(-1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilation_rates=[1,2] #,4,8]\n",
    "sequence_length = 6\n",
    "num_features = 16\n",
    "num_epochs = 5\n",
    "latent_dim = 70\n",
    "hidden_dim = 400\n",
    "t_max = num_epochs\n",
    "batch_size = 2048\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=num_features, hidden_dim=hidden_dim, latent_dim=latent_dim ):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act1 = nn.PReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act2 = nn.PReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.act1(self.fc1(x))\n",
    "        h = self.act2(self.fc2(h))\n",
    "        h = self.act3(self.fc3(h))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc2(z))\n",
    "        return torch.sigmoid(self.fc3(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single block of dilated convolution\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, input_channels, num_filters, kernel_size, dilation_rate):\n",
    "        super(TCNBlock, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_channels, num_filters, kernel_size,\n",
    "                                dilation=dilation_rate, padding='same',\n",
    "                                bias=False)\n",
    "        self.conv1d = param.weight_norm(self.conv1d)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "# Define the Temporal Convolutional Network\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_channels=latent_dim, num_filters=64, kernel_size=3, num_blocks=4, dilation_rates=[1,2,]):\n",
    "        super(TCN, self).__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.blocks.append(VAE())\n",
    "        current_input_channels = input_channels\n",
    "\n",
    "        for dilation_rate in dilation_rates:\n",
    "            self.blocks.append(TCNBlock(current_input_channels, num_filters, kernel_size, dilation_rate))\n",
    "            current_input_channels = num_filters\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(latent_dim)\n",
    "        self.ff1 = nn.Linear(num_filters*sequence_length, 1)\n",
    "        self.tcnact = nn.Tanhshrink()\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = 0\n",
    "        for block in self.blocks:\n",
    "            if b == 0:\n",
    "                x, x_mu, x_logvar = block(x)\n",
    "                N = x.shape[0]\n",
    "                x = x.view(N * sequence_length, latent_dim)\n",
    "                x = self.batch_norm(x)\n",
    "                x = x.view(N, sequence_length, latent_dim)\n",
    "                x = torch.transpose(x,1,2)\n",
    "            else:\n",
    "                x = block(x)\n",
    "            b+=1\n",
    "        \n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.ff1(x)\n",
    "        x1 = self.tcnact(x)\n",
    "        return x1, x_mu, x_logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Training Loss: 32.495, MSE Loss: 0.009761, , KL Loss: 2.713169, Test Loss: 0.009371, Learning Rate: 0.0009140576474687263\n",
      "Epoch [2/5] - Training Loss: 0.991, MSE Loss: 0.008386, , KL Loss: 1.328184, Test Loss: 0.007914, Learning Rate: 0.0006890576474687263\n",
      "Epoch [3/5] - Training Loss: 0.707, MSE Loss: 0.007374, , KL Loss: 0.682163, Test Loss: 0.007238, Learning Rate: 0.0004109423525312736\n",
      "Epoch [4/5] - Training Loss: 0.605, MSE Loss: 0.007111, , KL Loss: 0.484102, Test Loss: 0.006896, Learning Rate: 0.00018594235253127368\n",
      "Epoch [5/5] - Training Loss: 0.588, MSE Loss: 0.006646, , KL Loss: 0.399354, Test Loss: 0.006573, Learning Rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "tcn = TCN().to(device)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(tcn.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=t_max, eta_min=0.0001)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train.reshape(-1))\n",
    "train_dataset = train_dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val, Y_val.reshape(-1))\n",
    "val_dataset = val_dataset\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_hist = []\n",
    "val_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    tcn.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        lower_val = torch.quantile(batch_X, 0.01, dim=0, keepdim=True)\n",
    "        upper_val = torch.quantile(batch_X, 0.99, dim=0, keepdim=True)\n",
    "        batch_X = torch.clamp(batch_X, min=lower_val, max=upper_val)\n",
    "        batch_X = batch_X.float()\n",
    "        predictions, p_mu, p_logvar = tcn(batch_X.float())\n",
    "\n",
    "        predictions = predictions.float()  \n",
    "        batch_y = batch_y.float()  \n",
    "        p_logvar = p_logvar.float()  \n",
    "        p_mu = p_mu.float() \n",
    "\n",
    "        lossmse = criterion(predictions, batch_y)\n",
    "        losskl = -0.5 * torch.sum(1 + p_logvar - p_mu.pow(2) - p_logvar.exp())\n",
    "\n",
    "        beta = (epoch + 1) / num_epochs\n",
    "        #beta = 0.1\n",
    "        loss = lossmse + beta * losskl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    average_loss = total_loss/len(train_loader)\n",
    "    train_hist.append(average_loss)\n",
    "\n",
    "    tcn.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            lower_valv = torch.quantile(batch_X_val, 0.01, dim=0, keepdim=True)\n",
    "            upper_valv = torch.quantile(batch_X_val, 0.99, dim=0, keepdim=True)\n",
    "            batch_X_val = torch.clamp(batch_X, min=lower_valv, max=upper_valv)\n",
    "            predictions_val, _, _ = tcn(batch_X_val.float())\n",
    "\n",
    "        \n",
    "            lossmsev = criterion(predictions_val,batch_y_val.float())\n",
    "            val_loss = lossmsev\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "        average_val_loss = total_val_loss / len(val_loader)\n",
    "        val_hist.append(average_val_loss)\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {average_loss:.3f}, MSE Loss: {lossmse:.6f}, , KL Loss: {losskl:.6f}, Test Loss: {average_val_loss:.6f}, Learning Rate: {optimizer.param_groups[0][\"lr\"]}')\n",
    "        #print(predictions.max().item(), p_logvar.max().item(), p_mu.max().item(), predictions.min().item(), p_logvar.min().item(),p_mu.min().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
