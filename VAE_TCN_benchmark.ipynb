{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.utils as param\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else \"mps\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(Path('tick_data/train.npy'))\n",
    "test_data = np.load(Path('tick_data/test.npy'))\n",
    "val_data = np.load(Path('tick_data/val.npy'))\n",
    "\n",
    "X_train = train_data[:,:,:-1].reshape(333,730,6,16).reshape(333*730,6,16)\n",
    "X_val = val_data[:,:,:-1].reshape(40,730,6,16).reshape(40*730,6,16)\n",
    "Y_train = train_data[:,:,-1].reshape(-1,1)\n",
    "Y_val = val_data[:,:,-1].reshape(-1,1)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "Y_train = torch.tensor(Y_train.reshape(-1), dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "Y_val = torch.tensor(Y_val.reshape(-1), dtype=torch.float32).to(device)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([482, 20, 20])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(np.load(Path(\"model_data/x_train.npy\")), dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(np.load(Path(\"model_data/x_val.npy\")), dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(np.load(Path(\"model_data/x_test.npy\")), dtype=torch.float32).to(device)\n",
    "Y_train = torch.tensor(np.load(Path(\"model_data/y_train.npy\")), dtype=torch.float32)[:, -1].to(device)\n",
    "Y_val = torch.tensor(np.load(Path(\"model_data/y_val.npy\")), dtype=torch.float32)[:, -1].to(device)\n",
    "Y_test = torch.tensor(np.load(Path(\"model_data/y_test.npy\")), dtype=torch.float32)[:, -1].to(device)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([482])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilation_rates=[1,2] #,4,8]\n",
    "# sequence_length = 6\n",
    "sequence_length = 20\n",
    "# num_features = 16\n",
    "num_features = 20\n",
    "num_epochs = 1_000\n",
    "latent_dim = 70\n",
    "hidden_dim = 400\n",
    "t_max = num_epochs\n",
    "batch_size = 2048\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=num_features, hidden_dim=hidden_dim, latent_dim=latent_dim ):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act1 = nn.PReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act2 = nn.PReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.act1(self.fc1(x))\n",
    "        h = self.act2(self.fc2(h))\n",
    "        h = self.act3(self.fc3(h))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc2(z))\n",
    "        return torch.sigmoid(self.fc3(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single block of dilated convolution\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, input_channels, num_filters, kernel_size, dilation_rate):\n",
    "        super(TCNBlock, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_channels, num_filters, kernel_size,\n",
    "                                dilation=dilation_rate, padding='same',\n",
    "                                bias=False)\n",
    "        self.conv1d = param.weight_norm(self.conv1d)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "# Define the Temporal Convolutional Network\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_channels=latent_dim, num_filters=64, kernel_size=3, num_blocks=4, dilation_rates=[1,2,]):\n",
    "        super(TCN, self).__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.blocks.append(VAE())\n",
    "        current_input_channels = input_channels\n",
    "\n",
    "        for dilation_rate in dilation_rates:\n",
    "            self.blocks.append(TCNBlock(current_input_channels, num_filters, kernel_size, dilation_rate))\n",
    "            current_input_channels = num_filters\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(latent_dim)\n",
    "        self.ff1 = nn.Linear(num_filters*sequence_length, 1)\n",
    "        self.tcnact = nn.Tanhshrink()\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = 0\n",
    "        for block in self.blocks:\n",
    "            if b == 0:\n",
    "                x, x_mu, x_logvar = block(x)\n",
    "                N = x.shape[0]\n",
    "                x = x.view(N * sequence_length, latent_dim)\n",
    "                x = self.batch_norm(x)\n",
    "                x = x.view(N, sequence_length, latent_dim)\n",
    "                x = torch.transpose(x,1,2)\n",
    "            else:\n",
    "                x = block(x)\n",
    "            b+=1\n",
    "        \n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.ff1(x)\n",
    "        x1 = self.tcnact(x)\n",
    "        return x1, x_mu, x_logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] - Training Loss: 1.192, MSE Loss: 0.324099, , KL Loss: 867.838806, Test Loss: 0.267465, Learning Rate: 0.0009999977793408363\n",
      "Epoch [2/1000] - Training Loss: 0.986, MSE Loss: 0.320355, , KL Loss: 332.916901, Test Loss: 0.255497, Learning Rate: 0.0009999911173852617\n",
      "Epoch [3/1000] - Training Loss: 0.790, MSE Loss: 0.306265, , KL Loss: 161.122238, Test Loss: 0.225266, Learning Rate: 0.0009999800141990275\n",
      "Epoch [4/1000] - Training Loss: 0.734, MSE Loss: 0.271019, , KL Loss: 115.697540, Test Loss: 0.170967, Learning Rate: 0.0009999644698917174\n",
      "Epoch [5/1000] - Training Loss: 1.197, MSE Loss: 0.209810, , KL Loss: 197.382706, Test Loss: 0.103436, Learning Rate: 0.0009999444846167473\n",
      "Epoch [6/1000] - Training Loss: 0.685, MSE Loss: 0.130737, , KL Loss: 92.418350, Test Loss: 0.070350, Learning Rate: 0.0009999200585713642\n",
      "Epoch [7/1000] - Training Loss: 1.052, MSE Loss: 0.069865, , KL Loss: 140.275330, Test Loss: 0.163303, Learning Rate: 0.000999891191996643\n",
      "Epoch [8/1000] - Training Loss: 0.839, MSE Loss: 0.129442, , KL Loss: 88.704514, Test Loss: 0.170954, Learning Rate: 0.000999857885177485\n",
      "Epoch [9/1000] - Training Loss: 0.777, MSE Loss: 0.141975, , KL Loss: 70.595490, Test Loss: 0.117685, Learning Rate: 0.0009998201384426152\n",
      "Epoch [10/1000] - Training Loss: 0.723, MSE Loss: 0.094094, , KL Loss: 62.878864, Test Loss: 0.075798, Learning Rate: 0.000999777952164579\n",
      "Epoch [11/1000] - Training Loss: 0.599, MSE Loss: 0.067638, , KL Loss: 48.268303, Test Loss: 0.069808, Learning Rate: 0.0009997313267597376\n",
      "Epoch [12/1000] - Training Loss: 0.515, MSE Loss: 0.075375, , KL Loss: 36.615257, Test Loss: 0.077762, Learning Rate: 0.0009996802626882648\n",
      "Epoch [13/1000] - Training Loss: 0.439, MSE Loss: 0.091156, , KL Loss: 26.771229, Test Loss: 0.084560, Learning Rate: 0.0009996247604541425\n",
      "Epoch [14/1000] - Training Loss: 1.230, MSE Loss: 0.102134, , KL Loss: 80.554199, Test Loss: 0.085572, Learning Rate: 0.0009995648206051558\n",
      "Epoch [15/1000] - Training Loss: 0.809, MSE Loss: 0.101015, , KL Loss: 47.170349, Test Loss: 0.080065, Learning Rate: 0.0009995004437328861\n",
      "Epoch [16/1000] - Training Loss: 0.933, MSE Loss: 0.094559, , KL Loss: 52.426514, Test Loss: 0.071888, Learning Rate: 0.0009994316304727075\n",
      "Epoch [17/1000] - Training Loss: 0.684, MSE Loss: 0.083292, , KL Loss: 35.346886, Test Loss: 0.067793, Learning Rate: 0.000999358381503779\n",
      "Epoch [18/1000] - Training Loss: 0.459, MSE Loss: 0.070576, , KL Loss: 21.603752, Test Loss: 0.071784, Learning Rate: 0.0009992806975490382\n",
      "Epoch [19/1000] - Training Loss: 0.481, MSE Loss: 0.067128, , KL Loss: 21.807060, Test Loss: 0.086842, Learning Rate: 0.0009991985793751947\n",
      "Epoch [20/1000] - Training Loss: 0.484, MSE Loss: 0.071638, , KL Loss: 20.619566, Test Loss: 0.099492, Learning Rate: 0.0009991120277927214\n",
      "Epoch [21/1000] - Training Loss: 0.470, MSE Loss: 0.081974, , KL Loss: 18.474167, Test Loss: 0.104594, Learning Rate: 0.000999021043655848\n",
      "Epoch [22/1000] - Training Loss: 0.430, MSE Loss: 0.082823, , KL Loss: 15.782887, Test Loss: 0.092591, Learning Rate: 0.0009989256278625505\n",
      "Epoch [23/1000] - Training Loss: 0.460, MSE Loss: 0.076001, , KL Loss: 16.676044, Test Loss: 0.079586, Learning Rate: 0.0009988257813545447\n",
      "Epoch [24/1000] - Training Loss: 0.402, MSE Loss: 0.068631, , KL Loss: 13.910627, Test Loss: 0.070108, Learning Rate: 0.0009987215051172752\n",
      "Epoch [25/1000] - Training Loss: 0.358, MSE Loss: 0.065321, , KL Loss: 11.698552, Test Loss: 0.067121, Learning Rate: 0.0009986128001799066\n",
      "Epoch [26/1000] - Training Loss: 0.344, MSE Loss: 0.067841, , KL Loss: 10.636301, Test Loss: 0.067698, Learning Rate: 0.0009984996676153121\n",
      "Epoch [27/1000] - Training Loss: 0.395, MSE Loss: 0.071649, , KL Loss: 11.993090, Test Loss: 0.068208, Learning Rate: 0.0009983821085400653\n",
      "Epoch [28/1000] - Training Loss: 0.351, MSE Loss: 0.073286, , KL Loss: 9.932037, Test Loss: 0.068473, Learning Rate: 0.0009982601241144264\n",
      "Epoch [29/1000] - Training Loss: 0.516, MSE Loss: 0.072980, , KL Loss: 15.260332, Test Loss: 0.067278, Learning Rate: 0.0009981337155423322\n",
      "Epoch [30/1000] - Training Loss: 0.332, MSE Loss: 0.070611, , KL Loss: 8.698986, Test Loss: 0.067586, Learning Rate: 0.0009980028840713845\n",
      "Epoch [31/1000] - Training Loss: 0.482, MSE Loss: 0.067317, , KL Loss: 13.383857, Test Loss: 0.069461, Learning Rate: 0.0009978676309928371\n",
      "Epoch [32/1000] - Training Loss: 0.437, MSE Loss: 0.065325, , KL Loss: 11.606981, Test Loss: 0.074495, Learning Rate: 0.0009977279576415833\n",
      "Epoch [33/1000] - Training Loss: 0.461, MSE Loss: 0.066273, , KL Loss: 11.969534, Test Loss: 0.079930, Learning Rate: 0.0009975838653961426\n",
      "Epoch [34/1000] - Training Loss: 0.318, MSE Loss: 0.068215, , KL Loss: 7.332865, Test Loss: 0.080903, Learning Rate: 0.0009974353556786474\n",
      "Epoch [35/1000] - Training Loss: 0.399, MSE Loss: 0.069587, , KL Loss: 9.402638, Test Loss: 0.080373, Learning Rate: 0.0009972824299548288\n",
      "Epoch [36/1000] - Training Loss: 0.394, MSE Loss: 0.068324, , KL Loss: 9.033927, Test Loss: 0.075762, Learning Rate: 0.0009971250897340017\n",
      "Epoch [37/1000] - Training Loss: 0.587, MSE Loss: 0.066647, , KL Loss: 14.063691, Test Loss: 0.070927, Learning Rate: 0.0009969633365690504\n",
      "Epoch [38/1000] - Training Loss: 0.401, MSE Loss: 0.065495, , KL Loss: 8.826818, Test Loss: 0.068441, Learning Rate: 0.0009967971720564138\n",
      "Epoch [39/1000] - Training Loss: 0.338, MSE Loss: 0.065855, , KL Loss: 6.966564, Test Loss: 0.066937, Learning Rate: 0.0009966265978360684\n",
      "Epoch [40/1000] - Training Loss: 0.382, MSE Loss: 0.067406, , KL Loss: 7.868710, Test Loss: 0.066986, Learning Rate: 0.0009964516155915129\n",
      "Epoch [41/1000] - Training Loss: 0.377, MSE Loss: 0.067853, , KL Loss: 7.548537, Test Loss: 0.067281, Learning Rate: 0.000996272227049751\n",
      "Epoch [42/1000] - Training Loss: 0.261, MSE Loss: 0.066680, , KL Loss: 4.637634, Test Loss: 0.067608, Learning Rate: 0.0009960884339812756\n",
      "Epoch [43/1000] - Training Loss: 0.274, MSE Loss: 0.066176, , KL Loss: 4.841954, Test Loss: 0.068078, Learning Rate: 0.00099590023820005\n",
      "Epoch [44/1000] - Training Loss: 0.281, MSE Loss: 0.065085, , KL Loss: 4.907912, Test Loss: 0.070480, Learning Rate: 0.0009957076415634905\n",
      "Epoch [45/1000] - Training Loss: 0.302, MSE Loss: 0.064627, , KL Loss: 5.281498, Test Loss: 0.072841, Learning Rate: 0.0009955106459724483\n",
      "Epoch [46/1000] - Training Loss: 0.378, MSE Loss: 0.065917, , KL Loss: 6.780996, Test Loss: 0.074731, Learning Rate: 0.0009953092533711901\n",
      "Epoch [47/1000] - Training Loss: 0.306, MSE Loss: 0.065786, , KL Loss: 5.115479, Test Loss: 0.074113, Learning Rate: 0.00099510346574738\n",
      "Epoch [48/1000] - Training Loss: 0.332, MSE Loss: 0.065899, , KL Loss: 5.548549, Test Loss: 0.072651, Learning Rate: 0.0009948932851320583\n",
      "Epoch [49/1000] - Training Loss: 0.258, MSE Loss: 0.065367, , KL Loss: 3.931254, Test Loss: 0.070580, Learning Rate: 0.0009946787135996233\n",
      "Epoch [50/1000] - Training Loss: 0.269, MSE Loss: 0.064716, , KL Loss: 4.087491, Test Loss: 0.068987, Learning Rate: 0.0009944597532678089\n",
      "Epoch [51/1000] - Training Loss: 0.370, MSE Loss: 0.064703, , KL Loss: 5.993954, Test Loss: 0.068452, Learning Rate: 0.0009942364062976656\n",
      "Epoch [52/1000] - Training Loss: 0.395, MSE Loss: 0.065166, , KL Loss: 6.338326, Test Loss: 0.067573, Learning Rate: 0.0009940086748935375\n",
      "Epoch [53/1000] - Training Loss: 0.297, MSE Loss: 0.065064, , KL Loss: 4.368916, Test Loss: 0.067306, Learning Rate: 0.0009937765613030418\n",
      "Epoch [54/1000] - Training Loss: 0.307, MSE Loss: 0.064971, , KL Loss: 4.480014, Test Loss: 0.067848, Learning Rate: 0.0009935400678170458\n",
      "Epoch [55/1000] - Training Loss: 0.349, MSE Loss: 0.065117, , KL Loss: 5.165646, Test Loss: 0.068567, Learning Rate: 0.0009932991967696449\n",
      "Epoch [56/1000] - Training Loss: 0.390, MSE Loss: 0.064481, , KL Loss: 5.817196, Test Loss: 0.069746, Learning Rate: 0.000993053950538139\n",
      "Epoch [57/1000] - Training Loss: 0.290, MSE Loss: 0.064491, , KL Loss: 3.950954, Test Loss: 0.070808, Learning Rate: 0.0009928043315430093\n",
      "Epoch [58/1000] - Training Loss: 0.340, MSE Loss: 0.064677, , KL Loss: 4.747198, Test Loss: 0.071119, Learning Rate: 0.0009925503422478948\n",
      "Epoch [59/1000] - Training Loss: 0.241, MSE Loss: 0.065013, , KL Loss: 2.990592, Test Loss: 0.071953, Learning Rate: 0.000992291985159567\n",
      "Epoch [60/1000] - Training Loss: 0.270, MSE Loss: 0.064692, , KL Loss: 3.429692, Test Loss: 0.071111, Learning Rate: 0.0009920292628279063\n",
      "Epoch [61/1000] - Training Loss: 0.224, MSE Loss: 0.064676, , KL Loss: 2.605202, Test Loss: 0.070374, Learning Rate: 0.0009917621778458758\n",
      "Epoch [62/1000] - Training Loss: 0.195, MSE Loss: 0.064717, , KL Loss: 2.106671, Test Loss: 0.069111, Learning Rate: 0.0009914907328494966\n",
      "Epoch [63/1000] - Training Loss: 0.207, MSE Loss: 0.064246, , KL Loss: 2.269619, Test Loss: 0.068878, Learning Rate: 0.0009912149305178212\n",
      "Epoch [64/1000] - Training Loss: 0.179, MSE Loss: 0.064790, , KL Loss: 1.790750, Test Loss: 0.067629, Learning Rate: 0.0009909347735729073\n",
      "Epoch [65/1000] - Training Loss: 0.185, MSE Loss: 0.064646, , KL Loss: 1.847698, Test Loss: 0.068008, Learning Rate: 0.0009906502647797908\n",
      "Epoch [66/1000] - Training Loss: 0.144, MSE Loss: 0.064870, , KL Loss: 1.197817, Test Loss: 0.068454, Learning Rate: 0.0009903614069464587\n",
      "Epoch [67/1000] - Training Loss: 0.208, MSE Loss: 0.064257, , KL Loss: 2.143437, Test Loss: 0.069230, Learning Rate: 0.000990068202923821\n",
      "Epoch [68/1000] - Training Loss: 0.350, MSE Loss: 0.064159, , KL Loss: 4.205523, Test Loss: 0.069627, Learning Rate: 0.000989770655605683\n",
      "Epoch [69/1000] - Training Loss: 0.385, MSE Loss: 0.064236, , KL Loss: 4.652054, Test Loss: 0.070210, Learning Rate: 0.000989468767928717\n",
      "Epoch [70/1000] - Training Loss: 0.166, MSE Loss: 0.064486, , KL Loss: 1.446982, Test Loss: 0.070555, Learning Rate: 0.0009891625428724321\n",
      "Epoch [71/1000] - Training Loss: 0.317, MSE Loss: 0.064568, , KL Loss: 3.558713, Test Loss: 0.071900, Learning Rate: 0.0009888519834591462\n",
      "Epoch [72/1000] - Training Loss: 0.151, MSE Loss: 0.064345, , KL Loss: 1.197843, Test Loss: 0.070723, Learning Rate: 0.0009885370927539554\n",
      "Epoch [73/1000] - Training Loss: 0.267, MSE Loss: 0.064173, , KL Loss: 2.781080, Test Loss: 0.069319, Learning Rate: 0.0009882178738647035\n",
      "Epoch [74/1000] - Training Loss: 0.228, MSE Loss: 0.064116, , KL Loss: 2.220062, Test Loss: 0.069112, Learning Rate: 0.0009878943299419525\n",
      "Epoch [75/1000] - Training Loss: 0.178, MSE Loss: 0.064243, , KL Loss: 1.517022, Test Loss: 0.068917, Learning Rate: 0.0009875664641789499\n",
      "Epoch [76/1000] - Training Loss: 0.162, MSE Loss: 0.064219, , KL Loss: 1.290728, Test Loss: 0.068252, Learning Rate: 0.0009872342798115986\n",
      "Epoch [77/1000] - Training Loss: 0.183, MSE Loss: 0.064177, , KL Loss: 1.539435, Test Loss: 0.068512, Learning Rate: 0.000986897780118424\n",
      "Epoch [78/1000] - Training Loss: 0.203, MSE Loss: 0.064243, , KL Loss: 1.776234, Test Loss: 0.069169, Learning Rate: 0.0009865569684205427\n",
      "Epoch [79/1000] - Training Loss: 0.256, MSE Loss: 0.064207, , KL Loss: 2.423347, Test Loss: 0.069266, Learning Rate: 0.0009862118480816282\n",
      "Epoch [80/1000] - Training Loss: 0.233, MSE Loss: 0.064137, , KL Loss: 2.106059, Test Loss: 0.069476, Learning Rate: 0.000985862422507879\n",
      "Epoch [81/1000] - Training Loss: 0.314, MSE Loss: 0.064501, , KL Loss: 3.077849, Test Loss: 0.069713, Learning Rate: 0.0009855086951479845\n",
      "Epoch [82/1000] - Training Loss: 0.216, MSE Loss: 0.063970, , KL Loss: 1.849153, Test Loss: 0.069465, Learning Rate: 0.0009851506694930907\n",
      "Epoch [83/1000] - Training Loss: 0.282, MSE Loss: 0.064090, , KL Loss: 2.619646, Test Loss: 0.069948, Learning Rate: 0.0009847883490767665\n",
      "Epoch [84/1000] - Training Loss: 0.187, MSE Loss: 0.064039, , KL Loss: 1.465007, Test Loss: 0.069083, Learning Rate: 0.0009844217374749681\n",
      "Epoch [85/1000] - Training Loss: 0.216, MSE Loss: 0.064308, , KL Loss: 1.781602, Test Loss: 0.068642, Learning Rate: 0.0009840508383060038\n",
      "Epoch [86/1000] - Training Loss: 0.161, MSE Loss: 0.064079, , KL Loss: 1.125193, Test Loss: 0.068309, Learning Rate: 0.000983675655230499\n",
      "Epoch [87/1000] - Training Loss: 0.184, MSE Loss: 0.064049, , KL Loss: 1.382475, Test Loss: 0.069144, Learning Rate: 0.000983296191951359\n",
      "Epoch [88/1000] - Training Loss: 0.208, MSE Loss: 0.063868, , KL Loss: 1.642784, Test Loss: 0.069252, Learning Rate: 0.000982912452213733\n",
      "Epoch [89/1000] - Training Loss: 0.589, MSE Loss: 0.064253, , KL Loss: 5.898839, Test Loss: 0.069367, Learning Rate: 0.0009825244398049778\n",
      "Epoch [90/1000] - Training Loss: 0.230, MSE Loss: 0.063835, , KL Loss: 1.849243, Test Loss: 0.069349, Learning Rate: 0.0009821321585546188\n",
      "Epoch [91/1000] - Training Loss: 0.917, MSE Loss: 0.064028, , KL Loss: 9.377541, Test Loss: 0.069304, Learning Rate: 0.0009817356123343138\n",
      "Epoch [92/1000] - Training Loss: 0.435, MSE Loss: 0.063984, , KL Loss: 4.034846, Test Loss: 0.069138, Learning Rate: 0.0009813348050578135\n",
      "Epoch [93/1000] - Training Loss: 0.351, MSE Loss: 0.063960, , KL Loss: 3.081491, Test Loss: 0.068864, Learning Rate: 0.0009809297406809243\n",
      "Epoch [94/1000] - Training Loss: 0.526, MSE Loss: 0.064280, , KL Loss: 4.914757, Test Loss: 0.068780, Learning Rate: 0.0009805204232014681\n",
      "Epoch [95/1000] - Training Loss: 0.270, MSE Loss: 0.063870, , KL Loss: 2.168417, Test Loss: 0.068886, Learning Rate: 0.0009801068566592428\n",
      "Epoch [96/1000] - Training Loss: 0.210, MSE Loss: 0.063914, , KL Loss: 1.519479, Test Loss: 0.068884, Learning Rate: 0.0009796890451359837\n",
      "Epoch [97/1000] - Training Loss: 0.819, MSE Loss: 0.063917, , KL Loss: 7.781067, Test Loss: 0.068529, Learning Rate: 0.0009792669927553212\n",
      "Epoch [98/1000] - Training Loss: 0.530, MSE Loss: 0.063952, , KL Loss: 4.754113, Test Loss: 0.069121, Learning Rate: 0.0009788407036827426\n",
      "Epoch [99/1000] - Training Loss: 0.410, MSE Loss: 0.063968, , KL Loss: 3.497274, Test Loss: 0.069023, Learning Rate: 0.0009784101821255487\n",
      "Epoch [100/1000] - Training Loss: 0.357, MSE Loss: 0.063842, , KL Loss: 2.926883, Test Loss: 0.068840, Learning Rate: 0.0009779754323328133\n",
      "Epoch [101/1000] - Training Loss: 0.459, MSE Loss: 0.064061, , KL Loss: 3.910349, Test Loss: 0.068524, Learning Rate: 0.0009775364585953414\n",
      "Epoch [102/1000] - Training Loss: 0.372, MSE Loss: 0.063993, , KL Loss: 3.014946, Test Loss: 0.068612, Learning Rate: 0.0009770932652456267\n",
      "Epoch [103/1000] - Training Loss: 0.275, MSE Loss: 0.063996, , KL Loss: 2.052738, Test Loss: 0.068730, Learning Rate: 0.0009766458566578084\n",
      "Epoch [104/1000] - Training Loss: 0.397, MSE Loss: 0.064115, , KL Loss: 3.204389, Test Loss: 0.068621, Learning Rate: 0.0009761942372476289\n",
      "Epoch [105/1000] - Training Loss: 0.556, MSE Loss: 0.063925, , KL Loss: 4.683134, Test Loss: 0.069492, Learning Rate: 0.0009757384114723894\n",
      "Epoch [106/1000] - Training Loss: 0.426, MSE Loss: 0.064185, , KL Loss: 3.408845, Test Loss: 0.069137, Learning Rate: 0.0009752783838309063\n",
      "Epoch [107/1000] - Training Loss: 0.439, MSE Loss: 0.063680, , KL Loss: 3.506894, Test Loss: 0.069247, Learning Rate: 0.0009748141588634665\n",
      "Epoch [108/1000] - Training Loss: 1.048, MSE Loss: 0.063726, , KL Loss: 9.116530, Test Loss: 0.068831, Learning Rate: 0.0009743457411517831\n",
      "Epoch [109/1000] - Training Loss: 0.409, MSE Loss: 0.063787, , KL Loss: 3.171630, Test Loss: 0.068884, Learning Rate: 0.0009738731353189498\n",
      "Epoch [110/1000] - Training Loss: 0.644, MSE Loss: 0.063605, , KL Loss: 5.272787, Test Loss: 0.069218, Learning Rate: 0.0009733963460293956\n",
      "Epoch [111/1000] - Training Loss: 0.451, MSE Loss: 0.063617, , KL Loss: 3.486572, Test Loss: 0.068400, Learning Rate: 0.000972915377988838\n",
      "Epoch [112/1000] - Training Loss: 0.535, MSE Loss: 0.063756, , KL Loss: 4.206591, Test Loss: 0.068710, Learning Rate: 0.0009724302359442376\n",
      "Epoch [113/1000] - Training Loss: 0.320, MSE Loss: 0.063662, , KL Loss: 2.272470, Test Loss: 0.068007, Learning Rate: 0.0009719409246837503\n",
      "Epoch [114/1000] - Training Loss: 0.440, MSE Loss: 0.063807, , KL Loss: 3.298804, Test Loss: 0.069381, Learning Rate: 0.000971447449036681\n",
      "Epoch [115/1000] - Training Loss: 0.444, MSE Loss: 0.063940, , KL Loss: 3.304251, Test Loss: 0.068963, Learning Rate: 0.0009709498138734346\n",
      "Epoch [116/1000] - Training Loss: 0.357, MSE Loss: 0.063776, , KL Loss: 2.526112, Test Loss: 0.068601, Learning Rate: 0.0009704480241054698\n",
      "Epoch [117/1000] - Training Loss: 0.327, MSE Loss: 0.063740, , KL Loss: 2.252819, Test Loss: 0.068721, Learning Rate: 0.0009699420846852487\n",
      "Epoch [118/1000] - Training Loss: 0.260, MSE Loss: 0.063664, , KL Loss: 1.663332, Test Loss: 0.068847, Learning Rate: 0.0009694320006061894\n",
      "Epoch [119/1000] - Training Loss: 0.527, MSE Loss: 0.063892, , KL Loss: 3.888232, Test Loss: 0.068386, Learning Rate: 0.0009689177769026155\n",
      "Epoch [120/1000] - Training Loss: 0.279, MSE Loss: 0.063602, , KL Loss: 1.798417, Test Loss: 0.068153, Learning Rate: 0.0009683994186497076\n",
      "Epoch [121/1000] - Training Loss: 0.387, MSE Loss: 0.063900, , KL Loss: 2.674225, Test Loss: 0.068621, Learning Rate: 0.0009678769309634523\n",
      "Epoch [122/1000] - Training Loss: 0.255, MSE Loss: 0.063373, , KL Loss: 1.566801, Test Loss: 0.068646, Learning Rate: 0.0009673503190005921\n",
      "Epoch [123/1000] - Training Loss: 0.280, MSE Loss: 0.063736, , KL Loss: 1.756161, Test Loss: 0.068752, Learning Rate: 0.0009668195879585745\n",
      "Epoch [124/1000] - Training Loss: 0.274, MSE Loss: 0.063760, , KL Loss: 1.695297, Test Loss: 0.069205, Learning Rate: 0.0009662847430755006\n",
      "Epoch [125/1000] - Training Loss: 0.294, MSE Loss: 0.063769, , KL Loss: 1.843673, Test Loss: 0.069321, Learning Rate: 0.0009657457896300736\n",
      "Epoch [126/1000] - Training Loss: 0.251, MSE Loss: 0.063784, , KL Loss: 1.487609, Test Loss: 0.068670, Learning Rate: 0.0009652027329415463\n",
      "Epoch [127/1000] - Training Loss: 0.227, MSE Loss: 0.063594, , KL Loss: 1.289683, Test Loss: 0.069246, Learning Rate: 0.0009646555783696689\n",
      "Epoch [128/1000] - Training Loss: 0.208, MSE Loss: 0.063695, , KL Loss: 1.124942, Test Loss: 0.068537, Learning Rate: 0.0009641043313146362\n",
      "Epoch [129/1000] - Training Loss: 0.295, MSE Loss: 0.064003, , KL Loss: 1.792566, Test Loss: 0.068845, Learning Rate: 0.000963548997217034\n",
      "Epoch [130/1000] - Training Loss: 0.254, MSE Loss: 0.063774, , KL Loss: 1.465565, Test Loss: 0.068933, Learning Rate: 0.0009629895815577859\n",
      "Epoch [131/1000] - Training Loss: 0.218, MSE Loss: 0.063872, , KL Loss: 1.178203, Test Loss: 0.068897, Learning Rate: 0.0009624260898580986\n",
      "Epoch [132/1000] - Training Loss: 0.202, MSE Loss: 0.063641, , KL Loss: 1.046216, Test Loss: 0.068708, Learning Rate: 0.0009618585276794073\n",
      "Epoch [133/1000] - Training Loss: 0.245, MSE Loss: 0.063740, , KL Loss: 1.362718, Test Loss: 0.068781, Learning Rate: 0.0009612869006233219\n",
      "Epoch [134/1000] - Training Loss: 0.268, MSE Loss: 0.063499, , KL Loss: 1.527704, Test Loss: 0.068210, Learning Rate: 0.0009607112143315708\n",
      "Epoch [135/1000] - Training Loss: 0.229, MSE Loss: 0.063749, , KL Loss: 1.223862, Test Loss: 0.068425, Learning Rate: 0.0009601314744859448\n",
      "Epoch [136/1000] - Training Loss: 0.170, MSE Loss: 0.063684, , KL Loss: 0.780060, Test Loss: 0.068708, Learning Rate: 0.0009595476868082425\n",
      "Epoch [137/1000] - Training Loss: 0.196, MSE Loss: 0.063617, , KL Loss: 0.963325, Test Loss: 0.068761, Learning Rate: 0.0009589598570602125\n",
      "Epoch [138/1000] - Training Loss: 0.169, MSE Loss: 0.063570, , KL Loss: 0.761703, Test Loss: 0.069071, Learning Rate: 0.0009583679910434971\n",
      "Epoch [139/1000] - Training Loss: 0.215, MSE Loss: 0.063427, , KL Loss: 1.092252, Test Loss: 0.068456, Learning Rate: 0.000957772094599575\n",
      "Epoch [140/1000] - Training Loss: 0.183, MSE Loss: 0.063534, , KL Loss: 0.853482, Test Loss: 0.068491, Learning Rate: 0.0009571721736097034\n",
      "Epoch [141/1000] - Training Loss: 0.208, MSE Loss: 0.063768, , KL Loss: 1.023738, Test Loss: 0.068015, Learning Rate: 0.0009565682339948604\n",
      "Epoch [142/1000] - Training Loss: 0.158, MSE Loss: 0.063485, , KL Loss: 0.665610, Test Loss: 0.068689, Learning Rate: 0.0009559602817156859\n",
      "Epoch [143/1000] - Training Loss: 0.174, MSE Loss: 0.063913, , KL Loss: 0.769667, Test Loss: 0.068109, Learning Rate: 0.0009553483227724239\n",
      "Epoch [144/1000] - Training Loss: 0.158, MSE Loss: 0.063499, , KL Loss: 0.657942, Test Loss: 0.068131, Learning Rate: 0.0009547323632048617\n",
      "Epoch [145/1000] - Training Loss: 0.184, MSE Loss: 0.063667, , KL Loss: 0.829272, Test Loss: 0.068741, Learning Rate: 0.0009541124090922718\n",
      "Epoch [146/1000] - Training Loss: 0.169, MSE Loss: 0.063386, , KL Loss: 0.724261, Test Loss: 0.069328, Learning Rate: 0.000953488466553351\n",
      "Epoch [147/1000] - Training Loss: 0.213, MSE Loss: 0.063438, , KL Loss: 1.020624, Test Loss: 0.068875, Learning Rate: 0.0009528605417461601\n",
      "Epoch [148/1000] - Training Loss: 0.174, MSE Loss: 0.063419, , KL Loss: 0.750271, Test Loss: 0.068877, Learning Rate: 0.0009522286408680635\n",
      "Epoch [149/1000] - Training Loss: 0.271, MSE Loss: 0.063355, , KL Loss: 1.391041, Test Loss: 0.068337, Learning Rate: 0.0009515927701556679\n",
      "Epoch [150/1000] - Training Loss: 0.165, MSE Loss: 0.063469, , KL Loss: 0.679767, Test Loss: 0.068705, Learning Rate: 0.0009509529358847603\n",
      "Epoch [151/1000] - Training Loss: 0.164, MSE Loss: 0.063721, , KL Loss: 0.663192, Test Loss: 0.068375, Learning Rate: 0.000950309144370247\n",
      "Epoch [152/1000] - Training Loss: 0.172, MSE Loss: 0.063441, , KL Loss: 0.715241, Test Loss: 0.068723, Learning Rate: 0.0009496614019660898\n",
      "Epoch [153/1000] - Training Loss: 0.125, MSE Loss: 0.063689, , KL Loss: 0.400798, Test Loss: 0.068884, Learning Rate: 0.0009490097150652452\n",
      "Epoch [154/1000] - Training Loss: 0.149, MSE Loss: 0.063328, , KL Loss: 0.554863, Test Loss: 0.068224, Learning Rate: 0.0009483540900995996\n",
      "Epoch [155/1000] - Training Loss: 0.144, MSE Loss: 0.063611, , KL Loss: 0.516043, Test Loss: 0.068606, Learning Rate: 0.0009476945335399068\n",
      "Epoch [156/1000] - Training Loss: 0.128, MSE Loss: 0.063479, , KL Loss: 0.410478, Test Loss: 0.067771, Learning Rate: 0.0009470310518957238\n",
      "Epoch [157/1000] - Training Loss: 0.137, MSE Loss: 0.063454, , KL Loss: 0.468652, Test Loss: 0.068215, Learning Rate: 0.0009463636517153465\n",
      "Epoch [158/1000] - Training Loss: 0.248, MSE Loss: 0.063466, , KL Loss: 1.165828, Test Loss: 0.068275, Learning Rate: 0.0009456923395857452\n",
      "Epoch [159/1000] - Training Loss: 0.181, MSE Loss: 0.063401, , KL Loss: 0.736679, Test Loss: 0.068382, Learning Rate: 0.0009450171221324998\n",
      "Epoch [160/1000] - Training Loss: 0.164, MSE Loss: 0.063495, , KL Loss: 0.627118, Test Loss: 0.068548, Learning Rate: 0.0009443380060197336\n",
      "Epoch [161/1000] - Training Loss: 0.151, MSE Loss: 0.063501, , KL Loss: 0.541646, Test Loss: 0.068751, Learning Rate: 0.0009436549979500488\n",
      "Epoch [162/1000] - Training Loss: 0.134, MSE Loss: 0.063706, , KL Loss: 0.436227, Test Loss: 0.068363, Learning Rate: 0.0009429681046644591\n",
      "Epoch [163/1000] - Training Loss: 0.175, MSE Loss: 0.063342, , KL Loss: 0.682262, Test Loss: 0.068950, Learning Rate: 0.0009422773329423241\n",
      "Epoch [164/1000] - Training Loss: 0.158, MSE Loss: 0.063736, , KL Loss: 0.574409, Test Loss: 0.068972, Learning Rate: 0.0009415826896012816\n",
      "Epoch [165/1000] - Training Loss: 0.187, MSE Loss: 0.063488, , KL Loss: 0.749453, Test Loss: 0.068615, Learning Rate: 0.0009408841814971811\n",
      "Epoch [166/1000] - Training Loss: 0.134, MSE Loss: 0.063577, , KL Loss: 0.425773, Test Loss: 0.068667, Learning Rate: 0.0009401818155240155\n",
      "Epoch [167/1000] - Training Loss: 0.211, MSE Loss: 0.063521, , KL Loss: 0.880198, Test Loss: 0.068500, Learning Rate: 0.0009394755986138535\n",
      "Epoch [168/1000] - Training Loss: 0.134, MSE Loss: 0.063485, , KL Loss: 0.416793, Test Loss: 0.068079, Learning Rate: 0.0009387655377367708\n",
      "Epoch [169/1000] - Training Loss: 0.268, MSE Loss: 0.063732, , KL Loss: 1.209551, Test Loss: 0.068531, Learning Rate: 0.0009380516399007819\n",
      "Epoch [170/1000] - Training Loss: 0.240, MSE Loss: 0.063464, , KL Loss: 1.036583, Test Loss: 0.068712, Learning Rate: 0.0009373339121517697\n",
      "Epoch [171/1000] - Training Loss: 0.175, MSE Loss: 0.063607, , KL Loss: 0.649998, Test Loss: 0.068844, Learning Rate: 0.0009366123615734177\n",
      "Epoch [172/1000] - Training Loss: 0.123, MSE Loss: 0.063624, , KL Loss: 0.342692, Test Loss: 0.068574, Learning Rate: 0.0009358869952871385\n",
      "Epoch [173/1000] - Training Loss: 0.193, MSE Loss: 0.063314, , KL Loss: 0.751840, Test Loss: 0.068365, Learning Rate: 0.0009351578204520049\n",
      "Epoch [174/1000] - Training Loss: 0.136, MSE Loss: 0.063407, , KL Loss: 0.419412, Test Loss: 0.068647, Learning Rate: 0.0009344248442646778\n",
      "Epoch [175/1000] - Training Loss: 0.137, MSE Loss: 0.063637, , KL Loss: 0.417471, Test Loss: 0.067866, Learning Rate: 0.0009336880739593365\n",
      "Epoch [176/1000] - Training Loss: 0.121, MSE Loss: 0.063459, , KL Loss: 0.329270, Test Loss: 0.068296, Learning Rate: 0.0009329475168076063\n",
      "Epoch [177/1000] - Training Loss: 0.190, MSE Loss: 0.063432, , KL Loss: 0.712457, Test Loss: 0.068203, Learning Rate: 0.0009322031801184873\n",
      "Epoch [178/1000] - Training Loss: 0.125, MSE Loss: 0.063292, , KL Loss: 0.346769, Test Loss: 0.068333, Learning Rate: 0.0009314550712382824\n",
      "Epoch [179/1000] - Training Loss: 0.147, MSE Loss: 0.063639, , KL Loss: 0.467282, Test Loss: 0.068948, Learning Rate: 0.0009307031975505239\n",
      "Epoch [180/1000] - Training Loss: 0.248, MSE Loss: 0.063431, , KL Loss: 1.027438, Test Loss: 0.068484, Learning Rate: 0.0009299475664759016\n",
      "Epoch [181/1000] - Training Loss: 0.137, MSE Loss: 0.063587, , KL Loss: 0.404729, Test Loss: 0.069060, Learning Rate: 0.0009291881854721894\n",
      "Epoch [182/1000] - Training Loss: 0.212, MSE Loss: 0.063359, , KL Loss: 0.818387, Test Loss: 0.068227, Learning Rate: 0.0009284250620341711\n",
      "Epoch [183/1000] - Training Loss: 0.189, MSE Loss: 0.063472, , KL Loss: 0.686791, Test Loss: 0.068299, Learning Rate: 0.0009276582036935667\n",
      "Epoch [184/1000] - Training Loss: 0.128, MSE Loss: 0.063537, , KL Loss: 0.352867, Test Loss: 0.068093, Learning Rate: 0.0009268876180189586\n",
      "Epoch [185/1000] - Training Loss: 0.258, MSE Loss: 0.063283, , KL Loss: 1.054374, Test Loss: 0.068185, Learning Rate: 0.0009261133126157165\n",
      "Epoch [186/1000] - Training Loss: 0.187, MSE Loss: 0.063306, , KL Loss: 0.666143, Test Loss: 0.068274, Learning Rate: 0.000925335295125922\n",
      "Epoch [187/1000] - Training Loss: 0.174, MSE Loss: 0.063289, , KL Loss: 0.593652, Test Loss: 0.068306, Learning Rate: 0.0009245535732282934\n",
      "Epoch [188/1000] - Training Loss: 0.168, MSE Loss: 0.063494, , KL Loss: 0.558257, Test Loss: 0.068268, Learning Rate: 0.0009237681546381106\n",
      "Epoch [189/1000] - Training Loss: 0.212, MSE Loss: 0.063440, , KL Loss: 0.784352, Test Loss: 0.068377, Learning Rate: 0.0009229790471071377\n",
      "Epoch [190/1000] - Training Loss: 0.116, MSE Loss: 0.063322, , KL Loss: 0.279847, Test Loss: 0.068414, Learning Rate: 0.0009221862584235477\n",
      "Epoch [191/1000] - Training Loss: 0.189, MSE Loss: 0.063300, , KL Loss: 0.655935, Test Loss: 0.068371, Learning Rate: 0.0009213897964118448\n",
      "Epoch [192/1000] - Training Loss: 0.149, MSE Loss: 0.063690, , KL Loss: 0.441822, Test Loss: 0.069212, Learning Rate: 0.0009205896689327873\n",
      "Epoch [193/1000] - Training Loss: 0.165, MSE Loss: 0.063387, , KL Loss: 0.525584, Test Loss: 0.068479, Learning Rate: 0.0009197858838833106\n",
      "Epoch [194/1000] - Training Loss: 0.125, MSE Loss: 0.063294, , KL Loss: 0.320592, Test Loss: 0.068697, Learning Rate: 0.0009189784491964487\n",
      "Epoch [195/1000] - Training Loss: 0.107, MSE Loss: 0.063177, , KL Loss: 0.222574, Test Loss: 0.068453, Learning Rate: 0.0009181673728412556\n",
      "Epoch [196/1000] - Training Loss: 0.118, MSE Loss: 0.063375, , KL Loss: 0.280834, Test Loss: 0.068295, Learning Rate: 0.0009173526628227279\n",
      "Epoch [197/1000] - Training Loss: 0.107, MSE Loss: 0.063348, , KL Loss: 0.223445, Test Loss: 0.068627, Learning Rate: 0.0009165343271817243\n",
      "Epoch [198/1000] - Training Loss: 0.131, MSE Loss: 0.063315, , KL Loss: 0.341663, Test Loss: 0.068847, Learning Rate: 0.0009157123739948873\n",
      "Epoch [199/1000] - Training Loss: 0.101, MSE Loss: 0.063347, , KL Loss: 0.187316, Test Loss: 0.068450, Learning Rate: 0.0009148868113745632\n",
      "Epoch [200/1000] - Training Loss: 0.123, MSE Loss: 0.063490, , KL Loss: 0.296518, Test Loss: 0.068297, Learning Rate: 0.0009140576474687215\n",
      "Epoch [201/1000] - Training Loss: 0.107, MSE Loss: 0.063426, , KL Loss: 0.215518, Test Loss: 0.068468, Learning Rate: 0.0009132248904608752\n",
      "Epoch [202/1000] - Training Loss: 0.105, MSE Loss: 0.063285, , KL Loss: 0.207923, Test Loss: 0.068881, Learning Rate: 0.00091238854857\n",
      "Epoch [203/1000] - Training Loss: 0.133, MSE Loss: 0.063301, , KL Loss: 0.345547, Test Loss: 0.068065, Learning Rate: 0.0009115486300504527\n",
      "Epoch [204/1000] - Training Loss: 0.113, MSE Loss: 0.063311, , KL Loss: 0.241129, Test Loss: 0.067920, Learning Rate: 0.0009107051431918896\n",
      "Epoch [205/1000] - Training Loss: 0.118, MSE Loss: 0.063455, , KL Loss: 0.265108, Test Loss: 0.068264, Learning Rate: 0.000909858096319186\n",
      "Epoch [206/1000] - Training Loss: 0.125, MSE Loss: 0.063260, , KL Loss: 0.301085, Test Loss: 0.067788, Learning Rate: 0.000909007497792352\n",
      "Epoch [207/1000] - Training Loss: 0.132, MSE Loss: 0.063532, , KL Loss: 0.330519, Test Loss: 0.068177, Learning Rate: 0.0009081533560064522\n",
      "Epoch [208/1000] - Training Loss: 0.114, MSE Loss: 0.063392, , KL Loss: 0.240936, Test Loss: 0.067998, Learning Rate: 0.0009072956793915209\n",
      "Epoch [209/1000] - Training Loss: 0.105, MSE Loss: 0.063362, , KL Loss: 0.198137, Test Loss: 0.068128, Learning Rate: 0.0009064344764124803\n",
      "Epoch [210/1000] - Training Loss: 0.121, MSE Loss: 0.063234, , KL Loss: 0.272788, Test Loss: 0.068325, Learning Rate: 0.0009055697555690558\n",
      "Epoch [211/1000] - Training Loss: 0.109, MSE Loss: 0.063294, , KL Loss: 0.216437, Test Loss: 0.068256, Learning Rate: 0.0009047015253956931\n",
      "Epoch [212/1000] - Training Loss: 0.110, MSE Loss: 0.063282, , KL Loss: 0.221601, Test Loss: 0.068346, Learning Rate: 0.0009038297944614736\n",
      "Epoch [213/1000] - Training Loss: 0.113, MSE Loss: 0.063213, , KL Loss: 0.235592, Test Loss: 0.067853, Learning Rate: 0.0009029545713700296\n",
      "Epoch [214/1000] - Training Loss: 0.120, MSE Loss: 0.063301, , KL Loss: 0.263602, Test Loss: 0.068299, Learning Rate: 0.0009020758647594597\n",
      "Epoch [215/1000] - Training Loss: 0.110, MSE Loss: 0.063213, , KL Loss: 0.215415, Test Loss: 0.068696, Learning Rate: 0.0009011936833022435\n",
      "Epoch [216/1000] - Training Loss: 0.125, MSE Loss: 0.063351, , KL Loss: 0.287113, Test Loss: 0.068052, Learning Rate: 0.0009003080357051557\n",
      "Epoch [217/1000] - Training Loss: 0.145, MSE Loss: 0.063323, , KL Loss: 0.376115, Test Loss: 0.068560, Learning Rate: 0.0008994189307091804\n",
      "Epoch [218/1000] - Training Loss: 0.093, MSE Loss: 0.063124, , KL Loss: 0.138446, Test Loss: 0.068538, Learning Rate: 0.0008985263770894254\n",
      "Epoch [219/1000] - Training Loss: 0.105, MSE Loss: 0.063272, , KL Loss: 0.192617, Test Loss: 0.068689, Learning Rate: 0.0008976303836550341\n",
      "Epoch [220/1000] - Training Loss: 0.101, MSE Loss: 0.063165, , KL Loss: 0.171517, Test Loss: 0.068406, Learning Rate: 0.0008967309592491002\n",
      "Epoch [221/1000] - Training Loss: 0.111, MSE Loss: 0.063330, , KL Loss: 0.216212, Test Loss: 0.067941, Learning Rate: 0.0008958281127485796\n",
      "Epoch [222/1000] - Training Loss: 0.095, MSE Loss: 0.063307, , KL Loss: 0.142979, Test Loss: 0.068081, Learning Rate: 0.0008949218530642025\n",
      "Epoch [223/1000] - Training Loss: 0.096, MSE Loss: 0.063074, , KL Loss: 0.145612, Test Loss: 0.068789, Learning Rate: 0.0008940121891403863\n",
      "Epoch [224/1000] - Training Loss: 0.080, MSE Loss: 0.063320, , KL Loss: 0.074182, Test Loss: 0.068655, Learning Rate: 0.0008930991299551466\n",
      "Epoch [225/1000] - Training Loss: 0.085, MSE Loss: 0.063163, , KL Loss: 0.098819, Test Loss: 0.068306, Learning Rate: 0.0008921826845200091\n",
      "Epoch [226/1000] - Training Loss: 0.092, MSE Loss: 0.063245, , KL Loss: 0.125986, Test Loss: 0.068572, Learning Rate: 0.0008912628618799201\n",
      "Epoch [227/1000] - Training Loss: 0.078, MSE Loss: 0.063217, , KL Loss: 0.064442, Test Loss: 0.067608, Learning Rate: 0.0008903396711131576\n",
      "Epoch [228/1000] - Training Loss: 0.082, MSE Loss: 0.063070, , KL Loss: 0.083934, Test Loss: 0.068604, Learning Rate: 0.0008894131213312421\n",
      "Epoch [229/1000] - Training Loss: 0.091, MSE Loss: 0.063130, , KL Loss: 0.121117, Test Loss: 0.068698, Learning Rate: 0.0008884832216788454\n",
      "Epoch [230/1000] - Training Loss: 0.077, MSE Loss: 0.063285, , KL Loss: 0.061449, Test Loss: 0.068846, Learning Rate: 0.0008875499813337022\n",
      "Epoch [231/1000] - Training Loss: 0.082, MSE Loss: 0.063353, , KL Loss: 0.081746, Test Loss: 0.068752, Learning Rate: 0.0008866134095065174\n",
      "Epoch [232/1000] - Training Loss: 0.079, MSE Loss: 0.063321, , KL Loss: 0.066796, Test Loss: 0.068583, Learning Rate: 0.0008856735154408773\n",
      "Epoch [233/1000] - Training Loss: 0.076, MSE Loss: 0.063245, , KL Loss: 0.055414, Test Loss: 0.068283, Learning Rate: 0.0008847303084131566\n",
      "Epoch [234/1000] - Training Loss: 0.075, MSE Loss: 0.063193, , KL Loss: 0.050980, Test Loss: 0.068194, Learning Rate: 0.0008837837977324279\n",
      "Epoch [235/1000] - Training Loss: 0.095, MSE Loss: 0.063242, , KL Loss: 0.136424, Test Loss: 0.068290, Learning Rate: 0.0008828339927403696\n",
      "Epoch [236/1000] - Training Loss: 0.078, MSE Loss: 0.063251, , KL Loss: 0.061862, Test Loss: 0.067551, Learning Rate: 0.0008818809028111735\n",
      "Epoch [237/1000] - Training Loss: 0.110, MSE Loss: 0.063326, , KL Loss: 0.198452, Test Loss: 0.068358, Learning Rate: 0.0008809245373514524\n",
      "Epoch [238/1000] - Training Loss: 0.081, MSE Loss: 0.063373, , KL Loss: 0.072481, Test Loss: 0.068122, Learning Rate: 0.0008799649058001472\n",
      "Epoch [239/1000] - Training Loss: 0.119, MSE Loss: 0.063182, , KL Loss: 0.235041, Test Loss: 0.068121, Learning Rate: 0.0008790020176284342\n",
      "Epoch [240/1000] - Training Loss: 0.090, MSE Loss: 0.063261, , KL Loss: 0.112306, Test Loss: 0.068760, Learning Rate: 0.0008780358823396305\n",
      "Epoch [241/1000] - Training Loss: 0.092, MSE Loss: 0.063318, , KL Loss: 0.120473, Test Loss: 0.068568, Learning Rate: 0.0008770665094691016\n",
      "Epoch [242/1000] - Training Loss: 0.098, MSE Loss: 0.063311, , KL Loss: 0.142420, Test Loss: 0.068651, Learning Rate: 0.0008760939085841663\n",
      "Epoch [243/1000] - Training Loss: 0.154, MSE Loss: 0.063223, , KL Loss: 0.372730, Test Loss: 0.068643, Learning Rate: 0.0008751180892840028\n",
      "Epoch [244/1000] - Training Loss: 0.182, MSE Loss: 0.063213, , KL Loss: 0.485447, Test Loss: 0.067970, Learning Rate: 0.0008741390611995535\n",
      "Epoch [245/1000] - Training Loss: 0.258, MSE Loss: 0.063261, , KL Loss: 0.794714, Test Loss: 0.068657, Learning Rate: 0.0008731568339934303\n",
      "Epoch [246/1000] - Training Loss: 0.238, MSE Loss: 0.063341, , KL Loss: 0.708948, Test Loss: 0.068357, Learning Rate: 0.0008721714173598194\n",
      "Epoch [247/1000] - Training Loss: 0.284, MSE Loss: 0.063329, , KL Loss: 0.894677, Test Loss: 0.068507, Learning Rate: 0.000871182821024385\n",
      "Epoch [248/1000] - Training Loss: 0.177, MSE Loss: 0.063224, , KL Loss: 0.459255, Test Loss: 0.068258, Learning Rate: 0.0008701910547441738\n",
      "Epoch [249/1000] - Training Loss: 0.209, MSE Loss: 0.063280, , KL Loss: 0.585360, Test Loss: 0.067637, Learning Rate: 0.0008691961283075187\n",
      "Epoch [250/1000] - Training Loss: 0.144, MSE Loss: 0.063363, , KL Loss: 0.322614, Test Loss: 0.068295, Learning Rate: 0.0008681980515339418\n",
      "Epoch [251/1000] - Training Loss: 0.165, MSE Loss: 0.063255, , KL Loss: 0.405518, Test Loss: 0.067897, Learning Rate: 0.000867196834274058\n",
      "Epoch [252/1000] - Training Loss: 0.157, MSE Loss: 0.063153, , KL Loss: 0.374001, Test Loss: 0.068132, Learning Rate: 0.0008661924864094775\n",
      "Epoch [253/1000] - Training Loss: 0.141, MSE Loss: 0.063294, , KL Loss: 0.308526, Test Loss: 0.068965, Learning Rate: 0.0008651850178527081\n",
      "Epoch [254/1000] - Training Loss: 0.150, MSE Loss: 0.063111, , KL Loss: 0.340949, Test Loss: 0.068149, Learning Rate: 0.000864174438547058\n",
      "Epoch [255/1000] - Training Loss: 0.265, MSE Loss: 0.063151, , KL Loss: 0.791098, Test Loss: 0.068363, Learning Rate: 0.0008631607584665367\n",
      "Epoch [256/1000] - Training Loss: 0.189, MSE Loss: 0.063224, , KL Loss: 0.491695, Test Loss: 0.068399, Learning Rate: 0.0008621439876157576\n",
      "Epoch [257/1000] - Training Loss: 0.185, MSE Loss: 0.063211, , KL Loss: 0.473187, Test Loss: 0.068468, Learning Rate: 0.0008611241360298382\n",
      "Epoch [258/1000] - Training Loss: 0.236, MSE Loss: 0.063181, , KL Loss: 0.668623, Test Loss: 0.068160, Learning Rate: 0.0008601012137743022\n",
      "Epoch [259/1000] - Training Loss: 0.139, MSE Loss: 0.063267, , KL Loss: 0.292872, Test Loss: 0.068265, Learning Rate: 0.0008590752309449792\n",
      "Epoch [260/1000] - Training Loss: 0.175, MSE Loss: 0.063280, , KL Loss: 0.429611, Test Loss: 0.068122, Learning Rate: 0.0008580461976679055\n",
      "Epoch [261/1000] - Training Loss: 0.257, MSE Loss: 0.063090, , KL Loss: 0.742896, Test Loss: 0.067826, Learning Rate: 0.000857014124099224\n",
      "Epoch [262/1000] - Training Loss: 0.132, MSE Loss: 0.063145, , KL Loss: 0.261676, Test Loss: 0.068399, Learning Rate: 0.0008559790204250842\n",
      "Epoch [263/1000] - Training Loss: 0.213, MSE Loss: 0.063289, , KL Loss: 0.569062, Test Loss: 0.068168, Learning Rate: 0.0008549408968615416\n",
      "Epoch [264/1000] - Training Loss: 0.154, MSE Loss: 0.063221, , KL Loss: 0.344798, Test Loss: 0.068270, Learning Rate: 0.0008538997636544566\n",
      "Epoch [265/1000] - Training Loss: 0.134, MSE Loss: 0.063370, , KL Loss: 0.265678, Test Loss: 0.068920, Learning Rate: 0.0008528556310793936\n",
      "Epoch [266/1000] - Training Loss: 0.212, MSE Loss: 0.063176, , KL Loss: 0.560787, Test Loss: 0.068503, Learning Rate: 0.0008518085094415197\n",
      "Epoch [267/1000] - Training Loss: 0.157, MSE Loss: 0.063100, , KL Loss: 0.351061, Test Loss: 0.068143, Learning Rate: 0.0008507584090755026\n",
      "Epoch [268/1000] - Training Loss: 0.126, MSE Loss: 0.063278, , KL Loss: 0.234137, Test Loss: 0.068164, Learning Rate: 0.000849705340345409\n",
      "Epoch [269/1000] - Training Loss: 0.262, MSE Loss: 0.063260, , KL Loss: 0.737769, Test Loss: 0.068004, Learning Rate: 0.0008486493136446022\n",
      "Epoch [270/1000] - Training Loss: 0.159, MSE Loss: 0.062964, , KL Loss: 0.357124, Test Loss: 0.068063, Learning Rate: 0.0008475903393956392\n",
      "Epoch [271/1000] - Training Loss: 0.107, MSE Loss: 0.063218, , KL Loss: 0.161047, Test Loss: 0.068184, Learning Rate: 0.0008465284280501685\n",
      "Epoch [272/1000] - Training Loss: 0.108, MSE Loss: 0.063142, , KL Loss: 0.165555, Test Loss: 0.067998, Learning Rate: 0.0008454635900888262\n",
      "Epoch [273/1000] - Training Loss: 0.146, MSE Loss: 0.063178, , KL Loss: 0.304973, Test Loss: 0.068102, Learning Rate: 0.0008443958360211332\n",
      "Epoch [274/1000] - Training Loss: 0.130, MSE Loss: 0.063196, , KL Loss: 0.242434, Test Loss: 0.068212, Learning Rate: 0.0008433251763853912\n",
      "Epoch [275/1000] - Training Loss: 0.259, MSE Loss: 0.063095, , KL Loss: 0.710567, Test Loss: 0.068273, Learning Rate: 0.0008422516217485785\n",
      "Epoch [276/1000] - Training Loss: 0.153, MSE Loss: 0.063051, , KL Loss: 0.324205, Test Loss: 0.067404, Learning Rate: 0.0008411751827062459\n",
      "Epoch [277/1000] - Training Loss: 0.171, MSE Loss: 0.063127, , KL Loss: 0.388976, Test Loss: 0.068184, Learning Rate: 0.000840095869882412\n",
      "Epoch [278/1000] - Training Loss: 0.140, MSE Loss: 0.063159, , KL Loss: 0.274865, Test Loss: 0.067989, Learning Rate: 0.0008390136939294589\n",
      "Epoch [279/1000] - Training Loss: 0.101, MSE Loss: 0.063000, , KL Loss: 0.137731, Test Loss: 0.067959, Learning Rate: 0.0008379286655280262\n",
      "Epoch [280/1000] - Training Loss: 0.148, MSE Loss: 0.063093, , KL Loss: 0.302604, Test Loss: 0.068210, Learning Rate: 0.0008368407953869062\n",
      "Epoch [281/1000] - Training Loss: 0.166, MSE Loss: 0.063062, , KL Loss: 0.365272, Test Loss: 0.068235, Learning Rate: 0.0008357500942429382\n",
      "Epoch [282/1000] - Training Loss: 0.133, MSE Loss: 0.063276, , KL Loss: 0.247339, Test Loss: 0.068516, Learning Rate: 0.000834656572860902\n",
      "Epoch [283/1000] - Training Loss: 0.128, MSE Loss: 0.063137, , KL Loss: 0.227736, Test Loss: 0.068222, Learning Rate: 0.000833560242033412\n",
      "Epoch [284/1000] - Training Loss: 0.132, MSE Loss: 0.063039, , KL Loss: 0.242860, Test Loss: 0.068062, Learning Rate: 0.0008324611125808112\n",
      "Epoch [285/1000] - Training Loss: 0.112, MSE Loss: 0.063119, , KL Loss: 0.172641, Test Loss: 0.068519, Learning Rate: 0.0008313591953510633\n",
      "Epoch [286/1000] - Training Loss: 0.114, MSE Loss: 0.063231, , KL Loss: 0.177636, Test Loss: 0.067934, Learning Rate: 0.0008302545012196466\n",
      "Epoch [287/1000] - Training Loss: 0.125, MSE Loss: 0.063047, , KL Loss: 0.215660, Test Loss: 0.068470, Learning Rate: 0.0008291470410894462\n",
      "Epoch [288/1000] - Training Loss: 0.137, MSE Loss: 0.063143, , KL Loss: 0.257109, Test Loss: 0.068247, Learning Rate: 0.0008280368258906464\n",
      "Epoch [289/1000] - Training Loss: 0.100, MSE Loss: 0.063093, , KL Loss: 0.128454, Test Loss: 0.068027, Learning Rate: 0.0008269238665806231\n",
      "Epoch [290/1000] - Training Loss: 0.182, MSE Loss: 0.063089, , KL Loss: 0.410212, Test Loss: 0.068063, Learning Rate: 0.0008258081741438355\n",
      "Epoch [291/1000] - Training Loss: 0.164, MSE Loss: 0.063033, , KL Loss: 0.346500, Test Loss: 0.069223, Learning Rate: 0.0008246897595917171\n",
      "Epoch [292/1000] - Training Loss: 0.114, MSE Loss: 0.063243, , KL Loss: 0.172633, Test Loss: 0.068151, Learning Rate: 0.0008235686339625685\n",
      "Epoch [293/1000] - Training Loss: 0.173, MSE Loss: 0.063152, , KL Loss: 0.376570, Test Loss: 0.068308, Learning Rate: 0.0008224448083214466\n",
      "Epoch [294/1000] - Training Loss: 0.131, MSE Loss: 0.063175, , KL Loss: 0.231262, Test Loss: 0.067974, Learning Rate: 0.0008213182937600572\n",
      "Epoch [295/1000] - Training Loss: 0.128, MSE Loss: 0.063150, , KL Loss: 0.218762, Test Loss: 0.067709, Learning Rate: 0.0008201891013966438\n",
      "Epoch [296/1000] - Training Loss: 0.107, MSE Loss: 0.063081, , KL Loss: 0.148147, Test Loss: 0.067873, Learning Rate: 0.0008190572423758796\n",
      "Epoch [297/1000] - Training Loss: 0.145, MSE Loss: 0.063216, , KL Loss: 0.276565, Test Loss: 0.068430, Learning Rate: 0.0008179227278687557\n",
      "Epoch [298/1000] - Training Loss: 0.114, MSE Loss: 0.063201, , KL Loss: 0.169610, Test Loss: 0.068476, Learning Rate: 0.0008167855690724728\n",
      "Epoch [299/1000] - Training Loss: 0.120, MSE Loss: 0.063143, , KL Loss: 0.188566, Test Loss: 0.068958, Learning Rate: 0.0008156457772103287\n",
      "Epoch [300/1000] - Training Loss: 0.106, MSE Loss: 0.063225, , KL Loss: 0.143575, Test Loss: 0.068421, Learning Rate: 0.0008145033635316091\n",
      "Epoch [301/1000] - Training Loss: 0.163, MSE Loss: 0.063052, , KL Loss: 0.331105, Test Loss: 0.068024, Learning Rate: 0.0008133583393114756\n",
      "Epoch [302/1000] - Training Loss: 0.118, MSE Loss: 0.063093, , KL Loss: 0.181644, Test Loss: 0.067780, Learning Rate: 0.0008122107158508553\n",
      "Epoch [303/1000] - Training Loss: 0.123, MSE Loss: 0.063007, , KL Loss: 0.199209, Test Loss: 0.067611, Learning Rate: 0.0008110605044763284\n",
      "Epoch [304/1000] - Training Loss: 0.120, MSE Loss: 0.063166, , KL Loss: 0.186592, Test Loss: 0.068147, Learning Rate: 0.0008099077165400165\n",
      "Epoch [305/1000] - Training Loss: 0.121, MSE Loss: 0.063084, , KL Loss: 0.189382, Test Loss: 0.068492, Learning Rate: 0.0008087523634194715\n",
      "Epoch [306/1000] - Training Loss: 0.090, MSE Loss: 0.063134, , KL Loss: 0.087341, Test Loss: 0.068267, Learning Rate: 0.000807594456517562\n",
      "Epoch [307/1000] - Training Loss: 0.101, MSE Loss: 0.063126, , KL Loss: 0.122242, Test Loss: 0.068186, Learning Rate: 0.0008064340072623618\n",
      "Epoch [308/1000] - Training Loss: 0.086, MSE Loss: 0.063074, , KL Loss: 0.073814, Test Loss: 0.068230, Learning Rate: 0.0008052710271070365\n",
      "Epoch [309/1000] - Training Loss: 0.100, MSE Loss: 0.063159, , KL Loss: 0.119263, Test Loss: 0.067974, Learning Rate: 0.0008041055275297308\n",
      "Epoch [310/1000] - Training Loss: 0.085, MSE Loss: 0.063077, , KL Loss: 0.071876, Test Loss: 0.067937, Learning Rate: 0.0008029375200334549\n",
      "Epoch [311/1000] - Training Loss: 0.087, MSE Loss: 0.063103, , KL Loss: 0.078112, Test Loss: 0.067811, Learning Rate: 0.0008017670161459714\n",
      "Epoch [312/1000] - Training Loss: 0.083, MSE Loss: 0.062906, , KL Loss: 0.065960, Test Loss: 0.067933, Learning Rate: 0.0008005940274196807\n",
      "Epoch [313/1000] - Training Loss: 0.098, MSE Loss: 0.062939, , KL Loss: 0.110826, Test Loss: 0.068529, Learning Rate: 0.0007994185654315085\n",
      "Epoch [314/1000] - Training Loss: 0.136, MSE Loss: 0.063267, , KL Loss: 0.231158, Test Loss: 0.068323, Learning Rate: 0.0007982406417827899\n",
      "Epoch [315/1000] - Training Loss: 0.089, MSE Loss: 0.063042, , KL Loss: 0.083057, Test Loss: 0.068537, Learning Rate: 0.0007970602680991554\n",
      "Epoch [316/1000] - Training Loss: 0.090, MSE Loss: 0.063059, , KL Loss: 0.086715, Test Loss: 0.068551, Learning Rate: 0.0007958774560304173\n",
      "Epoch [317/1000] - Training Loss: 0.102, MSE Loss: 0.063144, , KL Loss: 0.121554, Test Loss: 0.068350, Learning Rate: 0.0007946922172504528\n",
      "Epoch [318/1000] - Training Loss: 0.116, MSE Loss: 0.063049, , KL Loss: 0.167762, Test Loss: 0.067956, Learning Rate: 0.0007935045634570902\n",
      "Epoch [319/1000] - Training Loss: 0.134, MSE Loss: 0.063046, , KL Loss: 0.221540, Test Loss: 0.067838, Learning Rate: 0.0007923145063719932\n",
      "Epoch [320/1000] - Training Loss: 0.111, MSE Loss: 0.062950, , KL Loss: 0.148614, Test Loss: 0.068141, Learning Rate: 0.0007911220577405446\n",
      "Epoch [321/1000] - Training Loss: 0.123, MSE Loss: 0.063184, , KL Loss: 0.187808, Test Loss: 0.068548, Learning Rate: 0.0007899272293317311\n",
      "Epoch [322/1000] - Training Loss: 0.103, MSE Loss: 0.063084, , KL Loss: 0.124107, Test Loss: 0.067960, Learning Rate: 0.0007887300329380264\n",
      "Epoch [323/1000] - Training Loss: 0.096, MSE Loss: 0.063005, , KL Loss: 0.101880, Test Loss: 0.068194, Learning Rate: 0.000787530480375276\n",
      "Epoch [324/1000] - Training Loss: 0.097, MSE Loss: 0.063067, , KL Loss: 0.105105, Test Loss: 0.068715, Learning Rate: 0.0007863285834825792\n",
      "Epoch [325/1000] - Training Loss: 0.113, MSE Loss: 0.063031, , KL Loss: 0.155258, Test Loss: 0.068642, Learning Rate: 0.0007851243541221731\n",
      "Epoch [326/1000] - Training Loss: 0.084, MSE Loss: 0.062954, , KL Loss: 0.065850, Test Loss: 0.068147, Learning Rate: 0.0007839178041793154\n",
      "Epoch [327/1000] - Training Loss: 0.090, MSE Loss: 0.063141, , KL Loss: 0.082466, Test Loss: 0.067696, Learning Rate: 0.0007827089455621668\n",
      "Epoch [328/1000] - Training Loss: 0.098, MSE Loss: 0.063264, , KL Loss: 0.106176, Test Loss: 0.067642, Learning Rate: 0.0007814977902016741\n",
      "Epoch [329/1000] - Training Loss: 0.102, MSE Loss: 0.063221, , KL Loss: 0.118905, Test Loss: 0.067738, Learning Rate: 0.0007802843500514514\n",
      "Epoch [330/1000] - Training Loss: 0.091, MSE Loss: 0.063098, , KL Loss: 0.085401, Test Loss: 0.068171, Learning Rate: 0.0007790686370876632\n",
      "Epoch [331/1000] - Training Loss: 0.100, MSE Loss: 0.063128, , KL Loss: 0.110845, Test Loss: 0.068316, Learning Rate: 0.0007778506633089057\n",
      "Epoch [332/1000] - Training Loss: 0.104, MSE Loss: 0.063022, , KL Loss: 0.124870, Test Loss: 0.069062, Learning Rate: 0.0007766304407360883\n",
      "Epoch [333/1000] - Training Loss: 0.081, MSE Loss: 0.063138, , KL Loss: 0.053893, Test Loss: 0.069115, Learning Rate: 0.0007754079814123155\n",
      "Epoch [334/1000] - Training Loss: 0.122, MSE Loss: 0.063058, , KL Loss: 0.176062, Test Loss: 0.068443, Learning Rate: 0.0007741832974027669\n",
      "Epoch [335/1000] - Training Loss: 0.089, MSE Loss: 0.062962, , KL Loss: 0.077342, Test Loss: 0.068176, Learning Rate: 0.0007729564007945795\n",
      "Epoch [336/1000] - Training Loss: 0.088, MSE Loss: 0.063208, , KL Loss: 0.074295, Test Loss: 0.068154, Learning Rate: 0.0007717273036967273\n",
      "Epoch [337/1000] - Training Loss: 0.104, MSE Loss: 0.063017, , KL Loss: 0.120594, Test Loss: 0.067514, Learning Rate: 0.0007704960182399026\n",
      "Epoch [338/1000] - Training Loss: 0.092, MSE Loss: 0.062961, , KL Loss: 0.086210, Test Loss: 0.067812, Learning Rate: 0.0007692625565763956\n",
      "Epoch [339/1000] - Training Loss: 0.095, MSE Loss: 0.063053, , KL Loss: 0.093877, Test Loss: 0.068292, Learning Rate: 0.0007680269308799752\n",
      "Epoch [340/1000] - Training Loss: 0.093, MSE Loss: 0.063117, , KL Loss: 0.088115, Test Loss: 0.068225, Learning Rate: 0.000766789153345768\n",
      "Epoch [341/1000] - Training Loss: 0.092, MSE Loss: 0.063129, , KL Loss: 0.084879, Test Loss: 0.068826, Learning Rate: 0.0007655492361901385\n",
      "Epoch [342/1000] - Training Loss: 0.083, MSE Loss: 0.063060, , KL Loss: 0.057934, Test Loss: 0.068331, Learning Rate: 0.0007643071916505687\n",
      "Epoch [343/1000] - Training Loss: 0.087, MSE Loss: 0.062983, , KL Loss: 0.069707, Test Loss: 0.068327, Learning Rate: 0.0007630630319855365\n",
      "Epoch [344/1000] - Training Loss: 0.080, MSE Loss: 0.063057, , KL Loss: 0.049395, Test Loss: 0.067771, Learning Rate: 0.0007618167694743958\n",
      "Epoch [345/1000] - Training Loss: 0.086, MSE Loss: 0.063046, , KL Loss: 0.067742, Test Loss: 0.067884, Learning Rate: 0.0007605684164172542\n",
      "Epoch [346/1000] - Training Loss: 0.093, MSE Loss: 0.062947, , KL Loss: 0.086575, Test Loss: 0.068227, Learning Rate: 0.0007593179851348524\n",
      "Epoch [347/1000] - Training Loss: 0.086, MSE Loss: 0.063104, , KL Loss: 0.065865, Test Loss: 0.068558, Learning Rate: 0.0007580654879684424\n",
      "Epoch [348/1000] - Training Loss: 0.117, MSE Loss: 0.063085, , KL Loss: 0.155057, Test Loss: 0.068063, Learning Rate: 0.0007568109372796658\n",
      "Epoch [349/1000] - Training Loss: 0.094, MSE Loss: 0.063064, , KL Loss: 0.088125, Test Loss: 0.067904, Learning Rate: 0.0007555543454504309\n",
      "Epoch [350/1000] - Training Loss: 0.093, MSE Loss: 0.063142, , KL Loss: 0.084204, Test Loss: 0.068148, Learning Rate: 0.0007542957248827922\n",
      "Epoch [351/1000] - Training Loss: 0.094, MSE Loss: 0.063229, , KL Loss: 0.086740, Test Loss: 0.067873, Learning Rate: 0.0007530350879988265\n",
      "Epoch [352/1000] - Training Loss: 0.095, MSE Loss: 0.062959, , KL Loss: 0.092445, Test Loss: 0.068071, Learning Rate: 0.0007517724472405108\n",
      "Epoch [353/1000] - Training Loss: 0.087, MSE Loss: 0.063006, , KL Loss: 0.068794, Test Loss: 0.068192, Learning Rate: 0.0007505078150695995\n",
      "Epoch [354/1000] - Training Loss: 0.095, MSE Loss: 0.063086, , KL Loss: 0.089803, Test Loss: 0.068049, Learning Rate: 0.0007492412039675018\n",
      "Epoch [355/1000] - Training Loss: 0.111, MSE Loss: 0.063067, , KL Loss: 0.136171, Test Loss: 0.067660, Learning Rate: 0.000747972626435158\n",
      "Epoch [356/1000] - Training Loss: 0.114, MSE Loss: 0.062972, , KL Loss: 0.142175, Test Loss: 0.067900, Learning Rate: 0.000746702094992916\n",
      "Epoch [357/1000] - Training Loss: 0.090, MSE Loss: 0.063127, , KL Loss: 0.074604, Test Loss: 0.067825, Learning Rate: 0.0007454296221804084\n",
      "Epoch [358/1000] - Training Loss: 0.090, MSE Loss: 0.063009, , KL Loss: 0.074791, Test Loss: 0.068292, Learning Rate: 0.0007441552205564279\n",
      "Epoch [359/1000] - Training Loss: 0.081, MSE Loss: 0.063155, , KL Loss: 0.050515, Test Loss: 0.068191, Learning Rate: 0.000742878902698804\n",
      "Epoch [360/1000] - Training Loss: 0.072, MSE Loss: 0.062946, , KL Loss: 0.026104, Test Loss: 0.068555, Learning Rate: 0.000741600681204279\n",
      "Epoch [361/1000] - Training Loss: 0.094, MSE Loss: 0.062976, , KL Loss: 0.084798, Test Loss: 0.068206, Learning Rate: 0.0007403205686883829\n",
      "Epoch [362/1000] - Training Loss: 0.076, MSE Loss: 0.063077, , KL Loss: 0.034549, Test Loss: 0.068278, Learning Rate: 0.0007390385777853091\n",
      "Epoch [363/1000] - Training Loss: 0.077, MSE Loss: 0.062856, , KL Loss: 0.038050, Test Loss: 0.067584, Learning Rate: 0.0007377547211477907\n",
      "Epoch [364/1000] - Training Loss: 0.074, MSE Loss: 0.063272, , KL Loss: 0.030532, Test Loss: 0.067620, Learning Rate: 0.0007364690114469742\n",
      "Epoch [365/1000] - Training Loss: 0.082, MSE Loss: 0.063126, , KL Loss: 0.052442, Test Loss: 0.067871, Learning Rate: 0.0007351814613722952\n",
      "Epoch [366/1000] - Training Loss: 0.073, MSE Loss: 0.063240, , KL Loss: 0.027424, Test Loss: 0.068571, Learning Rate: 0.0007338920836313535\n",
      "Epoch [367/1000] - Training Loss: 0.075, MSE Loss: 0.063139, , KL Loss: 0.030970, Test Loss: 0.068145, Learning Rate: 0.0007326008909497863\n",
      "Epoch [368/1000] - Training Loss: 0.102, MSE Loss: 0.063131, , KL Loss: 0.105858, Test Loss: 0.068291, Learning Rate: 0.0007313078960711445\n",
      "Epoch [369/1000] - Training Loss: 0.076, MSE Loss: 0.063029, , KL Loss: 0.034135, Test Loss: 0.068127, Learning Rate: 0.0007300131117567654\n",
      "Epoch [370/1000] - Training Loss: 0.097, MSE Loss: 0.062905, , KL Loss: 0.091116, Test Loss: 0.067828, Learning Rate: 0.0007287165507856476\n",
      "Epoch [371/1000] - Training Loss: 0.086, MSE Loss: 0.062983, , KL Loss: 0.062736, Test Loss: 0.067689, Learning Rate: 0.0007274182259543241\n",
      "Epoch [372/1000] - Training Loss: 0.079, MSE Loss: 0.063155, , KL Loss: 0.043432, Test Loss: 0.068153, Learning Rate: 0.0007261181500767374\n",
      "Epoch [373/1000] - Training Loss: 0.082, MSE Loss: 0.062855, , KL Loss: 0.051005, Test Loss: 0.068324, Learning Rate: 0.0007248163359841111\n",
      "Epoch [374/1000] - Training Loss: 0.087, MSE Loss: 0.063127, , KL Loss: 0.064421, Test Loss: 0.068534, Learning Rate: 0.0007235127965248247\n",
      "Epoch [375/1000] - Training Loss: 0.083, MSE Loss: 0.063102, , KL Loss: 0.052386, Test Loss: 0.068264, Learning Rate: 0.0007222075445642867\n",
      "Epoch [376/1000] - Training Loss: 0.097, MSE Loss: 0.063020, , KL Loss: 0.089194, Test Loss: 0.067889, Learning Rate: 0.0007209005929848069\n",
      "Epoch [377/1000] - Training Loss: 0.079, MSE Loss: 0.063098, , KL Loss: 0.041254, Test Loss: 0.067936, Learning Rate: 0.0007195919546854696\n",
      "Epoch [378/1000] - Training Loss: 0.083, MSE Loss: 0.063045, , KL Loss: 0.053850, Test Loss: 0.068200, Learning Rate: 0.0007182816425820065\n",
      "Epoch [379/1000] - Training Loss: 0.073, MSE Loss: 0.063040, , KL Loss: 0.026566, Test Loss: 0.067992, Learning Rate: 0.0007169696696066694\n",
      "Epoch [380/1000] - Training Loss: 0.079, MSE Loss: 0.063020, , KL Loss: 0.041686, Test Loss: 0.068364, Learning Rate: 0.0007156560487081014\n",
      "Epoch [381/1000] - Training Loss: 0.075, MSE Loss: 0.062809, , KL Loss: 0.031604, Test Loss: 0.067939, Learning Rate: 0.000714340792851211\n",
      "Epoch [382/1000] - Training Loss: 0.072, MSE Loss: 0.063156, , KL Loss: 0.023247, Test Loss: 0.067920, Learning Rate: 0.000713023915017042\n",
      "Epoch [383/1000] - Training Loss: 0.076, MSE Loss: 0.063018, , KL Loss: 0.034885, Test Loss: 0.068221, Learning Rate: 0.0007117054282026473\n",
      "Epoch [384/1000] - Training Loss: 0.084, MSE Loss: 0.062978, , KL Loss: 0.053659, Test Loss: 0.068588, Learning Rate: 0.0007103853454209592\n",
      "Epoch [385/1000] - Training Loss: 0.087, MSE Loss: 0.063147, , KL Loss: 0.061508, Test Loss: 0.068049, Learning Rate: 0.0007090636797006622\n",
      "Epoch [386/1000] - Training Loss: 0.078, MSE Loss: 0.062901, , KL Loss: 0.038502, Test Loss: 0.068299, Learning Rate: 0.0007077404440860631\n",
      "Epoch [387/1000] - Training Loss: 0.084, MSE Loss: 0.062980, , KL Loss: 0.053539, Test Loss: 0.068274, Learning Rate: 0.0007064156516369631\n",
      "Epoch [388/1000] - Training Loss: 0.070, MSE Loss: 0.062896, , KL Loss: 0.018143, Test Loss: 0.067744, Learning Rate: 0.0007050893154285292\n",
      "Epoch [389/1000] - Training Loss: 0.080, MSE Loss: 0.062852, , KL Loss: 0.044946, Test Loss: 0.067934, Learning Rate: 0.000703761448551164\n",
      "Epoch [390/1000] - Training Loss: 0.069, MSE Loss: 0.062746, , KL Loss: 0.017241, Test Loss: 0.068080, Learning Rate: 0.0007024320641103777\n",
      "Epoch [391/1000] - Training Loss: 0.082, MSE Loss: 0.063120, , KL Loss: 0.048282, Test Loss: 0.068344, Learning Rate: 0.0007011011752266578\n",
      "Epoch [392/1000] - Training Loss: 0.076, MSE Loss: 0.063111, , KL Loss: 0.032089, Test Loss: 0.068463, Learning Rate: 0.0006997687950353405\n",
      "Epoch [393/1000] - Training Loss: 0.076, MSE Loss: 0.062913, , KL Loss: 0.034138, Test Loss: 0.068310, Learning Rate: 0.0006984349366864803\n",
      "Epoch [394/1000] - Training Loss: 0.081, MSE Loss: 0.063040, , KL Loss: 0.046039, Test Loss: 0.067952, Learning Rate: 0.0006970996133447208\n",
      "Epoch [395/1000] - Training Loss: 0.072, MSE Loss: 0.063127, , KL Loss: 0.021330, Test Loss: 0.068229, Learning Rate: 0.0006957628381891637\n",
      "Epoch [396/1000] - Training Loss: 0.079, MSE Loss: 0.063067, , KL Loss: 0.040519, Test Loss: 0.068228, Learning Rate: 0.0006944246244132409\n",
      "Epoch [397/1000] - Training Loss: 0.075, MSE Loss: 0.062788, , KL Loss: 0.031262, Test Loss: 0.067849, Learning Rate: 0.0006930849852245817\n",
      "Epoch [398/1000] - Training Loss: 0.087, MSE Loss: 0.062949, , KL Loss: 0.059722, Test Loss: 0.067804, Learning Rate: 0.0006917439338448838\n",
      "Epoch [399/1000] - Training Loss: 0.082, MSE Loss: 0.062958, , KL Loss: 0.047464, Test Loss: 0.067737, Learning Rate: 0.0006904014835097834\n",
      "Epoch [400/1000] - Training Loss: 0.090, MSE Loss: 0.062918, , KL Loss: 0.068427, Test Loss: 0.068042, Learning Rate: 0.000689057647468723\n",
      "Epoch [401/1000] - Training Loss: 0.071, MSE Loss: 0.063005, , KL Loss: 0.020180, Test Loss: 0.068379, Learning Rate: 0.0006877124389848221\n",
      "Epoch [402/1000] - Training Loss: 0.114, MSE Loss: 0.062940, , KL Loss: 0.126491, Test Loss: 0.068754, Learning Rate: 0.0006863658713347449\n",
      "Epoch [403/1000] - Training Loss: 0.080, MSE Loss: 0.062970, , KL Loss: 0.043189, Test Loss: 0.068371, Learning Rate: 0.0006850179578085711\n",
      "Epoch [404/1000] - Training Loss: 0.107, MSE Loss: 0.063023, , KL Loss: 0.108694, Test Loss: 0.067813, Learning Rate: 0.0006836687117096624\n",
      "Epoch [405/1000] - Training Loss: 0.086, MSE Loss: 0.063062, , KL Loss: 0.056450, Test Loss: 0.067550, Learning Rate: 0.0006823181463545334\n",
      "Epoch [406/1000] - Training Loss: 0.093, MSE Loss: 0.063005, , KL Loss: 0.073474, Test Loss: 0.068162, Learning Rate: 0.000680966275072719\n",
      "Epoch [407/1000] - Training Loss: 0.098, MSE Loss: 0.063052, , KL Loss: 0.085555, Test Loss: 0.068504, Learning Rate: 0.0006796131112066428\n",
      "Epoch [408/1000] - Training Loss: 0.116, MSE Loss: 0.062975, , KL Loss: 0.129781, Test Loss: 0.067977, Learning Rate: 0.000678258668111486\n",
      "Epoch [409/1000] - Training Loss: 0.084, MSE Loss: 0.062937, , KL Loss: 0.050984, Test Loss: 0.068482, Learning Rate: 0.000676902959155055\n",
      "Epoch [410/1000] - Training Loss: 0.095, MSE Loss: 0.063101, , KL Loss: 0.078114, Test Loss: 0.067678, Learning Rate: 0.0006755459977176501\n",
      "Epoch [411/1000] - Training Loss: 0.086, MSE Loss: 0.062832, , KL Loss: 0.057415, Test Loss: 0.068392, Learning Rate: 0.0006741877971919325\n",
      "Epoch [412/1000] - Training Loss: 0.106, MSE Loss: 0.063116, , KL Loss: 0.103719, Test Loss: 0.067831, Learning Rate: 0.0006728283709827933\n",
      "Epoch [413/1000] - Training Loss: 0.120, MSE Loss: 0.063006, , KL Loss: 0.138527, Test Loss: 0.067789, Learning Rate: 0.0006714677325072202\n",
      "Epoch [414/1000] - Training Loss: 0.085, MSE Loss: 0.062788, , KL Loss: 0.052520, Test Loss: 0.067820, Learning Rate: 0.0006701058951941659\n",
      "Epoch [415/1000] - Training Loss: 0.112, MSE Loss: 0.062979, , KL Loss: 0.116983, Test Loss: 0.067796, Learning Rate: 0.0006687428724844147\n",
      "Epoch [416/1000] - Training Loss: 0.106, MSE Loss: 0.063049, , KL Loss: 0.103814, Test Loss: 0.068495, Learning Rate: 0.0006673786778304505\n",
      "Epoch [417/1000] - Training Loss: 0.124, MSE Loss: 0.062973, , KL Loss: 0.146511, Test Loss: 0.068181, Learning Rate: 0.0006660133246963239\n",
      "Epoch [418/1000] - Training Loss: 0.101, MSE Loss: 0.063028, , KL Loss: 0.090313, Test Loss: 0.068511, Learning Rate: 0.0006646468265575189\n",
      "Epoch [419/1000] - Training Loss: 0.093, MSE Loss: 0.063048, , KL Loss: 0.072494, Test Loss: 0.068378, Learning Rate: 0.0006632791969008205\n",
      "Epoch [420/1000] - Training Loss: 0.133, MSE Loss: 0.062840, , KL Loss: 0.167380, Test Loss: 0.068117, Learning Rate: 0.0006619104492241814\n",
      "Epoch [421/1000] - Training Loss: 0.105, MSE Loss: 0.063015, , KL Loss: 0.100816, Test Loss: 0.068155, Learning Rate: 0.0006605405970365888\n",
      "Epoch [422/1000] - Training Loss: 0.108, MSE Loss: 0.062858, , KL Loss: 0.108072, Test Loss: 0.067879, Learning Rate: 0.0006591696538579302\n",
      "Epoch [423/1000] - Training Loss: 0.115, MSE Loss: 0.063057, , KL Loss: 0.123536, Test Loss: 0.068096, Learning Rate: 0.0006577976332188618\n",
      "Epoch [424/1000] - Training Loss: 0.108, MSE Loss: 0.063064, , KL Loss: 0.106540, Test Loss: 0.068364, Learning Rate: 0.0006564245486606731\n",
      "Epoch [425/1000] - Training Loss: 0.107, MSE Loss: 0.062852, , KL Loss: 0.103039, Test Loss: 0.068270, Learning Rate: 0.0006550504137351544\n",
      "Epoch [426/1000] - Training Loss: 0.116, MSE Loss: 0.063047, , KL Loss: 0.123599, Test Loss: 0.068439, Learning Rate: 0.0006536752420044627\n",
      "Epoch [427/1000] - Training Loss: 0.096, MSE Loss: 0.062925, , KL Loss: 0.077757, Test Loss: 0.068257, Learning Rate: 0.0006522990470409879\n",
      "Epoch [428/1000] - Training Loss: 0.084, MSE Loss: 0.063101, , KL Loss: 0.049877, Test Loss: 0.068662, Learning Rate: 0.0006509218424272184\n",
      "Epoch [429/1000] - Training Loss: 0.083, MSE Loss: 0.062812, , KL Loss: 0.048100, Test Loss: 0.067860, Learning Rate: 0.0006495436417556082\n",
      "Epoch [430/1000] - Training Loss: 0.079, MSE Loss: 0.062841, , KL Loss: 0.037944, Test Loss: 0.068006, Learning Rate: 0.0006481644586284413\n",
      "Epoch [431/1000] - Training Loss: 0.102, MSE Loss: 0.063018, , KL Loss: 0.089521, Test Loss: 0.068195, Learning Rate: 0.0006467843066576981\n",
      "Epoch [432/1000] - Training Loss: 0.096, MSE Loss: 0.062918, , KL Loss: 0.076243, Test Loss: 0.068156, Learning Rate: 0.0006454031994649217\n",
      "Epoch [433/1000] - Training Loss: 0.087, MSE Loss: 0.062934, , KL Loss: 0.055538, Test Loss: 0.068211, Learning Rate: 0.0006440211506810823\n",
      "Epoch [434/1000] - Training Loss: 0.110, MSE Loss: 0.063002, , KL Loss: 0.107468, Test Loss: 0.068051, Learning Rate: 0.0006426381739464438\n",
      "Epoch [435/1000] - Training Loss: 0.072, MSE Loss: 0.062788, , KL Loss: 0.021687, Test Loss: 0.067817, Learning Rate: 0.0006412542829104278\n",
      "Epoch [436/1000] - Training Loss: 0.105, MSE Loss: 0.062928, , KL Loss: 0.095801, Test Loss: 0.067685, Learning Rate: 0.0006398694912314803\n",
      "Epoch [437/1000] - Training Loss: 0.087, MSE Loss: 0.062916, , KL Loss: 0.055983, Test Loss: 0.068529, Learning Rate: 0.0006384838125769361\n",
      "Epoch [438/1000] - Training Loss: 0.094, MSE Loss: 0.062925, , KL Loss: 0.070361, Test Loss: 0.068583, Learning Rate: 0.0006370972606228843\n",
      "Epoch [439/1000] - Training Loss: 0.103, MSE Loss: 0.062977, , KL Loss: 0.091434, Test Loss: 0.068122, Learning Rate: 0.0006357098490540327\n",
      "Epoch [440/1000] - Training Loss: 0.092, MSE Loss: 0.062898, , KL Loss: 0.067223, Test Loss: 0.067684, Learning Rate: 0.0006343215915635733\n",
      "Epoch [441/1000] - Training Loss: 0.092, MSE Loss: 0.062943, , KL Loss: 0.065481, Test Loss: 0.067909, Learning Rate: 0.0006329325018530473\n",
      "Epoch [442/1000] - Training Loss: 0.104, MSE Loss: 0.062999, , KL Loss: 0.093049, Test Loss: 0.067694, Learning Rate: 0.0006315425936322091\n",
      "Epoch [443/1000] - Training Loss: 0.094, MSE Loss: 0.062988, , KL Loss: 0.070915, Test Loss: 0.067798, Learning Rate: 0.0006301518806188917\n",
      "Epoch [444/1000] - Training Loss: 0.098, MSE Loss: 0.062987, , KL Loss: 0.078767, Test Loss: 0.067902, Learning Rate: 0.0006287603765388715\n",
      "Epoch [445/1000] - Training Loss: 0.093, MSE Loss: 0.062910, , KL Loss: 0.068263, Test Loss: 0.067989, Learning Rate: 0.0006273680951257316\n",
      "Epoch [446/1000] - Training Loss: 0.099, MSE Loss: 0.062929, , KL Loss: 0.080871, Test Loss: 0.068244, Learning Rate: 0.0006259750501207275\n",
      "Epoch [447/1000] - Training Loss: 0.083, MSE Loss: 0.062896, , KL Loss: 0.045162, Test Loss: 0.068218, Learning Rate: 0.0006245812552726512\n",
      "Epoch [448/1000] - Training Loss: 0.171, MSE Loss: 0.062984, , KL Loss: 0.240249, Test Loss: 0.068428, Learning Rate: 0.000623186724337695\n",
      "Epoch [449/1000] - Training Loss: 0.138, MSE Loss: 0.062963, , KL Loss: 0.166219, Test Loss: 0.068315, Learning Rate: 0.0006217914710793162\n",
      "Epoch [450/1000] - Training Loss: 0.118, MSE Loss: 0.062873, , KL Loss: 0.123035, Test Loss: 0.068112, Learning Rate: 0.0006203955092681014\n",
      "Epoch [451/1000] - Training Loss: 0.078, MSE Loss: 0.062945, , KL Loss: 0.032411, Test Loss: 0.067382, Learning Rate: 0.0006189988526816297\n",
      "Epoch [452/1000] - Training Loss: 0.135, MSE Loss: 0.062917, , KL Loss: 0.159975, Test Loss: 0.067693, Learning Rate: 0.0006176015151043382\n",
      "Epoch [453/1000] - Training Loss: 0.076, MSE Loss: 0.062916, , KL Loss: 0.028637, Test Loss: 0.067651, Learning Rate: 0.0006162035103273843\n",
      "Epoch [454/1000] - Training Loss: 0.120, MSE Loss: 0.062893, , KL Loss: 0.125311, Test Loss: 0.068316, Learning Rate: 0.000614804852148511\n",
      "Epoch [455/1000] - Training Loss: 0.088, MSE Loss: 0.062830, , KL Loss: 0.055407, Test Loss: 0.068792, Learning Rate: 0.0006134055543719096\n",
      "Epoch [456/1000] - Training Loss: 0.089, MSE Loss: 0.062773, , KL Loss: 0.058093, Test Loss: 0.068628, Learning Rate: 0.0006120056308080847\n",
      "Epoch [457/1000] - Training Loss: 0.129, MSE Loss: 0.062857, , KL Loss: 0.145447, Test Loss: 0.068682, Learning Rate: 0.0006106050952737162\n",
      "Epoch [458/1000] - Training Loss: 0.074, MSE Loss: 0.062889, , KL Loss: 0.023639, Test Loss: 0.068172, Learning Rate: 0.0006092039615915246\n",
      "Epoch [459/1000] - Training Loss: 0.151, MSE Loss: 0.062944, , KL Loss: 0.192551, Test Loss: 0.068488, Learning Rate: 0.0006078022435901338\n",
      "Epoch [460/1000] - Training Loss: 0.141, MSE Loss: 0.062804, , KL Loss: 0.170334, Test Loss: 0.067625, Learning Rate: 0.0006063999551039344\n",
      "Epoch [461/1000] - Training Loss: 0.107, MSE Loss: 0.063032, , KL Loss: 0.094357, Test Loss: 0.067854, Learning Rate: 0.0006049971099729478\n",
      "Epoch [462/1000] - Training Loss: 0.131, MSE Loss: 0.062856, , KL Loss: 0.147918, Test Loss: 0.068012, Learning Rate: 0.0006035937220426888\n",
      "Epoch [463/1000] - Training Loss: 0.167, MSE Loss: 0.063003, , KL Loss: 0.224146, Test Loss: 0.068001, Learning Rate: 0.0006021898051640302\n",
      "Epoch [464/1000] - Training Loss: 0.132, MSE Loss: 0.062890, , KL Loss: 0.149414, Test Loss: 0.068757, Learning Rate: 0.0006007853731930643\n",
      "Epoch [465/1000] - Training Loss: 0.134, MSE Loss: 0.062874, , KL Loss: 0.153090, Test Loss: 0.068428, Learning Rate: 0.000599380439990968\n",
      "Epoch [466/1000] - Training Loss: 0.093, MSE Loss: 0.062905, , KL Loss: 0.064549, Test Loss: 0.068246, Learning Rate: 0.0005979750194238645\n",
      "Epoch [467/1000] - Training Loss: 0.126, MSE Loss: 0.062896, , KL Loss: 0.134784, Test Loss: 0.067902, Learning Rate: 0.0005965691253626877\n",
      "Epoch [468/1000] - Training Loss: 0.183, MSE Loss: 0.062850, , KL Loss: 0.256556, Test Loss: 0.068055, Learning Rate: 0.0005951627716830442\n",
      "Epoch [469/1000] - Training Loss: 0.118, MSE Loss: 0.062933, , KL Loss: 0.117398, Test Loss: 0.068231, Learning Rate: 0.0005937559722650773\n",
      "Epoch [470/1000] - Training Loss: 0.214, MSE Loss: 0.062878, , KL Loss: 0.320745, Test Loss: 0.068235, Learning Rate: 0.0005923487409933289\n",
      "Epoch [471/1000] - Training Loss: 0.108, MSE Loss: 0.062844, , KL Loss: 0.095904, Test Loss: 0.068030, Learning Rate: 0.0005909410917566039\n",
      "Epoch [472/1000] - Training Loss: 0.132, MSE Loss: 0.062865, , KL Loss: 0.147067, Test Loss: 0.067880, Learning Rate: 0.000589533038447832\n",
      "Epoch [473/1000] - Training Loss: 0.085, MSE Loss: 0.062922, , KL Loss: 0.046683, Test Loss: 0.067775, Learning Rate: 0.0005881245949639304\n",
      "Epoch [474/1000] - Training Loss: 0.117, MSE Loss: 0.062905, , KL Loss: 0.113843, Test Loss: 0.067726, Learning Rate: 0.0005867157752056685\n",
      "Epoch [475/1000] - Training Loss: 0.091, MSE Loss: 0.062913, , KL Loss: 0.058683, Test Loss: 0.068097, Learning Rate: 0.0005853065930775278\n",
      "Epoch [476/1000] - Training Loss: 0.084, MSE Loss: 0.062948, , KL Loss: 0.044452, Test Loss: 0.068426, Learning Rate: 0.0005838970624875673\n",
      "Epoch [477/1000] - Training Loss: 0.103, MSE Loss: 0.062949, , KL Loss: 0.084639, Test Loss: 0.068789, Learning Rate: 0.0005824871973472849\n",
      "Epoch [478/1000] - Training Loss: 0.094, MSE Loss: 0.063022, , KL Loss: 0.065098, Test Loss: 0.068665, Learning Rate: 0.0005810770115714804\n",
      "Epoch [479/1000] - Training Loss: 0.079, MSE Loss: 0.062867, , KL Loss: 0.034259, Test Loss: 0.068102, Learning Rate: 0.0005796665190781177\n",
      "Epoch [480/1000] - Training Loss: 0.086, MSE Loss: 0.062886, , KL Loss: 0.047371, Test Loss: 0.067238, Learning Rate: 0.0005782557337881888\n",
      "Epoch [481/1000] - Training Loss: 0.103, MSE Loss: 0.062982, , KL Loss: 0.082843, Test Loss: 0.067509, Learning Rate: 0.0005768446696255747\n",
      "Epoch [482/1000] - Training Loss: 0.080, MSE Loss: 0.062795, , KL Loss: 0.035305, Test Loss: 0.067408, Learning Rate: 0.0005754333405169088\n",
      "Epoch [483/1000] - Training Loss: 0.076, MSE Loss: 0.062929, , KL Loss: 0.027749, Test Loss: 0.067992, Learning Rate: 0.00057402176039144\n",
      "Epoch [484/1000] - Training Loss: 0.108, MSE Loss: 0.063074, , KL Loss: 0.093758, Test Loss: 0.068237, Learning Rate: 0.0005726099431808941\n",
      "Epoch [485/1000] - Training Loss: 0.104, MSE Loss: 0.062829, , KL Loss: 0.085232, Test Loss: 0.068386, Learning Rate: 0.000571197902819337\n",
      "Epoch [486/1000] - Training Loss: 0.084, MSE Loss: 0.062714, , KL Loss: 0.044394, Test Loss: 0.068222, Learning Rate: 0.0005697856532430371\n",
      "Epoch [487/1000] - Training Loss: 0.094, MSE Loss: 0.062908, , KL Loss: 0.062907, Test Loss: 0.068324, Learning Rate: 0.0005683732083903275\n",
      "Epoch [488/1000] - Training Loss: 0.119, MSE Loss: 0.062882, , KL Loss: 0.115136, Test Loss: 0.067862, Learning Rate: 0.0005669605822014685\n",
      "Epoch [489/1000] - Training Loss: 0.071, MSE Loss: 0.062880, , KL Loss: 0.017046, Test Loss: 0.067889, Learning Rate: 0.0005655477886185106\n",
      "Epoch [490/1000] - Training Loss: 0.096, MSE Loss: 0.062932, , KL Loss: 0.068482, Test Loss: 0.067655, Learning Rate: 0.0005641348415851557\n",
      "Epoch [491/1000] - Training Loss: 0.136, MSE Loss: 0.062825, , KL Loss: 0.150032, Test Loss: 0.067843, Learning Rate: 0.000562721755046621\n",
      "Epoch [492/1000] - Training Loss: 0.077, MSE Loss: 0.062864, , KL Loss: 0.029701, Test Loss: 0.068111, Learning Rate: 0.0005613085429495\n",
      "Epoch [493/1000] - Training Loss: 0.124, MSE Loss: 0.063036, , KL Loss: 0.123852, Test Loss: 0.068073, Learning Rate: 0.0005598952192416254\n",
      "Epoch [494/1000] - Training Loss: 0.115, MSE Loss: 0.062820, , KL Loss: 0.104656, Test Loss: 0.068238, Learning Rate: 0.0005584817978719318\n",
      "Epoch [495/1000] - Training Loss: 0.100, MSE Loss: 0.062948, , KL Loss: 0.074555, Test Loss: 0.068458, Learning Rate: 0.0005570682927903175\n",
      "Epoch [496/1000] - Training Loss: 0.113, MSE Loss: 0.062915, , KL Loss: 0.099975, Test Loss: 0.067804, Learning Rate: 0.0005556547179475068\n",
      "Epoch [497/1000] - Training Loss: 0.100, MSE Loss: 0.062935, , KL Loss: 0.075521, Test Loss: 0.068307, Learning Rate: 0.000554241087294913\n",
      "Epoch [498/1000] - Training Loss: 0.088, MSE Loss: 0.062874, , KL Loss: 0.050529, Test Loss: 0.068163, Learning Rate: 0.0005528274147844998\n",
      "Epoch [499/1000] - Training Loss: 0.086, MSE Loss: 0.062826, , KL Loss: 0.045567, Test Loss: 0.068061, Learning Rate: 0.000551413714368644\n",
      "Epoch [500/1000] - Training Loss: 0.095, MSE Loss: 0.062920, , KL Loss: 0.064031, Test Loss: 0.067811, Learning Rate: 0.0005499999999999983\n",
      "Epoch [501/1000] - Training Loss: 0.089, MSE Loss: 0.063114, , KL Loss: 0.051983, Test Loss: 0.067659, Learning Rate: 0.0005485862856313525\n",
      "Epoch [502/1000] - Training Loss: 0.085, MSE Loss: 0.062890, , KL Loss: 0.044145, Test Loss: 0.067804, Learning Rate: 0.0005471725852154969\n",
      "Epoch [503/1000] - Training Loss: 0.076, MSE Loss: 0.062903, , KL Loss: 0.026378, Test Loss: 0.067821, Learning Rate: 0.0005457589127050836\n",
      "Epoch [504/1000] - Training Loss: 0.077, MSE Loss: 0.063014, , KL Loss: 0.026899, Test Loss: 0.068124, Learning Rate: 0.0005443452820524898\n",
      "Epoch [505/1000] - Training Loss: 0.099, MSE Loss: 0.062755, , KL Loss: 0.072555, Test Loss: 0.068064, Learning Rate: 0.0005429317072096792\n",
      "Epoch [506/1000] - Training Loss: 0.066, MSE Loss: 0.062819, , KL Loss: 0.007244, Test Loss: 0.068722, Learning Rate: 0.0005415182021280648\n",
      "Epoch [507/1000] - Training Loss: 0.117, MSE Loss: 0.062910, , KL Loss: 0.105935, Test Loss: 0.068593, Learning Rate: 0.0005401047807583713\n",
      "Epoch [508/1000] - Training Loss: 0.087, MSE Loss: 0.062909, , KL Loss: 0.047594, Test Loss: 0.068093, Learning Rate: 0.0005386914570504967\n",
      "Epoch [509/1000] - Training Loss: 0.082, MSE Loss: 0.062814, , KL Loss: 0.037662, Test Loss: 0.068046, Learning Rate: 0.0005372782449533757\n",
      "Epoch [510/1000] - Training Loss: 0.097, MSE Loss: 0.062841, , KL Loss: 0.066730, Test Loss: 0.067739, Learning Rate: 0.000535865158414841\n",
      "Epoch [511/1000] - Training Loss: 0.087, MSE Loss: 0.063091, , KL Loss: 0.047160, Test Loss: 0.067511, Learning Rate: 0.0005344522113814864\n",
      "Epoch [512/1000] - Training Loss: 0.075, MSE Loss: 0.062845, , KL Loss: 0.022786, Test Loss: 0.067352, Learning Rate: 0.0005330394177985282\n",
      "Epoch [513/1000] - Training Loss: 0.085, MSE Loss: 0.062760, , KL Loss: 0.044069, Test Loss: 0.067711, Learning Rate: 0.0005316267916096693\n",
      "Epoch [514/1000] - Training Loss: 0.077, MSE Loss: 0.062862, , KL Loss: 0.027881, Test Loss: 0.067769, Learning Rate: 0.0005302143467569597\n",
      "Epoch [515/1000] - Training Loss: 0.067, MSE Loss: 0.062812, , KL Loss: 0.008461, Test Loss: 0.068136, Learning Rate: 0.0005288020971806597\n",
      "Epoch [516/1000] - Training Loss: 0.088, MSE Loss: 0.062850, , KL Loss: 0.047991, Test Loss: 0.068394, Learning Rate: 0.0005273900568191027\n",
      "Epoch [517/1000] - Training Loss: 0.069, MSE Loss: 0.062865, , KL Loss: 0.011658, Test Loss: 0.068169, Learning Rate: 0.0005259782396085568\n",
      "Epoch [518/1000] - Training Loss: 0.091, MSE Loss: 0.062943, , KL Loss: 0.053851, Test Loss: 0.067850, Learning Rate: 0.000524566659483088\n",
      "Epoch [519/1000] - Training Loss: 0.068, MSE Loss: 0.062874, , KL Loss: 0.010285, Test Loss: 0.068151, Learning Rate: 0.0005231553303744222\n",
      "Epoch [520/1000] - Training Loss: 0.079, MSE Loss: 0.062906, , KL Loss: 0.031686, Test Loss: 0.067594, Learning Rate: 0.0005217442662118081\n",
      "Epoch [521/1000] - Training Loss: 0.074, MSE Loss: 0.062923, , KL Loss: 0.021982, Test Loss: 0.067576, Learning Rate: 0.000520333480921879\n",
      "Epoch [522/1000] - Training Loss: 0.070, MSE Loss: 0.062943, , KL Loss: 0.014318, Test Loss: 0.067810, Learning Rate: 0.0005189229884285166\n",
      "Epoch [523/1000] - Training Loss: 0.076, MSE Loss: 0.062780, , KL Loss: 0.025427, Test Loss: 0.068192, Learning Rate: 0.0005175128026527119\n",
      "Epoch [524/1000] - Training Loss: 0.069, MSE Loss: 0.062951, , KL Loss: 0.011962, Test Loss: 0.068072, Learning Rate: 0.0005161029375124294\n",
      "Epoch [525/1000] - Training Loss: 0.076, MSE Loss: 0.062914, , KL Loss: 0.025764, Test Loss: 0.067610, Learning Rate: 0.000514693406922469\n",
      "Epoch [526/1000] - Training Loss: 0.067, MSE Loss: 0.062837, , KL Loss: 0.007085, Test Loss: 0.068305, Learning Rate: 0.0005132842247943284\n",
      "Epoch [527/1000] - Training Loss: 0.073, MSE Loss: 0.062913, , KL Loss: 0.019447, Test Loss: 0.067833, Learning Rate: 0.0005118754050360663\n",
      "Epoch [528/1000] - Training Loss: 0.073, MSE Loss: 0.062813, , KL Loss: 0.020025, Test Loss: 0.067753, Learning Rate: 0.0005104669615521649\n",
      "Epoch [529/1000] - Training Loss: 0.072, MSE Loss: 0.062950, , KL Loss: 0.017880, Test Loss: 0.067803, Learning Rate: 0.000509058908243393\n",
      "Epoch [530/1000] - Training Loss: 0.077, MSE Loss: 0.062840, , KL Loss: 0.026433, Test Loss: 0.067877, Learning Rate: 0.0005076512590066679\n",
      "Epoch [531/1000] - Training Loss: 0.064, MSE Loss: 0.062689, , KL Loss: 0.003372, Test Loss: 0.068354, Learning Rate: 0.0005062440277349198\n",
      "Epoch [532/1000] - Training Loss: 0.099, MSE Loss: 0.062821, , KL Loss: 0.067932, Test Loss: 0.068404, Learning Rate: 0.0005048372283169529\n",
      "Epoch [533/1000] - Training Loss: 0.068, MSE Loss: 0.062920, , KL Loss: 0.008770, Test Loss: 0.068184, Learning Rate: 0.0005034308746373094\n",
      "Epoch [534/1000] - Training Loss: 0.107, MSE Loss: 0.062771, , KL Loss: 0.082505, Test Loss: 0.068072, Learning Rate: 0.0005020249805761326\n",
      "Epoch [535/1000] - Training Loss: 0.068, MSE Loss: 0.062841, , KL Loss: 0.009360, Test Loss: 0.067988, Learning Rate: 0.0005006195600090292\n",
      "Epoch [536/1000] - Training Loss: 0.074, MSE Loss: 0.062905, , KL Loss: 0.020397, Test Loss: 0.067886, Learning Rate: 0.0004992146268069328\n",
      "Epoch [537/1000] - Training Loss: 0.064, MSE Loss: 0.062896, , KL Loss: 0.002958, Test Loss: 0.067662, Learning Rate: 0.000497810194835967\n",
      "Epoch [538/1000] - Training Loss: 0.069, MSE Loss: 0.062829, , KL Loss: 0.012148, Test Loss: 0.068226, Learning Rate: 0.0004964062779573083\n",
      "Epoch [539/1000] - Training Loss: 0.068, MSE Loss: 0.062928, , KL Loss: 0.009380, Test Loss: 0.068115, Learning Rate: 0.0004950028900270494\n",
      "Epoch [540/1000] - Training Loss: 0.065, MSE Loss: 0.062794, , KL Loss: 0.004945, Test Loss: 0.068202, Learning Rate: 0.0004936000448960629\n",
      "Epoch [541/1000] - Training Loss: 0.063, MSE Loss: 0.063021, , KL Loss: 0.000863, Test Loss: 0.068237, Learning Rate: 0.0004921977564098636\n",
      "Epoch [542/1000] - Training Loss: 0.066, MSE Loss: 0.062819, , KL Loss: 0.005311, Test Loss: 0.068260, Learning Rate: 0.0004907960384084727\n",
      "Epoch [543/1000] - Training Loss: 0.067, MSE Loss: 0.062813, , KL Loss: 0.008507, Test Loss: 0.068224, Learning Rate: 0.0004893949047262812\n",
      "Epoch [544/1000] - Training Loss: 0.061, MSE Loss: 0.062850, , KL Loss: -0.004278, Test Loss: 0.067869, Learning Rate: 0.0004879943691919127\n",
      "Epoch [545/1000] - Training Loss: 0.068, MSE Loss: 0.062809, , KL Loss: 0.009446, Test Loss: 0.068085, Learning Rate: 0.0004865944456280877\n",
      "Epoch [546/1000] - Training Loss: 0.063, MSE Loss: 0.062778, , KL Loss: -0.000202, Test Loss: 0.068323, Learning Rate: 0.0004851951478514864\n",
      "Epoch [547/1000] - Training Loss: 0.063, MSE Loss: 0.062851, , KL Loss: 0.000678, Test Loss: 0.067824, Learning Rate: 0.0004837964896726131\n",
      "Epoch [548/1000] - Training Loss: 0.066, MSE Loss: 0.062887, , KL Loss: 0.006121, Test Loss: 0.067898, Learning Rate: 0.00048239848489565915\n",
      "Epoch [549/1000] - Training Loss: 0.061, MSE Loss: 0.062944, , KL Loss: -0.004165, Test Loss: 0.068061, Learning Rate: 0.0004810011473183676\n",
      "Epoch [550/1000] - Training Loss: 0.072, MSE Loss: 0.062982, , KL Loss: 0.015605, Test Loss: 0.068152, Learning Rate: 0.000479604490731896\n",
      "Epoch [551/1000] - Training Loss: 0.061, MSE Loss: 0.062782, , KL Loss: -0.002817, Test Loss: 0.068322, Learning Rate: 0.000478208528920681\n",
      "Epoch [552/1000] - Training Loss: 0.060, MSE Loss: 0.062895, , KL Loss: -0.005041, Test Loss: 0.068191, Learning Rate: 0.0004768132756623022\n",
      "Epoch [553/1000] - Training Loss: 0.066, MSE Loss: 0.062783, , KL Loss: 0.005497, Test Loss: 0.068096, Learning Rate: 0.000475418744727346\n",
      "Epoch [554/1000] - Training Loss: 0.070, MSE Loss: 0.062879, , KL Loss: 0.012546, Test Loss: 0.067787, Learning Rate: 0.0004740249498792695\n",
      "Epoch [555/1000] - Training Loss: 0.068, MSE Loss: 0.062882, , KL Loss: 0.009264, Test Loss: 0.067821, Learning Rate: 0.0004726319048742654\n",
      "Epoch [556/1000] - Training Loss: 0.092, MSE Loss: 0.062944, , KL Loss: 0.053018, Test Loss: 0.067433, Learning Rate: 0.0004712396234611256\n",
      "Epoch [557/1000] - Training Loss: 0.067, MSE Loss: 0.062822, , KL Loss: 0.006748, Test Loss: 0.068073, Learning Rate: 0.00046984811938110513\n",
      "Epoch [558/1000] - Training Loss: 0.066, MSE Loss: 0.062810, , KL Loss: 0.005457, Test Loss: 0.068029, Learning Rate: 0.000468457406367788\n",
      "Epoch [559/1000] - Training Loss: 0.069, MSE Loss: 0.062794, , KL Loss: 0.011193, Test Loss: 0.068291, Learning Rate: 0.0004670674981469498\n",
      "Epoch [560/1000] - Training Loss: 0.069, MSE Loss: 0.062829, , KL Loss: 0.011242, Test Loss: 0.068430, Learning Rate: 0.0004656784084364237\n",
      "Epoch [561/1000] - Training Loss: 0.067, MSE Loss: 0.062911, , KL Loss: 0.007781, Test Loss: 0.068686, Learning Rate: 0.00046429015094596435\n",
      "Epoch [562/1000] - Training Loss: 0.061, MSE Loss: 0.062769, , KL Loss: -0.002761, Test Loss: 0.067905, Learning Rate: 0.00046290273937711277\n",
      "Epoch [563/1000] - Training Loss: 0.061, MSE Loss: 0.062812, , KL Loss: -0.002930, Test Loss: 0.067463, Learning Rate: 0.0004615161874230607\n",
      "Epoch [564/1000] - Training Loss: 0.058, MSE Loss: 0.062741, , KL Loss: -0.007638, Test Loss: 0.067661, Learning Rate: 0.0004601305087685167\n",
      "Epoch [565/1000] - Training Loss: 0.065, MSE Loss: 0.062779, , KL Loss: 0.003301, Test Loss: 0.067736, Learning Rate: 0.00045874571708956924\n",
      "Epoch [566/1000] - Training Loss: 0.059, MSE Loss: 0.062814, , KL Loss: -0.006089, Test Loss: 0.068187, Learning Rate: 0.0004573618260535532\n",
      "Epoch [567/1000] - Training Loss: 0.065, MSE Loss: 0.062944, , KL Loss: 0.003370, Test Loss: 0.068126, Learning Rate: 0.00045597884931891457\n",
      "Epoch [568/1000] - Training Loss: 0.061, MSE Loss: 0.062772, , KL Loss: -0.002715, Test Loss: 0.068098, Learning Rate: 0.0004545968005350752\n",
      "Epoch [569/1000] - Training Loss: 0.064, MSE Loss: 0.062941, , KL Loss: 0.002472, Test Loss: 0.067663, Learning Rate: 0.0004532156933422988\n",
      "Epoch [570/1000] - Training Loss: 0.062, MSE Loss: 0.062727, , KL Loss: -0.001757, Test Loss: 0.067944, Learning Rate: 0.0004518355413715557\n",
      "Epoch [571/1000] - Training Loss: 0.059, MSE Loss: 0.062819, , KL Loss: -0.007410, Test Loss: 0.068016, Learning Rate: 0.0004504563582443887\n",
      "Epoch [572/1000] - Training Loss: 0.066, MSE Loss: 0.062660, , KL Loss: 0.006166, Test Loss: 0.067783, Learning Rate: 0.0004490781575727783\n",
      "Epoch [573/1000] - Training Loss: 0.058, MSE Loss: 0.062899, , KL Loss: -0.007782, Test Loss: 0.067803, Learning Rate: 0.00044770095295900905\n",
      "Epoch [574/1000] - Training Loss: 0.073, MSE Loss: 0.062891, , KL Loss: 0.016807, Test Loss: 0.067771, Learning Rate: 0.0004463247579955342\n",
      "Epoch [575/1000] - Training Loss: 0.057, MSE Loss: 0.062794, , KL Loss: -0.009461, Test Loss: 0.067754, Learning Rate: 0.00044494958626484254\n",
      "Epoch [576/1000] - Training Loss: 0.063, MSE Loss: 0.062704, , KL Loss: 0.000170, Test Loss: 0.068447, Learning Rate: 0.0004435754513393238\n",
      "Epoch [577/1000] - Training Loss: 0.061, MSE Loss: 0.062769, , KL Loss: -0.003416, Test Loss: 0.067789, Learning Rate: 0.0004422023667811351\n",
      "Epoch [578/1000] - Training Loss: 0.060, MSE Loss: 0.062742, , KL Loss: -0.004428, Test Loss: 0.067949, Learning Rate: 0.0004408303461420665\n",
      "Epoch [579/1000] - Training Loss: 0.062, MSE Loss: 0.062754, , KL Loss: -0.000849, Test Loss: 0.067702, Learning Rate: 0.00043945940296340804\n",
      "Epoch [580/1000] - Training Loss: 0.060, MSE Loss: 0.062803, , KL Loss: -0.004715, Test Loss: 0.068006, Learning Rate: 0.0004380895507758153\n",
      "Epoch [581/1000] - Training Loss: 0.063, MSE Loss: 0.062765, , KL Loss: 0.000490, Test Loss: 0.067731, Learning Rate: 0.00043672080309917617\n",
      "Epoch [582/1000] - Training Loss: 0.066, MSE Loss: 0.062874, , KL Loss: 0.005215, Test Loss: 0.068252, Learning Rate: 0.000435353173442478\n",
      "Epoch [583/1000] - Training Loss: 0.058, MSE Loss: 0.062758, , KL Loss: -0.008693, Test Loss: 0.067784, Learning Rate: 0.00043398667530367304\n",
      "Epoch [584/1000] - Training Loss: 0.067, MSE Loss: 0.062764, , KL Loss: 0.007176, Test Loss: 0.067804, Learning Rate: 0.00043262132216954636\n",
      "Epoch [585/1000] - Training Loss: 0.063, MSE Loss: 0.062904, , KL Loss: 0.000234, Test Loss: 0.067604, Learning Rate: 0.0004312571275155822\n",
      "Epoch [586/1000] - Training Loss: 0.060, MSE Loss: 0.062755, , KL Loss: -0.004696, Test Loss: 0.067457, Learning Rate: 0.00042989410480583106\n",
      "Epoch [587/1000] - Training Loss: 0.067, MSE Loss: 0.062839, , KL Loss: 0.007406, Test Loss: 0.067800, Learning Rate: 0.0004285322674927766\n",
      "Epoch [588/1000] - Training Loss: 0.058, MSE Loss: 0.062764, , KL Loss: -0.008336, Test Loss: 0.067777, Learning Rate: 0.00042717162901720357\n",
      "Epoch [589/1000] - Training Loss: 0.070, MSE Loss: 0.062809, , KL Loss: 0.011893, Test Loss: 0.067581, Learning Rate: 0.00042581220280806447\n",
      "Epoch [590/1000] - Training Loss: 0.059, MSE Loss: 0.062703, , KL Loss: -0.006928, Test Loss: 0.068474, Learning Rate: 0.00042445400228234673\n",
      "Epoch [591/1000] - Training Loss: 0.060, MSE Loss: 0.062890, , KL Loss: -0.004622, Test Loss: 0.068375, Learning Rate: 0.0004230970408449418\n",
      "Epoch [592/1000] - Training Loss: 0.062, MSE Loss: 0.062813, , KL Loss: -0.001750, Test Loss: 0.068173, Learning Rate: 0.0004217413318885107\n",
      "Epoch [593/1000] - Training Loss: 0.066, MSE Loss: 0.062780, , KL Loss: 0.005911, Test Loss: 0.068120, Learning Rate: 0.0004203868887933539\n",
      "Epoch [594/1000] - Training Loss: 0.066, MSE Loss: 0.062806, , KL Loss: 0.005054, Test Loss: 0.067454, Learning Rate: 0.00041903372492727763\n",
      "Epoch [595/1000] - Training Loss: 0.073, MSE Loss: 0.062839, , KL Loss: 0.017082, Test Loss: 0.067399, Learning Rate: 0.0004176818536454632\n",
      "Epoch [596/1000] - Training Loss: 0.066, MSE Loss: 0.062844, , KL Loss: 0.004618, Test Loss: 0.067478, Learning Rate: 0.0004163312882903342\n",
      "Epoch [597/1000] - Training Loss: 0.061, MSE Loss: 0.062694, , KL Loss: -0.003021, Test Loss: 0.067645, Learning Rate: 0.0004149820421914257\n",
      "Epoch [598/1000] - Training Loss: 0.066, MSE Loss: 0.062808, , KL Loss: 0.005354, Test Loss: 0.068082, Learning Rate: 0.0004136341286652517\n",
      "Epoch [599/1000] - Training Loss: 0.068, MSE Loss: 0.062722, , KL Loss: 0.008440, Test Loss: 0.068140, Learning Rate: 0.00041228756101517453\n",
      "Epoch [600/1000] - Training Loss: 0.063, MSE Loss: 0.062740, , KL Loss: 0.001169, Test Loss: 0.068181, Learning Rate: 0.0004109423525312736\n",
      "Epoch [601/1000] - Training Loss: 0.068, MSE Loss: 0.062921, , KL Loss: 0.008894, Test Loss: 0.067856, Learning Rate: 0.0004095985164902133\n",
      "Epoch [602/1000] - Training Loss: 0.064, MSE Loss: 0.062792, , KL Loss: 0.002187, Test Loss: 0.068148, Learning Rate: 0.00040825606615511276\n",
      "Epoch [603/1000] - Training Loss: 0.062, MSE Loss: 0.062759, , KL Loss: -0.000810, Test Loss: 0.068331, Learning Rate: 0.000406915014775415\n",
      "Epoch [604/1000] - Training Loss: 0.057, MSE Loss: 0.062894, , KL Loss: -0.009205, Test Loss: 0.067716, Learning Rate: 0.0004055753755867557\n",
      "Epoch [605/1000] - Training Loss: 0.076, MSE Loss: 0.062811, , KL Loss: 0.021648, Test Loss: 0.067530, Learning Rate: 0.00040423716181083264\n",
      "Epoch [606/1000] - Training Loss: 0.058, MSE Loss: 0.062718, , KL Loss: -0.007059, Test Loss: 0.068268, Learning Rate: 0.0004029003866552758\n",
      "Epoch [607/1000] - Training Loss: 0.062, MSE Loss: 0.062859, , KL Loss: -0.001474, Test Loss: 0.067770, Learning Rate: 0.0004015650633135161\n",
      "Epoch [608/1000] - Training Loss: 0.082, MSE Loss: 0.062784, , KL Loss: 0.031860, Test Loss: 0.067875, Learning Rate: 0.00040023120496465595\n",
      "Epoch [609/1000] - Training Loss: 0.062, MSE Loss: 0.062796, , KL Loss: -0.001007, Test Loss: 0.067736, Learning Rate: 0.0003988988247733386\n",
      "Epoch [610/1000] - Training Loss: 0.065, MSE Loss: 0.062765, , KL Loss: 0.004409, Test Loss: 0.068280, Learning Rate: 0.00039756793588961877\n",
      "Epoch [611/1000] - Training Loss: 0.074, MSE Loss: 0.062822, , KL Loss: 0.017773, Test Loss: 0.068182, Learning Rate: 0.0003962385514488324\n",
      "Epoch [612/1000] - Training Loss: 0.064, MSE Loss: 0.062689, , KL Loss: 0.002156, Test Loss: 0.068082, Learning Rate: 0.00039491068457146717\n",
      "Epoch [613/1000] - Training Loss: 0.073, MSE Loss: 0.062787, , KL Loss: 0.017460, Test Loss: 0.068191, Learning Rate: 0.00039358434836303324\n",
      "Epoch [614/1000] - Training Loss: 0.062, MSE Loss: 0.062965, , KL Loss: -0.000980, Test Loss: 0.068023, Learning Rate: 0.00039225955591393327\n",
      "Epoch [615/1000] - Training Loss: 0.063, MSE Loss: 0.062783, , KL Loss: 0.000067, Test Loss: 0.068014, Learning Rate: 0.0003909363202993342\n",
      "Epoch [616/1000] - Training Loss: 0.063, MSE Loss: 0.062844, , KL Loss: -0.000356, Test Loss: 0.068146, Learning Rate: 0.00038961465457903707\n",
      "Epoch [617/1000] - Training Loss: 0.064, MSE Loss: 0.062740, , KL Loss: 0.001545, Test Loss: 0.067746, Learning Rate: 0.0003882945717973491\n",
      "Epoch [618/1000] - Training Loss: 0.062, MSE Loss: 0.062748, , KL Loss: -0.001805, Test Loss: 0.067554, Learning Rate: 0.00038697608498295435\n",
      "Epoch [619/1000] - Training Loss: 0.067, MSE Loss: 0.062732, , KL Loss: 0.006303, Test Loss: 0.067649, Learning Rate: 0.0003856592071487854\n",
      "Epoch [620/1000] - Training Loss: 0.060, MSE Loss: 0.062802, , KL Loss: -0.004395, Test Loss: 0.068080, Learning Rate: 0.00038434395129189486\n",
      "Epoch [621/1000] - Training Loss: 0.071, MSE Loss: 0.062857, , KL Loss: 0.013722, Test Loss: 0.068124, Learning Rate: 0.0003830303303933269\n",
      "Epoch [622/1000] - Training Loss: 0.061, MSE Loss: 0.062695, , KL Loss: -0.002951, Test Loss: 0.068335, Learning Rate: 0.00038171835741798976\n",
      "Epoch [623/1000] - Training Loss: 0.073, MSE Loss: 0.062755, , KL Loss: 0.016764, Test Loss: 0.068438, Learning Rate: 0.00038040804531452665\n",
      "Epoch [624/1000] - Training Loss: 0.063, MSE Loss: 0.062785, , KL Loss: 0.001030, Test Loss: 0.068125, Learning Rate: 0.00037909940701518944\n",
      "Epoch [625/1000] - Training Loss: 0.079, MSE Loss: 0.062827, , KL Loss: 0.025277, Test Loss: 0.068192, Learning Rate: 0.0003777924554357095\n",
      "Epoch [626/1000] - Training Loss: 0.062, MSE Loss: 0.062897, , KL Loss: -0.002154, Test Loss: 0.068348, Learning Rate: 0.00037648720347517145\n",
      "Epoch [627/1000] - Training Loss: 0.067, MSE Loss: 0.062784, , KL Loss: 0.006124, Test Loss: 0.067739, Learning Rate: 0.00037518366401588515\n",
      "Epoch [628/1000] - Training Loss: 0.062, MSE Loss: 0.062730, , KL Loss: -0.001805, Test Loss: 0.067694, Learning Rate: 0.0003738818499232589\n",
      "Epoch [629/1000] - Training Loss: 0.062, MSE Loss: 0.062899, , KL Loss: -0.000638, Test Loss: 0.067955, Learning Rate: 0.0003725817740456719\n",
      "Epoch [630/1000] - Training Loss: 0.061, MSE Loss: 0.062799, , KL Loss: -0.002161, Test Loss: 0.067999, Learning Rate: 0.00037128344921434853\n",
      "Epoch [631/1000] - Training Loss: 0.061, MSE Loss: 0.062664, , KL Loss: -0.002150, Test Loss: 0.067867, Learning Rate: 0.0003699868882432308\n",
      "Epoch [632/1000] - Training Loss: 0.066, MSE Loss: 0.062653, , KL Loss: 0.005345, Test Loss: 0.068371, Learning Rate: 0.0003686921039288517\n",
      "Epoch [633/1000] - Training Loss: 0.060, MSE Loss: 0.062738, , KL Loss: -0.004138, Test Loss: 0.067991, Learning Rate: 0.00036739910905020997\n",
      "Epoch [634/1000] - Training Loss: 0.065, MSE Loss: 0.062686, , KL Loss: 0.004408, Test Loss: 0.067695, Learning Rate: 0.00036610791636864287\n",
      "Epoch [635/1000] - Training Loss: 0.068, MSE Loss: 0.062736, , KL Loss: 0.007867, Test Loss: 0.068011, Learning Rate: 0.0003648185386277009\n",
      "Epoch [636/1000] - Training Loss: 0.059, MSE Loss: 0.062707, , KL Loss: -0.005412, Test Loss: 0.068217, Learning Rate: 0.00036353098855302203\n",
      "Epoch [637/1000] - Training Loss: 0.062, MSE Loss: 0.062746, , KL Loss: -0.001483, Test Loss: 0.067897, Learning Rate: 0.0003622452788522056\n",
      "Epoch [638/1000] - Training Loss: 0.058, MSE Loss: 0.062958, , KL Loss: -0.008135, Test Loss: 0.067843, Learning Rate: 0.00036096142221468707\n",
      "Epoch [639/1000] - Training Loss: 0.063, MSE Loss: 0.062703, , KL Loss: -0.000153, Test Loss: 0.068053, Learning Rate: 0.0003596794313116135\n",
      "Epoch [640/1000] - Training Loss: 0.062, MSE Loss: 0.062668, , KL Loss: -0.000499, Test Loss: 0.068270, Learning Rate: 0.00035839931879571717\n",
      "Epoch [641/1000] - Training Loss: 0.056, MSE Loss: 0.062857, , KL Loss: -0.010792, Test Loss: 0.067961, Learning Rate: 0.00035712109730119223\n",
      "Epoch [642/1000] - Training Loss: 0.059, MSE Loss: 0.062806, , KL Loss: -0.006686, Test Loss: 0.067831, Learning Rate: 0.0003558447794435685\n",
      "Epoch [643/1000] - Training Loss: 0.058, MSE Loss: 0.062732, , KL Loss: -0.007039, Test Loss: 0.068045, Learning Rate: 0.00035457037781958786\n",
      "Epoch [644/1000] - Training Loss: 0.059, MSE Loss: 0.062664, , KL Loss: -0.006452, Test Loss: 0.067826, Learning Rate: 0.0003532979050070802\n",
      "Epoch [645/1000] - Training Loss: 0.058, MSE Loss: 0.062700, , KL Loss: -0.007014, Test Loss: 0.067906, Learning Rate: 0.00035202737356483806\n",
      "Epoch [646/1000] - Training Loss: 0.056, MSE Loss: 0.062842, , KL Loss: -0.010080, Test Loss: 0.068104, Learning Rate: 0.0003507587960324943\n",
      "Epoch [647/1000] - Training Loss: 0.057, MSE Loss: 0.062879, , KL Loss: -0.009149, Test Loss: 0.067938, Learning Rate: 0.0003494921849303966\n",
      "Epoch [648/1000] - Training Loss: 0.056, MSE Loss: 0.062698, , KL Loss: -0.009661, Test Loss: 0.067948, Learning Rate: 0.0003482275527594855\n",
      "Epoch [649/1000] - Training Loss: 0.068, MSE Loss: 0.062800, , KL Loss: 0.008495, Test Loss: 0.068315, Learning Rate: 0.0003469649120011696\n",
      "Epoch [650/1000] - Training Loss: 0.061, MSE Loss: 0.062677, , KL Loss: -0.003225, Test Loss: 0.067980, Learning Rate: 0.0003457042751172038\n",
      "Epoch [651/1000] - Training Loss: 0.059, MSE Loss: 0.062695, , KL Loss: -0.005289, Test Loss: 0.068050, Learning Rate: 0.0003444456545495652\n",
      "Epoch [652/1000] - Training Loss: 0.054, MSE Loss: 0.062774, , KL Loss: -0.012734, Test Loss: 0.067738, Learning Rate: 0.0003431890627203303\n",
      "Epoch [653/1000] - Training Loss: 0.060, MSE Loss: 0.062769, , KL Loss: -0.003654, Test Loss: 0.067825, Learning Rate: 0.00034193451203155345\n",
      "Epoch [654/1000] - Training Loss: 0.057, MSE Loss: 0.062742, , KL Loss: -0.008923, Test Loss: 0.067724, Learning Rate: 0.0003406820148651436\n",
      "Epoch [655/1000] - Training Loss: 0.059, MSE Loss: 0.062857, , KL Loss: -0.006385, Test Loss: 0.067608, Learning Rate: 0.00033943158358274193\n",
      "Epoch [656/1000] - Training Loss: 0.057, MSE Loss: 0.062893, , KL Loss: -0.008302, Test Loss: 0.067721, Learning Rate: 0.0003381832305256004\n",
      "Epoch [657/1000] - Training Loss: 0.056, MSE Loss: 0.062685, , KL Loss: -0.010751, Test Loss: 0.068194, Learning Rate: 0.00033693696801445947\n",
      "Epoch [658/1000] - Training Loss: 0.055, MSE Loss: 0.062768, , KL Loss: -0.011157, Test Loss: 0.067919, Learning Rate: 0.0003356928083494273\n",
      "Epoch [659/1000] - Training Loss: 0.055, MSE Loss: 0.062954, , KL Loss: -0.011997, Test Loss: 0.067932, Learning Rate: 0.0003344507638098575\n",
      "Epoch [660/1000] - Training Loss: 0.055, MSE Loss: 0.062676, , KL Loss: -0.012197, Test Loss: 0.067985, Learning Rate: 0.0003332108466542282\n",
      "Epoch [661/1000] - Training Loss: 0.055, MSE Loss: 0.062689, , KL Loss: -0.012150, Test Loss: 0.068084, Learning Rate: 0.00033197306912002085\n",
      "Epoch [662/1000] - Training Loss: 0.055, MSE Loss: 0.062719, , KL Loss: -0.011960, Test Loss: 0.068109, Learning Rate: 0.0003307374434236005\n",
      "Epoch [663/1000] - Training Loss: 0.056, MSE Loss: 0.062828, , KL Loss: -0.011027, Test Loss: 0.067865, Learning Rate: 0.0003295039817600937\n",
      "Epoch [664/1000] - Training Loss: 0.055, MSE Loss: 0.062808, , KL Loss: -0.011263, Test Loss: 0.067674, Learning Rate: 0.0003282726963032687\n",
      "Epoch [665/1000] - Training Loss: 0.056, MSE Loss: 0.062760, , KL Loss: -0.010211, Test Loss: 0.067827, Learning Rate: 0.0003270435992054166\n",
      "Epoch [666/1000] - Training Loss: 0.057, MSE Loss: 0.062660, , KL Loss: -0.008431, Test Loss: 0.067717, Learning Rate: 0.00032581670259722913\n",
      "Epoch [667/1000] - Training Loss: 0.056, MSE Loss: 0.062820, , KL Loss: -0.009987, Test Loss: 0.068377, Learning Rate: 0.00032459201858768067\n",
      "Epoch [668/1000] - Training Loss: 0.054, MSE Loss: 0.062751, , KL Loss: -0.013701, Test Loss: 0.067755, Learning Rate: 0.00032336955926390765\n",
      "Epoch [669/1000] - Training Loss: 0.058, MSE Loss: 0.062800, , KL Loss: -0.007495, Test Loss: 0.068014, Learning Rate: 0.00032214933669109046\n",
      "Epoch [670/1000] - Training Loss: 0.056, MSE Loss: 0.062940, , KL Loss: -0.009837, Test Loss: 0.067792, Learning Rate: 0.0003209313629123329\n",
      "Epoch [671/1000] - Training Loss: 0.055, MSE Loss: 0.062749, , KL Loss: -0.011573, Test Loss: 0.067831, Learning Rate: 0.0003197156499485449\n",
      "Epoch [672/1000] - Training Loss: 0.056, MSE Loss: 0.062796, , KL Loss: -0.009987, Test Loss: 0.067699, Learning Rate: 0.00031850220979832213\n",
      "Epoch [673/1000] - Training Loss: 0.053, MSE Loss: 0.062753, , KL Loss: -0.014126, Test Loss: 0.067558, Learning Rate: 0.0003172910544378294\n",
      "Epoch [674/1000] - Training Loss: 0.055, MSE Loss: 0.062790, , KL Loss: -0.011136, Test Loss: 0.067672, Learning Rate: 0.00031608219582068093\n",
      "Epoch [675/1000] - Training Loss: 0.057, MSE Loss: 0.062780, , KL Loss: -0.008218, Test Loss: 0.068060, Learning Rate: 0.00031487564587782306\n",
      "Epoch [676/1000] - Training Loss: 0.058, MSE Loss: 0.062753, , KL Loss: -0.007661, Test Loss: 0.067986, Learning Rate: 0.0003136714165174169\n",
      "Epoch [677/1000] - Training Loss: 0.059, MSE Loss: 0.062740, , KL Loss: -0.005683, Test Loss: 0.067873, Learning Rate: 0.0003124695196247202\n",
      "Epoch [678/1000] - Training Loss: 0.058, MSE Loss: 0.062828, , KL Loss: -0.006745, Test Loss: 0.068228, Learning Rate: 0.00031126996706196987\n",
      "Epoch [679/1000] - Training Loss: 0.056, MSE Loss: 0.062840, , KL Loss: -0.010743, Test Loss: 0.068371, Learning Rate: 0.00031007277066826526\n",
      "Epoch [680/1000] - Training Loss: 0.055, MSE Loss: 0.062782, , KL Loss: -0.011846, Test Loss: 0.068161, Learning Rate: 0.0003088779422594517\n",
      "Epoch [681/1000] - Training Loss: 0.057, MSE Loss: 0.062841, , KL Loss: -0.008961, Test Loss: 0.067950, Learning Rate: 0.00030768549362800314\n",
      "Epoch [682/1000] - Training Loss: 0.059, MSE Loss: 0.062680, , KL Loss: -0.005699, Test Loss: 0.067526, Learning Rate: 0.00030649543654290604\n",
      "Epoch [683/1000] - Training Loss: 0.055, MSE Loss: 0.062666, , KL Loss: -0.010652, Test Loss: 0.068180, Learning Rate: 0.00030530778274954345\n",
      "Epoch [684/1000] - Training Loss: 0.062, MSE Loss: 0.062777, , KL Loss: -0.001789, Test Loss: 0.067931, Learning Rate: 0.00030412254396957906\n",
      "Epoch [685/1000] - Training Loss: 0.055, MSE Loss: 0.062723, , KL Loss: -0.011318, Test Loss: 0.067449, Learning Rate: 0.00030293973190084094\n",
      "Epoch [686/1000] - Training Loss: 0.056, MSE Loss: 0.062722, , KL Loss: -0.009467, Test Loss: 0.067762, Learning Rate: 0.0003017593582172065\n",
      "Epoch [687/1000] - Training Loss: 0.059, MSE Loss: 0.062849, , KL Loss: -0.006034, Test Loss: 0.068076, Learning Rate: 0.00030058143456848787\n",
      "Epoch [688/1000] - Training Loss: 0.054, MSE Loss: 0.062761, , KL Loss: -0.013083, Test Loss: 0.067652, Learning Rate: 0.0002994059725803156\n",
      "Epoch [689/1000] - Training Loss: 0.058, MSE Loss: 0.062874, , KL Loss: -0.007563, Test Loss: 0.067994, Learning Rate: 0.0002982329838540251\n",
      "Epoch [690/1000] - Training Loss: 0.058, MSE Loss: 0.062902, , KL Loss: -0.007613, Test Loss: 0.068077, Learning Rate: 0.0002970624799665414\n",
      "Epoch [691/1000] - Training Loss: 0.055, MSE Loss: 0.062767, , KL Loss: -0.011515, Test Loss: 0.068169, Learning Rate: 0.00029589447247026554\n",
      "Epoch [692/1000] - Training Loss: 0.055, MSE Loss: 0.062675, , KL Loss: -0.010722, Test Loss: 0.067818, Learning Rate: 0.0002947289728929599\n",
      "Epoch [693/1000] - Training Loss: 0.056, MSE Loss: 0.062748, , KL Loss: -0.009986, Test Loss: 0.067613, Learning Rate: 0.0002935659927376347\n",
      "Epoch [694/1000] - Training Loss: 0.055, MSE Loss: 0.062848, , KL Loss: -0.011365, Test Loss: 0.068041, Learning Rate: 0.00029240554348243427\n",
      "Epoch [695/1000] - Training Loss: 0.057, MSE Loss: 0.062715, , KL Loss: -0.007815, Test Loss: 0.067793, Learning Rate: 0.0002912476365805249\n",
      "Epoch [696/1000] - Training Loss: 0.058, MSE Loss: 0.062739, , KL Loss: -0.006449, Test Loss: 0.067806, Learning Rate: 0.0002900922834599799\n",
      "Epoch [697/1000] - Training Loss: 0.053, MSE Loss: 0.062859, , KL Loss: -0.013467, Test Loss: 0.067835, Learning Rate: 0.00028893949552366804\n",
      "Epoch [698/1000] - Training Loss: 0.055, MSE Loss: 0.062789, , KL Loss: -0.010516, Test Loss: 0.067823, Learning Rate: 0.0002877892841491411\n",
      "Epoch [699/1000] - Training Loss: 0.055, MSE Loss: 0.062887, , KL Loss: -0.011377, Test Loss: 0.067680, Learning Rate: 0.0002866416606885209\n",
      "Epoch [700/1000] - Training Loss: 0.053, MSE Loss: 0.062654, , KL Loss: -0.014378, Test Loss: 0.067842, Learning Rate: 0.0002854966364683874\n",
      "Epoch [701/1000] - Training Loss: 0.053, MSE Loss: 0.062658, , KL Loss: -0.013793, Test Loss: 0.067861, Learning Rate: 0.0002843542227896677\n",
      "Epoch [702/1000] - Training Loss: 0.057, MSE Loss: 0.062688, , KL Loss: -0.008693, Test Loss: 0.067791, Learning Rate: 0.00028321443092752365\n",
      "Epoch [703/1000] - Training Loss: 0.053, MSE Loss: 0.062702, , KL Loss: -0.013792, Test Loss: 0.068189, Learning Rate: 0.00028207727213124065\n",
      "Epoch [704/1000] - Training Loss: 0.054, MSE Loss: 0.062769, , KL Loss: -0.012830, Test Loss: 0.067874, Learning Rate: 0.000280942757624117\n",
      "Epoch [705/1000] - Training Loss: 0.054, MSE Loss: 0.062808, , KL Loss: -0.013109, Test Loss: 0.067730, Learning Rate: 0.00027981089860335256\n",
      "Epoch [706/1000] - Training Loss: 0.054, MSE Loss: 0.062752, , KL Loss: -0.011780, Test Loss: 0.067972, Learning Rate: 0.0002786817062399392\n",
      "Epoch [707/1000] - Training Loss: 0.052, MSE Loss: 0.062749, , KL Loss: -0.014503, Test Loss: 0.068272, Learning Rate: 0.00027755519167854974\n",
      "Epoch [708/1000] - Training Loss: 0.053, MSE Loss: 0.062781, , KL Loss: -0.013493, Test Loss: 0.067795, Learning Rate: 0.00027643136603742785\n",
      "Epoch [709/1000] - Training Loss: 0.053, MSE Loss: 0.062820, , KL Loss: -0.013965, Test Loss: 0.067623, Learning Rate: 0.00027531024040827924\n",
      "Epoch [710/1000] - Training Loss: 0.054, MSE Loss: 0.062734, , KL Loss: -0.011638, Test Loss: 0.067355, Learning Rate: 0.00027419182585616104\n",
      "Epoch [711/1000] - Training Loss: 0.053, MSE Loss: 0.062768, , KL Loss: -0.013289, Test Loss: 0.067249, Learning Rate: 0.0002730761334193734\n",
      "Epoch [712/1000] - Training Loss: 0.052, MSE Loss: 0.062780, , KL Loss: -0.014532, Test Loss: 0.067720, Learning Rate: 0.0002719631741093499\n",
      "Epoch [713/1000] - Training Loss: 0.056, MSE Loss: 0.062814, , KL Loss: -0.009841, Test Loss: 0.067678, Learning Rate: 0.00027085295891055017\n",
      "Epoch [714/1000] - Training Loss: 0.054, MSE Loss: 0.062665, , KL Loss: -0.012620, Test Loss: 0.067550, Learning Rate: 0.0002697454987803498\n",
      "Epoch [715/1000] - Training Loss: 0.054, MSE Loss: 0.062679, , KL Loss: -0.012372, Test Loss: 0.068251, Learning Rate: 0.00026864080464893304\n",
      "Epoch [716/1000] - Training Loss: 0.053, MSE Loss: 0.062650, , KL Loss: -0.013684, Test Loss: 0.068503, Learning Rate: 0.0002675388874191852\n",
      "Epoch [717/1000] - Training Loss: 0.053, MSE Loss: 0.062928, , KL Loss: -0.013777, Test Loss: 0.067806, Learning Rate: 0.00026643975796658445\n",
      "Epoch [718/1000] - Training Loss: 0.055, MSE Loss: 0.062722, , KL Loss: -0.010433, Test Loss: 0.067811, Learning Rate: 0.0002653434271390944\n",
      "Epoch [719/1000] - Training Loss: 0.053, MSE Loss: 0.062792, , KL Loss: -0.014025, Test Loss: 0.067687, Learning Rate: 0.00026424990575705804\n",
      "Epoch [720/1000] - Training Loss: 0.052, MSE Loss: 0.062703, , KL Loss: -0.014972, Test Loss: 0.067925, Learning Rate: 0.0002631592046130901\n",
      "Epoch [721/1000] - Training Loss: 0.057, MSE Loss: 0.062777, , KL Loss: -0.008485, Test Loss: 0.067522, Learning Rate: 0.0002620713344719702\n",
      "Epoch [722/1000] - Training Loss: 0.053, MSE Loss: 0.062757, , KL Loss: -0.013332, Test Loss: 0.067351, Learning Rate: 0.00026098630607053756\n",
      "Epoch [723/1000] - Training Loss: 0.054, MSE Loss: 0.062765, , KL Loss: -0.012553, Test Loss: 0.067369, Learning Rate: 0.0002599041301175844\n",
      "Epoch [724/1000] - Training Loss: 0.056, MSE Loss: 0.062708, , KL Loss: -0.009432, Test Loss: 0.067914, Learning Rate: 0.00025882481729375047\n",
      "Epoch [725/1000] - Training Loss: 0.054, MSE Loss: 0.062628, , KL Loss: -0.012422, Test Loss: 0.067728, Learning Rate: 0.00025774837825141784\n",
      "Epoch [726/1000] - Training Loss: 0.055, MSE Loss: 0.062725, , KL Loss: -0.010356, Test Loss: 0.068031, Learning Rate: 0.000256674823614605\n",
      "Epoch [727/1000] - Training Loss: 0.054, MSE Loss: 0.062895, , KL Loss: -0.012870, Test Loss: 0.068545, Learning Rate: 0.000255604163978863\n",
      "Epoch [728/1000] - Training Loss: 0.052, MSE Loss: 0.062671, , KL Loss: -0.014405, Test Loss: 0.067821, Learning Rate: 0.00025453640991117014\n",
      "Epoch [729/1000] - Training Loss: 0.053, MSE Loss: 0.062682, , KL Loss: -0.012971, Test Loss: 0.068112, Learning Rate: 0.00025347157194982795\n",
      "Epoch [730/1000] - Training Loss: 0.054, MSE Loss: 0.062877, , KL Loss: -0.012498, Test Loss: 0.067884, Learning Rate: 0.000252409660604357\n",
      "Epoch [731/1000] - Training Loss: 0.054, MSE Loss: 0.062872, , KL Loss: -0.012546, Test Loss: 0.068019, Learning Rate: 0.00025135068635539413\n",
      "Epoch [732/1000] - Training Loss: 0.054, MSE Loss: 0.062781, , KL Loss: -0.012616, Test Loss: 0.067978, Learning Rate: 0.0002502946596545873\n",
      "Epoch [733/1000] - Training Loss: 0.053, MSE Loss: 0.062888, , KL Loss: -0.013112, Test Loss: 0.067363, Learning Rate: 0.00024924159092449367\n",
      "Epoch [734/1000] - Training Loss: 0.055, MSE Loss: 0.062739, , KL Loss: -0.010836, Test Loss: 0.067703, Learning Rate: 0.00024819149055847647\n",
      "Epoch [735/1000] - Training Loss: 0.052, MSE Loss: 0.062765, , KL Loss: -0.014015, Test Loss: 0.067878, Learning Rate: 0.00024714436892060254\n",
      "Epoch [736/1000] - Training Loss: 0.054, MSE Loss: 0.062755, , KL Loss: -0.012403, Test Loss: 0.067941, Learning Rate: 0.00024610023634553946\n",
      "Epoch [737/1000] - Training Loss: 0.056, MSE Loss: 0.062754, , KL Loss: -0.008725, Test Loss: 0.067937, Learning Rate: 0.0002450591031384543\n",
      "Epoch [738/1000] - Training Loss: 0.055, MSE Loss: 0.062864, , KL Loss: -0.011152, Test Loss: 0.068206, Learning Rate: 0.00024402097957491176\n",
      "Epoch [739/1000] - Training Loss: 0.054, MSE Loss: 0.062775, , KL Loss: -0.011203, Test Loss: 0.068182, Learning Rate: 0.00024298587590077214\n",
      "Epoch [740/1000] - Training Loss: 0.053, MSE Loss: 0.062782, , KL Loss: -0.013755, Test Loss: 0.067851, Learning Rate: 0.00024195380233209068\n",
      "Epoch [741/1000] - Training Loss: 0.053, MSE Loss: 0.062614, , KL Loss: -0.012687, Test Loss: 0.067979, Learning Rate: 0.00024092476905501677\n",
      "Epoch [742/1000] - Training Loss: 0.054, MSE Loss: 0.062798, , KL Loss: -0.012325, Test Loss: 0.068022, Learning Rate: 0.00023989878622569365\n",
      "Epoch [743/1000] - Training Loss: 0.054, MSE Loss: 0.062810, , KL Loss: -0.011702, Test Loss: 0.067716, Learning Rate: 0.00023887586397015762\n",
      "Epoch [744/1000] - Training Loss: 0.052, MSE Loss: 0.062717, , KL Loss: -0.013788, Test Loss: 0.067988, Learning Rate: 0.00023785601238423838\n",
      "Epoch [745/1000] - Training Loss: 0.055, MSE Loss: 0.062719, , KL Loss: -0.010529, Test Loss: 0.067842, Learning Rate: 0.00023683924153345906\n",
      "Epoch [746/1000] - Training Loss: 0.053, MSE Loss: 0.062761, , KL Loss: -0.012423, Test Loss: 0.067780, Learning Rate: 0.00023582556145293785\n",
      "Epoch [747/1000] - Training Loss: 0.052, MSE Loss: 0.062791, , KL Loss: -0.014378, Test Loss: 0.067568, Learning Rate: 0.00023481498214728778\n",
      "Epoch [748/1000] - Training Loss: 0.052, MSE Loss: 0.062723, , KL Loss: -0.014407, Test Loss: 0.067885, Learning Rate: 0.00023380751359051825\n",
      "Epoch [749/1000] - Training Loss: 0.055, MSE Loss: 0.062794, , KL Loss: -0.010748, Test Loss: 0.067750, Learning Rate: 0.00023280316572593782\n",
      "Epoch [750/1000] - Training Loss: 0.052, MSE Loss: 0.062699, , KL Loss: -0.014170, Test Loss: 0.067780, Learning Rate: 0.00023180194846605413\n",
      "Epoch [751/1000] - Training Loss: 0.053, MSE Loss: 0.062710, , KL Loss: -0.012330, Test Loss: 0.067948, Learning Rate: 0.00023080387169247733\n",
      "Epoch [752/1000] - Training Loss: 0.055, MSE Loss: 0.062704, , KL Loss: -0.010760, Test Loss: 0.068058, Learning Rate: 0.0002298089452558221\n",
      "Epoch [753/1000] - Training Loss: 0.053, MSE Loss: 0.062819, , KL Loss: -0.013441, Test Loss: 0.067854, Learning Rate: 0.00022881717897561094\n",
      "Epoch [754/1000] - Training Loss: 0.055, MSE Loss: 0.062646, , KL Loss: -0.010569, Test Loss: 0.067770, Learning Rate: 0.00022782858264017646\n",
      "Epoch [755/1000] - Training Loss: 0.054, MSE Loss: 0.062629, , KL Loss: -0.011756, Test Loss: 0.067994, Learning Rate: 0.0002268431660065656\n",
      "Epoch [756/1000] - Training Loss: 0.052, MSE Loss: 0.062749, , KL Loss: -0.013701, Test Loss: 0.068051, Learning Rate: 0.00022586093880044242\n",
      "Epoch [757/1000] - Training Loss: 0.053, MSE Loss: 0.062807, , KL Loss: -0.012320, Test Loss: 0.067809, Learning Rate: 0.00022488191071599314\n",
      "Epoch [758/1000] - Training Loss: 0.052, MSE Loss: 0.062674, , KL Loss: -0.014444, Test Loss: 0.067976, Learning Rate: 0.00022390609141582967\n",
      "Epoch [759/1000] - Training Loss: 0.052, MSE Loss: 0.062743, , KL Loss: -0.013514, Test Loss: 0.067993, Learning Rate: 0.00022293349053089436\n",
      "Epoch [760/1000] - Training Loss: 0.052, MSE Loss: 0.062726, , KL Loss: -0.014513, Test Loss: 0.068072, Learning Rate: 0.00022196411766036537\n",
      "Epoch [761/1000] - Training Loss: 0.052, MSE Loss: 0.062645, , KL Loss: -0.013957, Test Loss: 0.068011, Learning Rate: 0.00022099798237156177\n",
      "Epoch [762/1000] - Training Loss: 0.051, MSE Loss: 0.062730, , KL Loss: -0.015001, Test Loss: 0.067893, Learning Rate: 0.00022003509419984865\n",
      "Epoch [763/1000] - Training Loss: 0.051, MSE Loss: 0.062639, , KL Loss: -0.015307, Test Loss: 0.068133, Learning Rate: 0.00021907546264854344\n",
      "Epoch [764/1000] - Training Loss: 0.054, MSE Loss: 0.062708, , KL Loss: -0.011110, Test Loss: 0.068060, Learning Rate: 0.00021811909718882244\n",
      "Epoch [765/1000] - Training Loss: 0.051, MSE Loss: 0.062778, , KL Loss: -0.014955, Test Loss: 0.067837, Learning Rate: 0.00021716600725962636\n",
      "Epoch [766/1000] - Training Loss: 0.051, MSE Loss: 0.062804, , KL Loss: -0.014923, Test Loss: 0.067997, Learning Rate: 0.00021621620226756796\n",
      "Epoch [767/1000] - Training Loss: 0.052, MSE Loss: 0.062704, , KL Loss: -0.014298, Test Loss: 0.068115, Learning Rate: 0.00021526969158683926\n",
      "Epoch [768/1000] - Training Loss: 0.051, MSE Loss: 0.062776, , KL Loss: -0.015780, Test Loss: 0.067642, Learning Rate: 0.00021432648455911867\n",
      "Epoch [769/1000] - Training Loss: 0.051, MSE Loss: 0.062703, , KL Loss: -0.015300, Test Loss: 0.068097, Learning Rate: 0.00021338659049347853\n",
      "Epoch [770/1000] - Training Loss: 0.051, MSE Loss: 0.062784, , KL Loss: -0.015503, Test Loss: 0.067718, Learning Rate: 0.00021245001866629379\n",
      "Epoch [771/1000] - Training Loss: 0.052, MSE Loss: 0.062723, , KL Loss: -0.013358, Test Loss: 0.067752, Learning Rate: 0.00021151677832115053\n",
      "Epoch [772/1000] - Training Loss: 0.051, MSE Loss: 0.062772, , KL Loss: -0.015766, Test Loss: 0.067600, Learning Rate: 0.00021058687866875388\n",
      "Epoch [773/1000] - Training Loss: 0.051, MSE Loss: 0.062812, , KL Loss: -0.015070, Test Loss: 0.067758, Learning Rate: 0.00020966032888683825\n",
      "Epoch [774/1000] - Training Loss: 0.054, MSE Loss: 0.062723, , KL Loss: -0.011216, Test Loss: 0.067679, Learning Rate: 0.00020873713812007572\n",
      "Epoch [775/1000] - Training Loss: 0.052, MSE Loss: 0.062751, , KL Loss: -0.013373, Test Loss: 0.067975, Learning Rate: 0.0002078173154799867\n",
      "Epoch [776/1000] - Training Loss: 0.052, MSE Loss: 0.062696, , KL Loss: -0.013148, Test Loss: 0.068037, Learning Rate: 0.00020690087004484917\n",
      "Epoch [777/1000] - Training Loss: 0.051, MSE Loss: 0.062743, , KL Loss: -0.014844, Test Loss: 0.068354, Learning Rate: 0.00020598781085960952\n",
      "Epoch [778/1000] - Training Loss: 0.052, MSE Loss: 0.062861, , KL Loss: -0.014054, Test Loss: 0.067858, Learning Rate: 0.00020507814693579315\n",
      "Epoch [779/1000] - Training Loss: 0.052, MSE Loss: 0.062589, , KL Loss: -0.013918, Test Loss: 0.067986, Learning Rate: 0.00020417188725141617\n",
      "Epoch [780/1000] - Training Loss: 0.052, MSE Loss: 0.062735, , KL Loss: -0.013713, Test Loss: 0.067712, Learning Rate: 0.0002032690407508955\n",
      "Epoch [781/1000] - Training Loss: 0.053, MSE Loss: 0.062729, , KL Loss: -0.012425, Test Loss: 0.067631, Learning Rate: 0.0002023696163449616\n",
      "Epoch [782/1000] - Training Loss: 0.051, MSE Loss: 0.062760, , KL Loss: -0.014585, Test Loss: 0.067217, Learning Rate: 0.00020147362291057043\n",
      "Epoch [783/1000] - Training Loss: 0.051, MSE Loss: 0.062747, , KL Loss: -0.014743, Test Loss: 0.067710, Learning Rate: 0.00020058106929081535\n",
      "Epoch [784/1000] - Training Loss: 0.051, MSE Loss: 0.062653, , KL Loss: -0.014503, Test Loss: 0.067703, Learning Rate: 0.0001996919642948401\n",
      "Epoch [785/1000] - Training Loss: 0.051, MSE Loss: 0.062791, , KL Loss: -0.014590, Test Loss: 0.067947, Learning Rate: 0.00019880631669775214\n",
      "Epoch [786/1000] - Training Loss: 0.052, MSE Loss: 0.062768, , KL Loss: -0.014052, Test Loss: 0.067806, Learning Rate: 0.00019792413524053593\n",
      "Epoch [787/1000] - Training Loss: 0.052, MSE Loss: 0.062666, , KL Loss: -0.013752, Test Loss: 0.067951, Learning Rate: 0.00019704542862996612\n",
      "Epoch [788/1000] - Training Loss: 0.052, MSE Loss: 0.062734, , KL Loss: -0.013051, Test Loss: 0.067961, Learning Rate: 0.00019617020553852214\n",
      "Epoch [789/1000] - Training Loss: 0.052, MSE Loss: 0.062676, , KL Loss: -0.013432, Test Loss: 0.067824, Learning Rate: 0.00019529847460430258\n",
      "Epoch [790/1000] - Training Loss: 0.051, MSE Loss: 0.062714, , KL Loss: -0.015307, Test Loss: 0.067946, Learning Rate: 0.00019443024443093986\n",
      "Epoch [791/1000] - Training Loss: 0.054, MSE Loss: 0.062752, , KL Loss: -0.010448, Test Loss: 0.067788, Learning Rate: 0.0001935655235875154\n",
      "Epoch [792/1000] - Training Loss: 0.053, MSE Loss: 0.062698, , KL Loss: -0.012812, Test Loss: 0.067817, Learning Rate: 0.00019270432060847461\n",
      "Epoch [793/1000] - Training Loss: 0.051, MSE Loss: 0.062750, , KL Loss: -0.014902, Test Loss: 0.067597, Learning Rate: 0.0001918466439935434\n",
      "Epoch [794/1000] - Training Loss: 0.051, MSE Loss: 0.062674, , KL Loss: -0.014929, Test Loss: 0.067593, Learning Rate: 0.00019099250220764365\n",
      "Epoch [795/1000] - Training Loss: 0.053, MSE Loss: 0.062684, , KL Loss: -0.011650, Test Loss: 0.067886, Learning Rate: 0.00019014190368080993\n",
      "Epoch [796/1000] - Training Loss: 0.052, MSE Loss: 0.062775, , KL Loss: -0.013925, Test Loss: 0.067703, Learning Rate: 0.000189294856808106\n",
      "Epoch [797/1000] - Training Loss: 0.050, MSE Loss: 0.062743, , KL Loss: -0.015709, Test Loss: 0.067517, Learning Rate: 0.0001884513699495431\n",
      "Epoch [798/1000] - Training Loss: 0.051, MSE Loss: 0.062754, , KL Loss: -0.014298, Test Loss: 0.067780, Learning Rate: 0.00018761145142999567\n",
      "Epoch [799/1000] - Training Loss: 0.051, MSE Loss: 0.062839, , KL Loss: -0.014734, Test Loss: 0.068170, Learning Rate: 0.00018677510953912048\n",
      "Epoch [800/1000] - Training Loss: 0.052, MSE Loss: 0.062671, , KL Loss: -0.013685, Test Loss: 0.068186, Learning Rate: 0.00018594235253127422\n",
      "Epoch [801/1000] - Training Loss: 0.051, MSE Loss: 0.062804, , KL Loss: -0.015329, Test Loss: 0.067900, Learning Rate: 0.00018511318862543254\n",
      "Epoch [802/1000] - Training Loss: 0.052, MSE Loss: 0.062702, , KL Loss: -0.013741, Test Loss: 0.068059, Learning Rate: 0.00018428762600510826\n",
      "Epoch [803/1000] - Training Loss: 0.052, MSE Loss: 0.062726, , KL Loss: -0.012811, Test Loss: 0.067921, Learning Rate: 0.00018346567281827126\n",
      "Epoch [804/1000] - Training Loss: 0.053, MSE Loss: 0.062802, , KL Loss: -0.011959, Test Loss: 0.067993, Learning Rate: 0.0001826473371772677\n",
      "Epoch [805/1000] - Training Loss: 0.051, MSE Loss: 0.062611, , KL Loss: -0.014522, Test Loss: 0.067776, Learning Rate: 0.00018183262715874002\n",
      "Epoch [806/1000] - Training Loss: 0.052, MSE Loss: 0.062675, , KL Loss: -0.013018, Test Loss: 0.067841, Learning Rate: 0.00018102155080354705\n",
      "Epoch [807/1000] - Training Loss: 0.056, MSE Loss: 0.062717, , KL Loss: -0.008812, Test Loss: 0.067674, Learning Rate: 0.00018021411611668494\n",
      "Epoch [808/1000] - Training Loss: 0.052, MSE Loss: 0.062803, , KL Loss: -0.012880, Test Loss: 0.067615, Learning Rate: 0.00017941033106720812\n",
      "Epoch [809/1000] - Training Loss: 0.054, MSE Loss: 0.062571, , KL Loss: -0.010281, Test Loss: 0.067615, Learning Rate: 0.0001786102035881507\n",
      "Epoch [810/1000] - Training Loss: 0.052, MSE Loss: 0.062733, , KL Loss: -0.013101, Test Loss: 0.067382, Learning Rate: 0.00017781374157644774\n",
      "Epoch [811/1000] - Training Loss: 0.051, MSE Loss: 0.062686, , KL Loss: -0.014827, Test Loss: 0.067786, Learning Rate: 0.00017702095289285765\n",
      "Epoch [812/1000] - Training Loss: 0.053, MSE Loss: 0.062630, , KL Loss: -0.011283, Test Loss: 0.067881, Learning Rate: 0.00017623184536188485\n",
      "Epoch [813/1000] - Training Loss: 0.051, MSE Loss: 0.062778, , KL Loss: -0.014514, Test Loss: 0.067921, Learning Rate: 0.00017544642677170202\n",
      "Epoch [814/1000] - Training Loss: 0.051, MSE Loss: 0.062835, , KL Loss: -0.015020, Test Loss: 0.068081, Learning Rate: 0.00017466470487407338\n",
      "Epoch [815/1000] - Training Loss: 0.051, MSE Loss: 0.062739, , KL Loss: -0.015015, Test Loss: 0.068310, Learning Rate: 0.00017388668738427889\n",
      "Epoch [816/1000] - Training Loss: 0.051, MSE Loss: 0.062785, , KL Loss: -0.014417, Test Loss: 0.068091, Learning Rate: 0.00017311238198103676\n",
      "Epoch [817/1000] - Training Loss: 0.051, MSE Loss: 0.062659, , KL Loss: -0.014685, Test Loss: 0.067791, Learning Rate: 0.0001723417963064288\n",
      "Epoch [818/1000] - Training Loss: 0.052, MSE Loss: 0.062766, , KL Loss: -0.013631, Test Loss: 0.067914, Learning Rate: 0.00017157493796582444\n",
      "Epoch [819/1000] - Training Loss: 0.051, MSE Loss: 0.062823, , KL Loss: -0.014259, Test Loss: 0.067833, Learning Rate: 0.00017081181452780609\n",
      "Epoch [820/1000] - Training Loss: 0.050, MSE Loss: 0.062756, , KL Loss: -0.015228, Test Loss: 0.067803, Learning Rate: 0.0001700524335240937\n",
      "Epoch [821/1000] - Training Loss: 0.050, MSE Loss: 0.062668, , KL Loss: -0.015173, Test Loss: 0.067747, Learning Rate: 0.00016929680244947155\n",
      "Epoch [822/1000] - Training Loss: 0.050, MSE Loss: 0.062704, , KL Loss: -0.015047, Test Loss: 0.067881, Learning Rate: 0.000168544928761713\n",
      "Epoch [823/1000] - Training Loss: 0.052, MSE Loss: 0.062872, , KL Loss: -0.012824, Test Loss: 0.067654, Learning Rate: 0.00016779681988150807\n",
      "Epoch [824/1000] - Training Loss: 0.050, MSE Loss: 0.062708, , KL Loss: -0.015780, Test Loss: 0.067331, Learning Rate: 0.0001670524831923892\n",
      "Epoch [825/1000] - Training Loss: 0.050, MSE Loss: 0.062669, , KL Loss: -0.014800, Test Loss: 0.067758, Learning Rate: 0.000166311926040659\n",
      "Epoch [826/1000] - Training Loss: 0.051, MSE Loss: 0.062717, , KL Loss: -0.014107, Test Loss: 0.068096, Learning Rate: 0.0001655751557353176\n",
      "Epoch [827/1000] - Training Loss: 0.051, MSE Loss: 0.062729, , KL Loss: -0.014659, Test Loss: 0.067714, Learning Rate: 0.00016484217954799063\n",
      "Epoch [828/1000] - Training Loss: 0.050, MSE Loss: 0.062702, , KL Loss: -0.015467, Test Loss: 0.068056, Learning Rate: 0.00016411300471285702\n",
      "Epoch [829/1000] - Training Loss: 0.050, MSE Loss: 0.062767, , KL Loss: -0.015085, Test Loss: 0.067967, Learning Rate: 0.00016338763842657785\n",
      "Epoch [830/1000] - Training Loss: 0.050, MSE Loss: 0.062677, , KL Loss: -0.015301, Test Loss: 0.067955, Learning Rate: 0.00016266608784822588\n",
      "Epoch [831/1000] - Training Loss: 0.050, MSE Loss: 0.062644, , KL Loss: -0.015629, Test Loss: 0.067897, Learning Rate: 0.00016194836009921377\n",
      "Epoch [832/1000] - Training Loss: 0.050, MSE Loss: 0.062821, , KL Loss: -0.015387, Test Loss: 0.067568, Learning Rate: 0.00016123446226322448\n",
      "Epoch [833/1000] - Training Loss: 0.050, MSE Loss: 0.062786, , KL Loss: -0.015185, Test Loss: 0.067703, Learning Rate: 0.00016052440138614188\n",
      "Epoch [834/1000] - Training Loss: 0.050, MSE Loss: 0.062658, , KL Loss: -0.014751, Test Loss: 0.067763, Learning Rate: 0.0001598181844759799\n",
      "Epoch [835/1000] - Training Loss: 0.051, MSE Loss: 0.062697, , KL Loss: -0.014149, Test Loss: 0.067989, Learning Rate: 0.00015911581850281442\n",
      "Epoch [836/1000] - Training Loss: 0.049, MSE Loss: 0.062798, , KL Loss: -0.016051, Test Loss: 0.067895, Learning Rate: 0.00015841731039871384\n",
      "Epoch [837/1000] - Training Loss: 0.050, MSE Loss: 0.062726, , KL Loss: -0.015050, Test Loss: 0.068045, Learning Rate: 0.00015772266705767145\n",
      "Epoch [838/1000] - Training Loss: 0.049, MSE Loss: 0.062639, , KL Loss: -0.016000, Test Loss: 0.067808, Learning Rate: 0.0001570318953355363\n",
      "Epoch [839/1000] - Training Loss: 0.050, MSE Loss: 0.062662, , KL Loss: -0.014783, Test Loss: 0.067836, Learning Rate: 0.00015634500204994665\n",
      "Epoch [840/1000] - Training Loss: 0.050, MSE Loss: 0.062861, , KL Loss: -0.014796, Test Loss: 0.068001, Learning Rate: 0.0001556619939802617\n",
      "Epoch [841/1000] - Training Loss: 0.050, MSE Loss: 0.062782, , KL Loss: -0.015283, Test Loss: 0.067959, Learning Rate: 0.00015498287786749564\n",
      "Epoch [842/1000] - Training Loss: 0.051, MSE Loss: 0.062716, , KL Loss: -0.014502, Test Loss: 0.068317, Learning Rate: 0.0001543076604142501\n",
      "Epoch [843/1000] - Training Loss: 0.052, MSE Loss: 0.062612, , KL Loss: -0.012200, Test Loss: 0.068187, Learning Rate: 0.00015363634828464872\n",
      "Epoch [844/1000] - Training Loss: 0.051, MSE Loss: 0.062833, , KL Loss: -0.014461, Test Loss: 0.068192, Learning Rate: 0.00015296894810427132\n",
      "Epoch [845/1000] - Training Loss: 0.050, MSE Loss: 0.062691, , KL Loss: -0.015028, Test Loss: 0.067712, Learning Rate: 0.00015230546646008834\n",
      "Epoch [846/1000] - Training Loss: 0.049, MSE Loss: 0.062636, , KL Loss: -0.016058, Test Loss: 0.067853, Learning Rate: 0.00015164590990039554\n",
      "Epoch [847/1000] - Training Loss: 0.051, MSE Loss: 0.062744, , KL Loss: -0.014301, Test Loss: 0.067880, Learning Rate: 0.0001509902849347499\n",
      "Epoch [848/1000] - Training Loss: 0.050, MSE Loss: 0.062770, , KL Loss: -0.014967, Test Loss: 0.067644, Learning Rate: 0.00015033859803390532\n",
      "Epoch [849/1000] - Training Loss: 0.049, MSE Loss: 0.062800, , KL Loss: -0.015882, Test Loss: 0.067718, Learning Rate: 0.00014969085562974833\n",
      "Epoch [850/1000] - Training Loss: 0.050, MSE Loss: 0.062665, , KL Loss: -0.015488, Test Loss: 0.067677, Learning Rate: 0.00014904706411523482\n",
      "Epoch [851/1000] - Training Loss: 0.050, MSE Loss: 0.062738, , KL Loss: -0.014883, Test Loss: 0.067933, Learning Rate: 0.00014840722984432722\n",
      "Epoch [852/1000] - Training Loss: 0.050, MSE Loss: 0.062688, , KL Loss: -0.014845, Test Loss: 0.067888, Learning Rate: 0.00014777135913193162\n",
      "Epoch [853/1000] - Training Loss: 0.051, MSE Loss: 0.062715, , KL Loss: -0.014133, Test Loss: 0.067894, Learning Rate: 0.00014713945825383512\n",
      "Epoch [854/1000] - Training Loss: 0.049, MSE Loss: 0.062774, , KL Loss: -0.015555, Test Loss: 0.067612, Learning Rate: 0.00014651153344664416\n",
      "Epoch [855/1000] - Training Loss: 0.050, MSE Loss: 0.062828, , KL Loss: -0.014869, Test Loss: 0.067499, Learning Rate: 0.0001458875909077233\n",
      "Epoch [856/1000] - Training Loss: 0.050, MSE Loss: 0.062702, , KL Loss: -0.015129, Test Loss: 0.067932, Learning Rate: 0.0001452676367951333\n",
      "Epoch [857/1000] - Training Loss: 0.049, MSE Loss: 0.062777, , KL Loss: -0.015867, Test Loss: 0.068184, Learning Rate: 0.00014465167722757118\n",
      "Epoch [858/1000] - Training Loss: 0.051, MSE Loss: 0.062662, , KL Loss: -0.014158, Test Loss: 0.067885, Learning Rate: 0.00014403971828430903\n",
      "Epoch [859/1000] - Training Loss: 0.049, MSE Loss: 0.062686, , KL Loss: -0.015965, Test Loss: 0.068009, Learning Rate: 0.00014343176600513464\n",
      "Epoch [860/1000] - Training Loss: 0.049, MSE Loss: 0.062625, , KL Loss: -0.015319, Test Loss: 0.067928, Learning Rate: 0.00014282782639029159\n",
      "Epoch [861/1000] - Training Loss: 0.049, MSE Loss: 0.062740, , KL Loss: -0.015551, Test Loss: 0.068417, Learning Rate: 0.00014222790540041993\n",
      "Epoch [862/1000] - Training Loss: 0.050, MSE Loss: 0.062652, , KL Loss: -0.015239, Test Loss: 0.067931, Learning Rate: 0.00014163200895649764\n",
      "Epoch [863/1000] - Training Loss: 0.049, MSE Loss: 0.062850, , KL Loss: -0.015570, Test Loss: 0.068058, Learning Rate: 0.00014104014293978223\n",
      "Epoch [864/1000] - Training Loss: 0.051, MSE Loss: 0.062733, , KL Loss: -0.014081, Test Loss: 0.067904, Learning Rate: 0.00014045231319175224\n",
      "Epoch [865/1000] - Training Loss: 0.050, MSE Loss: 0.062765, , KL Loss: -0.015145, Test Loss: 0.067973, Learning Rate: 0.0001398685255140499\n",
      "Epoch [866/1000] - Training Loss: 0.049, MSE Loss: 0.062660, , KL Loss: -0.015881, Test Loss: 0.067970, Learning Rate: 0.00013928878566842404\n",
      "Epoch [867/1000] - Training Loss: 0.049, MSE Loss: 0.062721, , KL Loss: -0.015813, Test Loss: 0.067758, Learning Rate: 0.0001387130993766728\n",
      "Epoch [868/1000] - Training Loss: 0.049, MSE Loss: 0.062834, , KL Loss: -0.015978, Test Loss: 0.067886, Learning Rate: 0.0001381414723205874\n",
      "Epoch [869/1000] - Training Loss: 0.049, MSE Loss: 0.062713, , KL Loss: -0.015776, Test Loss: 0.067864, Learning Rate: 0.00013757391014189617\n",
      "Epoch [870/1000] - Training Loss: 0.049, MSE Loss: 0.062634, , KL Loss: -0.015705, Test Loss: 0.067975, Learning Rate: 0.00013701041844220874\n",
      "Epoch [871/1000] - Training Loss: 0.050, MSE Loss: 0.062633, , KL Loss: -0.014592, Test Loss: 0.067639, Learning Rate: 0.0001364510027829607\n",
      "Epoch [872/1000] - Training Loss: 0.049, MSE Loss: 0.062790, , KL Loss: -0.015561, Test Loss: 0.067772, Learning Rate: 0.00013589566868535868\n",
      "Epoch [873/1000] - Training Loss: 0.049, MSE Loss: 0.062801, , KL Loss: -0.015868, Test Loss: 0.067737, Learning Rate: 0.000135344421630326\n",
      "Epoch [874/1000] - Training Loss: 0.049, MSE Loss: 0.062838, , KL Loss: -0.015543, Test Loss: 0.067634, Learning Rate: 0.00013479726705844857\n",
      "Epoch [875/1000] - Training Loss: 0.049, MSE Loss: 0.062653, , KL Loss: -0.015752, Test Loss: 0.067818, Learning Rate: 0.00013425421036992124\n",
      "Epoch [876/1000] - Training Loss: 0.049, MSE Loss: 0.062667, , KL Loss: -0.015636, Test Loss: 0.067893, Learning Rate: 0.0001337152569244941\n",
      "Epoch [877/1000] - Training Loss: 0.050, MSE Loss: 0.062687, , KL Loss: -0.014863, Test Loss: 0.067572, Learning Rate: 0.0001331804120414203\n",
      "Epoch [878/1000] - Training Loss: 0.050, MSE Loss: 0.062756, , KL Loss: -0.014181, Test Loss: 0.067829, Learning Rate: 0.00013264968099940269\n",
      "Epoch [879/1000] - Training Loss: 0.049, MSE Loss: 0.062728, , KL Loss: -0.015927, Test Loss: 0.068011, Learning Rate: 0.00013212306903654254\n",
      "Epoch [880/1000] - Training Loss: 0.050, MSE Loss: 0.062774, , KL Loss: -0.014371, Test Loss: 0.068232, Learning Rate: 0.00013160058135028708\n",
      "Epoch [881/1000] - Training Loss: 0.049, MSE Loss: 0.062715, , KL Loss: -0.015505, Test Loss: 0.067860, Learning Rate: 0.00013108222309737914\n",
      "Epoch [882/1000] - Training Loss: 0.049, MSE Loss: 0.062781, , KL Loss: -0.015910, Test Loss: 0.068124, Learning Rate: 0.00013056799939380531\n",
      "Epoch [883/1000] - Training Loss: 0.049, MSE Loss: 0.062701, , KL Loss: -0.015331, Test Loss: 0.067924, Learning Rate: 0.00013005791531474595\n",
      "Epoch [884/1000] - Training Loss: 0.049, MSE Loss: 0.062778, , KL Loss: -0.015944, Test Loss: 0.067866, Learning Rate: 0.00012955197589452482\n",
      "Epoch [885/1000] - Training Loss: 0.049, MSE Loss: 0.062630, , KL Loss: -0.015916, Test Loss: 0.068248, Learning Rate: 0.00012905018612655996\n",
      "Epoch [886/1000] - Training Loss: 0.049, MSE Loss: 0.062707, , KL Loss: -0.014986, Test Loss: 0.067968, Learning Rate: 0.0001285525509633137\n",
      "Epoch [887/1000] - Training Loss: 0.049, MSE Loss: 0.062674, , KL Loss: -0.015923, Test Loss: 0.067983, Learning Rate: 0.00012805907531624413\n",
      "Epoch [888/1000] - Training Loss: 0.049, MSE Loss: 0.062733, , KL Loss: -0.015343, Test Loss: 0.067901, Learning Rate: 0.00012756976405575688\n",
      "Epoch [889/1000] - Training Loss: 0.050, MSE Loss: 0.062744, , KL Loss: -0.014795, Test Loss: 0.067892, Learning Rate: 0.00012708462201115636\n",
      "Epoch [890/1000] - Training Loss: 0.049, MSE Loss: 0.062658, , KL Loss: -0.015647, Test Loss: 0.068212, Learning Rate: 0.0001266036539705988\n",
      "Epoch [891/1000] - Training Loss: 0.049, MSE Loss: 0.062631, , KL Loss: -0.015105, Test Loss: 0.067910, Learning Rate: 0.00012612686468104446\n",
      "Epoch [892/1000] - Training Loss: 0.049, MSE Loss: 0.062741, , KL Loss: -0.015191, Test Loss: 0.067882, Learning Rate: 0.00012565425884821115\n",
      "Epoch [893/1000] - Training Loss: 0.048, MSE Loss: 0.062748, , KL Loss: -0.016172, Test Loss: 0.067745, Learning Rate: 0.00012518584113652782\n",
      "Epoch [894/1000] - Training Loss: 0.049, MSE Loss: 0.062710, , KL Loss: -0.014823, Test Loss: 0.068195, Learning Rate: 0.00012472161616908806\n",
      "Epoch [895/1000] - Training Loss: 0.050, MSE Loss: 0.062606, , KL Loss: -0.013907, Test Loss: 0.067638, Learning Rate: 0.0001242615885276048\n",
      "Epoch [896/1000] - Training Loss: 0.049, MSE Loss: 0.062724, , KL Loss: -0.015204, Test Loss: 0.067841, Learning Rate: 0.00012380576275236528\n",
      "Epoch [897/1000] - Training Loss: 0.048, MSE Loss: 0.062787, , KL Loss: -0.015946, Test Loss: 0.067743, Learning Rate: 0.00012335414334218583\n",
      "Epoch [898/1000] - Training Loss: 0.049, MSE Loss: 0.062707, , KL Loss: -0.015586, Test Loss: 0.067799, Learning Rate: 0.00012290673475436756\n",
      "Epoch [899/1000] - Training Loss: 0.048, MSE Loss: 0.062712, , KL Loss: -0.015940, Test Loss: 0.067600, Learning Rate: 0.00012246354140465286\n",
      "Epoch [900/1000] - Training Loss: 0.049, MSE Loss: 0.062690, , KL Loss: -0.015029, Test Loss: 0.067709, Learning Rate: 0.00012202456766718108\n",
      "Epoch [901/1000] - Training Loss: 0.049, MSE Loss: 0.062711, , KL Loss: -0.015496, Test Loss: 0.068091, Learning Rate: 0.00012158981787444573\n",
      "Epoch [902/1000] - Training Loss: 0.048, MSE Loss: 0.062598, , KL Loss: -0.015719, Test Loss: 0.068142, Learning Rate: 0.00012115929631725172\n",
      "Epoch [903/1000] - Training Loss: 0.048, MSE Loss: 0.062619, , KL Loss: -0.015983, Test Loss: 0.067889, Learning Rate: 0.00012073300724467311\n",
      "Epoch [904/1000] - Training Loss: 0.049, MSE Loss: 0.062782, , KL Loss: -0.014743, Test Loss: 0.067880, Learning Rate: 0.00012031095486401082\n",
      "Epoch [905/1000] - Training Loss: 0.048, MSE Loss: 0.062712, , KL Loss: -0.016201, Test Loss: 0.067904, Learning Rate: 0.0001198931433407516\n",
      "Epoch [906/1000] - Training Loss: 0.050, MSE Loss: 0.062692, , KL Loss: -0.014538, Test Loss: 0.067974, Learning Rate: 0.0001194795767985264\n",
      "Epoch [907/1000] - Training Loss: 0.048, MSE Loss: 0.062780, , KL Loss: -0.015920, Test Loss: 0.067694, Learning Rate: 0.00011907025931907013\n",
      "Epoch [908/1000] - Training Loss: 0.048, MSE Loss: 0.062776, , KL Loss: -0.015825, Test Loss: 0.068078, Learning Rate: 0.000118665194942181\n",
      "Epoch [909/1000] - Training Loss: 0.048, MSE Loss: 0.062689, , KL Loss: -0.016247, Test Loss: 0.067603, Learning Rate: 0.00011826438766568088\n",
      "Epoch [910/1000] - Training Loss: 0.048, MSE Loss: 0.062719, , KL Loss: -0.015768, Test Loss: 0.067318, Learning Rate: 0.00011786784144537574\n",
      "Epoch [911/1000] - Training Loss: 0.048, MSE Loss: 0.062578, , KL Loss: -0.016213, Test Loss: 0.067566, Learning Rate: 0.00011747556019501677\n",
      "Epoch [912/1000] - Training Loss: 0.048, MSE Loss: 0.062679, , KL Loss: -0.016116, Test Loss: 0.067363, Learning Rate: 0.0001170875477862615\n",
      "Epoch [913/1000] - Training Loss: 0.049, MSE Loss: 0.062712, , KL Loss: -0.015373, Test Loss: 0.067583, Learning Rate: 0.00011670380804863568\n",
      "Epoch [914/1000] - Training Loss: 0.048, MSE Loss: 0.062746, , KL Loss: -0.015964, Test Loss: 0.067877, Learning Rate: 0.00011632434476949574\n",
      "Epoch [915/1000] - Training Loss: 0.048, MSE Loss: 0.062725, , KL Loss: -0.015940, Test Loss: 0.068002, Learning Rate: 0.00011594916169399103\n",
      "Epoch [916/1000] - Training Loss: 0.048, MSE Loss: 0.062682, , KL Loss: -0.015924, Test Loss: 0.067949, Learning Rate: 0.00011557826252502686\n",
      "Epoch [917/1000] - Training Loss: 0.049, MSE Loss: 0.062821, , KL Loss: -0.015477, Test Loss: 0.067689, Learning Rate: 0.00011521165092322845\n",
      "Epoch [918/1000] - Training Loss: 0.048, MSE Loss: 0.062689, , KL Loss: -0.015698, Test Loss: 0.067634, Learning Rate: 0.00011484933050690435\n",
      "Epoch [919/1000] - Training Loss: 0.048, MSE Loss: 0.062740, , KL Loss: -0.015830, Test Loss: 0.068016, Learning Rate: 0.0001144913048520107\n",
      "Epoch [920/1000] - Training Loss: 0.048, MSE Loss: 0.062735, , KL Loss: -0.016003, Test Loss: 0.067901, Learning Rate: 0.00011413757749211612\n",
      "Epoch [921/1000] - Training Loss: 0.048, MSE Loss: 0.062830, , KL Loss: -0.016217, Test Loss: 0.068140, Learning Rate: 0.00011378815191836694\n",
      "Epoch [922/1000] - Training Loss: 0.048, MSE Loss: 0.062582, , KL Loss: -0.015285, Test Loss: 0.067760, Learning Rate: 0.00011344303157945249\n",
      "Epoch [923/1000] - Training Loss: 0.048, MSE Loss: 0.062786, , KL Loss: -0.016460, Test Loss: 0.067677, Learning Rate: 0.00011310221988157115\n",
      "Epoch [924/1000] - Training Loss: 0.047, MSE Loss: 0.062612, , KL Loss: -0.016445, Test Loss: 0.067850, Learning Rate: 0.0001127657201883968\n",
      "Epoch [925/1000] - Training Loss: 0.048, MSE Loss: 0.062756, , KL Loss: -0.016369, Test Loss: 0.067671, Learning Rate: 0.00011243353582104564\n",
      "Epoch [926/1000] - Training Loss: 0.048, MSE Loss: 0.062700, , KL Loss: -0.016408, Test Loss: 0.067661, Learning Rate: 0.00011210567005804316\n",
      "Epoch [927/1000] - Training Loss: 0.049, MSE Loss: 0.062803, , KL Loss: -0.015359, Test Loss: 0.067550, Learning Rate: 0.00011178212613529215\n",
      "Epoch [928/1000] - Training Loss: 0.049, MSE Loss: 0.062624, , KL Loss: -0.014344, Test Loss: 0.067766, Learning Rate: 0.00011146290724604034\n",
      "Epoch [929/1000] - Training Loss: 0.048, MSE Loss: 0.062681, , KL Loss: -0.015460, Test Loss: 0.067633, Learning Rate: 0.00011114801654084957\n",
      "Epoch [930/1000] - Training Loss: 0.048, MSE Loss: 0.062730, , KL Loss: -0.015978, Test Loss: 0.068092, Learning Rate: 0.00011083745712756378\n",
      "Epoch [931/1000] - Training Loss: 0.048, MSE Loss: 0.062674, , KL Loss: -0.016174, Test Loss: 0.068438, Learning Rate: 0.00011053123207127901\n",
      "Epoch [932/1000] - Training Loss: 0.049, MSE Loss: 0.062678, , KL Loss: -0.014956, Test Loss: 0.067759, Learning Rate: 0.000110229344394313\n",
      "Epoch [933/1000] - Training Loss: 0.048, MSE Loss: 0.062702, , KL Loss: -0.016170, Test Loss: 0.067984, Learning Rate: 0.00010993179707617529\n",
      "Epoch [934/1000] - Training Loss: 0.048, MSE Loss: 0.062563, , KL Loss: -0.015692, Test Loss: 0.067710, Learning Rate: 0.00010963859305353763\n",
      "Epoch [935/1000] - Training Loss: 0.048, MSE Loss: 0.062760, , KL Loss: -0.015464, Test Loss: 0.067727, Learning Rate: 0.00010934973522020542\n",
      "Epoch [936/1000] - Training Loss: 0.048, MSE Loss: 0.062673, , KL Loss: -0.015549, Test Loss: 0.068283, Learning Rate: 0.00010906522642708897\n",
      "Epoch [937/1000] - Training Loss: 0.048, MSE Loss: 0.062682, , KL Loss: -0.016066, Test Loss: 0.067780, Learning Rate: 0.00010878506948217507\n",
      "Epoch [938/1000] - Training Loss: 0.048, MSE Loss: 0.062602, , KL Loss: -0.015770, Test Loss: 0.067774, Learning Rate: 0.00010850926715049976\n",
      "Epoch [939/1000] - Training Loss: 0.049, MSE Loss: 0.062633, , KL Loss: -0.014693, Test Loss: 0.067833, Learning Rate: 0.0001082378221541206\n",
      "Epoch [940/1000] - Training Loss: 0.048, MSE Loss: 0.062725, , KL Loss: -0.016101, Test Loss: 0.067479, Learning Rate: 0.00010797073717209013\n",
      "Epoch [941/1000] - Training Loss: 0.048, MSE Loss: 0.062763, , KL Loss: -0.015481, Test Loss: 0.067578, Learning Rate: 0.00010770801484042943\n",
      "Epoch [942/1000] - Training Loss: 0.047, MSE Loss: 0.062788, , KL Loss: -0.016235, Test Loss: 0.067821, Learning Rate: 0.00010744965775210168\n",
      "Epoch [943/1000] - Training Loss: 0.049, MSE Loss: 0.062755, , KL Loss: -0.014952, Test Loss: 0.067946, Learning Rate: 0.0001071956684569872\n",
      "Epoch [944/1000] - Training Loss: 0.048, MSE Loss: 0.062799, , KL Loss: -0.015649, Test Loss: 0.067976, Learning Rate: 0.00010694604946185765\n",
      "Epoch [945/1000] - Training Loss: 0.048, MSE Loss: 0.062696, , KL Loss: -0.015302, Test Loss: 0.067939, Learning Rate: 0.00010670080323035181\n",
      "Epoch [946/1000] - Training Loss: 0.048, MSE Loss: 0.062567, , KL Loss: -0.015906, Test Loss: 0.067743, Learning Rate: 0.00010645993218295093\n",
      "Epoch [947/1000] - Training Loss: 0.048, MSE Loss: 0.062810, , KL Loss: -0.016143, Test Loss: 0.067976, Learning Rate: 0.00010622343869695505\n",
      "Epoch [948/1000] - Training Loss: 0.047, MSE Loss: 0.062627, , KL Loss: -0.016141, Test Loss: 0.067713, Learning Rate: 0.00010599132510645942\n",
      "Epoch [949/1000] - Training Loss: 0.047, MSE Loss: 0.062655, , KL Loss: -0.016282, Test Loss: 0.067738, Learning Rate: 0.00010576359370233142\n",
      "Epoch [950/1000] - Training Loss: 0.047, MSE Loss: 0.062565, , KL Loss: -0.015877, Test Loss: 0.067534, Learning Rate: 0.00010554024673218809\n",
      "Epoch [951/1000] - Training Loss: 0.048, MSE Loss: 0.062874, , KL Loss: -0.016008, Test Loss: 0.067838, Learning Rate: 0.00010532128640037384\n",
      "Epoch [952/1000] - Training Loss: 0.048, MSE Loss: 0.062597, , KL Loss: -0.015717, Test Loss: 0.067669, Learning Rate: 0.00010510671486793875\n",
      "Epoch [953/1000] - Training Loss: 0.048, MSE Loss: 0.062694, , KL Loss: -0.015732, Test Loss: 0.068000, Learning Rate: 0.00010489653425261722\n",
      "Epoch [954/1000] - Training Loss: 0.049, MSE Loss: 0.062735, , KL Loss: -0.014834, Test Loss: 0.068003, Learning Rate: 0.00010469074662880711\n",
      "Epoch [955/1000] - Training Loss: 0.048, MSE Loss: 0.062790, , KL Loss: -0.015927, Test Loss: 0.067922, Learning Rate: 0.00010448935402754913\n",
      "Epoch [956/1000] - Training Loss: 0.047, MSE Loss: 0.062767, , KL Loss: -0.016051, Test Loss: 0.068019, Learning Rate: 0.000104292358436507\n",
      "Epoch [957/1000] - Training Loss: 0.048, MSE Loss: 0.062732, , KL Loss: -0.015904, Test Loss: 0.067876, Learning Rate: 0.00010409976179994762\n",
      "Epoch [958/1000] - Training Loss: 0.047, MSE Loss: 0.062673, , KL Loss: -0.016284, Test Loss: 0.067746, Learning Rate: 0.00010391156601872206\n",
      "Epoch [959/1000] - Training Loss: 0.048, MSE Loss: 0.062688, , KL Loss: -0.015342, Test Loss: 0.067971, Learning Rate: 0.00010372777295024671\n",
      "Epoch [960/1000] - Training Loss: 0.047, MSE Loss: 0.062625, , KL Loss: -0.016070, Test Loss: 0.067832, Learning Rate: 0.00010354838440848503\n",
      "Epoch [961/1000] - Training Loss: 0.047, MSE Loss: 0.062758, , KL Loss: -0.016428, Test Loss: 0.067850, Learning Rate: 0.00010337340216392934\n",
      "Epoch [962/1000] - Training Loss: 0.047, MSE Loss: 0.062735, , KL Loss: -0.015846, Test Loss: 0.067745, Learning Rate: 0.00010320282794358391\n",
      "Epoch [963/1000] - Training Loss: 0.048, MSE Loss: 0.062630, , KL Loss: -0.015606, Test Loss: 0.067877, Learning Rate: 0.00010303666343094731\n",
      "Epoch [964/1000] - Training Loss: 0.047, MSE Loss: 0.062633, , KL Loss: -0.015990, Test Loss: 0.067608, Learning Rate: 0.00010287491026599625\n",
      "Epoch [965/1000] - Training Loss: 0.047, MSE Loss: 0.062558, , KL Loss: -0.016063, Test Loss: 0.068136, Learning Rate: 0.00010271757004516916\n",
      "Epoch [966/1000] - Training Loss: 0.048, MSE Loss: 0.062679, , KL Loss: -0.015677, Test Loss: 0.067845, Learning Rate: 0.00010256464432135051\n",
      "Epoch [967/1000] - Training Loss: 0.048, MSE Loss: 0.062718, , KL Loss: -0.015406, Test Loss: 0.067933, Learning Rate: 0.00010241613460385548\n",
      "Epoch [968/1000] - Training Loss: 0.049, MSE Loss: 0.062802, , KL Loss: -0.014690, Test Loss: 0.067994, Learning Rate: 0.00010227204235841494\n",
      "Epoch [969/1000] - Training Loss: 0.047, MSE Loss: 0.062635, , KL Loss: -0.016021, Test Loss: 0.067860, Learning Rate: 0.00010213236900716127\n",
      "Epoch [970/1000] - Training Loss: 0.047, MSE Loss: 0.062704, , KL Loss: -0.016357, Test Loss: 0.067800, Learning Rate: 0.00010199711592861402\n",
      "Epoch [971/1000] - Training Loss: 0.047, MSE Loss: 0.062708, , KL Loss: -0.015893, Test Loss: 0.067700, Learning Rate: 0.00010186628445766648\n",
      "Epoch [972/1000] - Training Loss: 0.047, MSE Loss: 0.062619, , KL Loss: -0.015720, Test Loss: 0.067337, Learning Rate: 0.00010173987588557239\n",
      "Epoch [973/1000] - Training Loss: 0.048, MSE Loss: 0.062797, , KL Loss: -0.015601, Test Loss: 0.067869, Learning Rate: 0.00010161789145993343\n",
      "Epoch [974/1000] - Training Loss: 0.048, MSE Loss: 0.062691, , KL Loss: -0.015526, Test Loss: 0.067609, Learning Rate: 0.00010150033238468656\n",
      "Epoch [975/1000] - Training Loss: 0.047, MSE Loss: 0.062654, , KL Loss: -0.015779, Test Loss: 0.068060, Learning Rate: 0.00010138719982009242\n",
      "Epoch [976/1000] - Training Loss: 0.048, MSE Loss: 0.062758, , KL Loss: -0.015624, Test Loss: 0.067984, Learning Rate: 0.00010127849488272374\n",
      "Epoch [977/1000] - Training Loss: 0.048, MSE Loss: 0.062708, , KL Loss: -0.014944, Test Loss: 0.068094, Learning Rate: 0.00010117421864545435\n",
      "Epoch [978/1000] - Training Loss: 0.047, MSE Loss: 0.062747, , KL Loss: -0.015611, Test Loss: 0.067639, Learning Rate: 0.00010107437213744867\n",
      "Epoch [979/1000] - Training Loss: 0.047, MSE Loss: 0.062663, , KL Loss: -0.016389, Test Loss: 0.067686, Learning Rate: 0.00010097895634415135\n",
      "Epoch [980/1000] - Training Loss: 0.047, MSE Loss: 0.062621, , KL Loss: -0.015755, Test Loss: 0.067958, Learning Rate: 0.00010088797220727782\n",
      "Epoch [981/1000] - Training Loss: 0.047, MSE Loss: 0.062478, , KL Loss: -0.016172, Test Loss: 0.067889, Learning Rate: 0.00010080142062480471\n",
      "Epoch [982/1000] - Training Loss: 0.047, MSE Loss: 0.062784, , KL Loss: -0.016382, Test Loss: 0.068136, Learning Rate: 0.00010071930245096126\n",
      "Epoch [983/1000] - Training Loss: 0.047, MSE Loss: 0.062664, , KL Loss: -0.016153, Test Loss: 0.067909, Learning Rate: 0.0001006416184962206\n",
      "Epoch [984/1000] - Training Loss: 0.048, MSE Loss: 0.062683, , KL Loss: -0.015036, Test Loss: 0.067847, Learning Rate: 0.00010056836952729215\n",
      "Epoch [985/1000] - Training Loss: 0.047, MSE Loss: 0.062767, , KL Loss: -0.016198, Test Loss: 0.067777, Learning Rate: 0.00010049955626711355\n",
      "Epoch [986/1000] - Training Loss: 0.047, MSE Loss: 0.062658, , KL Loss: -0.015616, Test Loss: 0.067695, Learning Rate: 0.00010043517939484389\n",
      "Epoch [987/1000] - Training Loss: 0.047, MSE Loss: 0.062755, , KL Loss: -0.016005, Test Loss: 0.067895, Learning Rate: 0.00010037523954585697\n",
      "Epoch [988/1000] - Training Loss: 0.047, MSE Loss: 0.062737, , KL Loss: -0.016182, Test Loss: 0.067590, Learning Rate: 0.00010031973731173487\n",
      "Epoch [989/1000] - Training Loss: 0.047, MSE Loss: 0.062781, , KL Loss: -0.016084, Test Loss: 0.067618, Learning Rate: 0.00010026867324026221\n",
      "Epoch [990/1000] - Training Loss: 0.047, MSE Loss: 0.062734, , KL Loss: -0.015571, Test Loss: 0.067655, Learning Rate: 0.0001002220478354208\n",
      "Epoch [991/1000] - Training Loss: 0.046, MSE Loss: 0.062641, , KL Loss: -0.016318, Test Loss: 0.067772, Learning Rate: 0.00010017986155738458\n",
      "Epoch [992/1000] - Training Loss: 0.047, MSE Loss: 0.062683, , KL Loss: -0.015975, Test Loss: 0.067752, Learning Rate: 0.00010014211482251504\n",
      "Epoch [993/1000] - Training Loss: 0.047, MSE Loss: 0.062643, , KL Loss: -0.015265, Test Loss: 0.067659, Learning Rate: 0.00010010880800335719\n",
      "Epoch [994/1000] - Training Loss: 0.047, MSE Loss: 0.062612, , KL Loss: -0.016200, Test Loss: 0.067914, Learning Rate: 0.00010007994142863598\n",
      "Epoch [995/1000] - Training Loss: 0.047, MSE Loss: 0.062730, , KL Loss: -0.015939, Test Loss: 0.067968, Learning Rate: 0.00010005551538325275\n",
      "Epoch [996/1000] - Training Loss: 0.047, MSE Loss: 0.062595, , KL Loss: -0.015687, Test Loss: 0.067792, Learning Rate: 0.00010003553010828276\n",
      "Epoch [997/1000] - Training Loss: 0.047, MSE Loss: 0.062698, , KL Loss: -0.015466, Test Loss: 0.067926, Learning Rate: 0.00010001998580097258\n",
      "Epoch [998/1000] - Training Loss: 0.047, MSE Loss: 0.062657, , KL Loss: -0.015783, Test Loss: 0.068211, Learning Rate: 0.0001000088826147383\n",
      "Epoch [999/1000] - Training Loss: 0.047, MSE Loss: 0.062586, , KL Loss: -0.015643, Test Loss: 0.068082, Learning Rate: 0.00010000222065916382\n",
      "Epoch [1000/1000] - Training Loss: 0.048, MSE Loss: 0.062572, , KL Loss: -0.014307, Test Loss: 0.067871, Learning Rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "tcn = TCN().to(device)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(tcn.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=t_max, eta_min=0.0001)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train.reshape(-1))\n",
    "train_dataset = train_dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val, Y_val.reshape(-1))\n",
    "val_dataset = val_dataset\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_hist = []\n",
    "val_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    tcn.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        lower_val = torch.quantile(batch_X, 0.01, dim=0, keepdim=True)\n",
    "        upper_val = torch.quantile(batch_X, 0.99, dim=0, keepdim=True)\n",
    "        batch_X = torch.clamp(batch_X, min=lower_val, max=upper_val)\n",
    "        batch_X = batch_X.float()\n",
    "        predictions, p_mu, p_logvar = tcn(batch_X.float())\n",
    "\n",
    "        predictions = predictions.float()  \n",
    "        batch_y = batch_y.float()  \n",
    "        p_logvar = p_logvar.float()  \n",
    "        p_mu = p_mu.float() \n",
    "\n",
    "        lossmse = criterion(predictions, batch_y)\n",
    "        losskl = -0.5 * torch.sum(1 + p_logvar - p_mu.pow(2) - p_logvar.exp())\n",
    "\n",
    "        beta = (epoch + 1) / num_epochs\n",
    "        #beta = 0.1\n",
    "        loss = lossmse + beta * losskl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    average_loss = total_loss/len(train_loader)\n",
    "    train_hist.append(average_loss)\n",
    "\n",
    "    tcn.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            lower_valv = torch.quantile(batch_X_val, 0.01, dim=0, keepdim=True)\n",
    "            upper_valv = torch.quantile(batch_X_val, 0.99, dim=0, keepdim=True)\n",
    "            batch_X_val = torch.clamp(batch_X, min=lower_valv, max=upper_valv)\n",
    "            predictions_val, _, _ = tcn(batch_X_val.float())\n",
    "\n",
    "        \n",
    "            lossmsev = criterion(predictions_val,batch_y_val.float())\n",
    "            val_loss = lossmsev\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "        average_val_loss = total_val_loss / len(val_loader)\n",
    "        val_hist.append(average_val_loss)\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {average_loss:.3f}, MSE Loss: {lossmse:.6f}, , KL Loss: {losskl:.6f}, Test Loss: {average_val_loss:.6f}, Learning Rate: {optimizer.param_groups[0][\"lr\"]}')\n",
    "        #print(predictions.max().item(), p_logvar.max().item(), p_mu.max().item(), predictions.min().item(), p_logvar.min().item(),p_mu.min().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.062857226\n",
      "r2: -0.0191123082248692\n",
      "R1: [[ 1.         -0.00735367]\n",
      " [-0.00735367  1.        ]]\n",
      "MSE: 0.066880226\n",
      "r2: -0.04961712606950752\n",
      "R1: [[1.         0.04475253]\n",
      " [0.04475253 1.        ]]\n",
      "MSE: 0.11064657\n",
      "r2: -0.06554955981932209\n",
      "R1: [[ 1.         -0.05079634]\n",
      " [-0.05079634  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def results(model, X, Y):\n",
    "    \"\"\"\"\"\"\n",
    "    model.eval()\n",
    "    pred, _, _ = model(X)\n",
    "    pred = pred.cpu().detach().numpy()\n",
    "    Y = Y.cpu().detach().numpy()\n",
    "    print(\"MSE:\", mean_squared_error(Y, pred))\n",
    "    print(\"r2:\", r2_score(Y, pred))\n",
    "    print(\"R1:\", np.corrcoef(pred.T, Y.T))\n",
    "\n",
    "results(tcn, X_train, Y_train)\n",
    "results(tcn, X_val, Y_val)\n",
    "results(tcn, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
